#!/usr/bin/env python
# coding: utf-8

"""
NeurIPS Open Polymer Prediction 2025 - Advanced Ensemble v2
é«˜åº¦ãªã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹ãƒãƒªãƒãƒ¼ç‰¹æ€§äºˆæ¸¬

ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¯ä»¥ä¸‹ã®ç‰¹å¾´ã‚’æŒã¡ã¾ã™ï¼š
1. RDKitãƒ™ãƒ¼ã‚¹ã®é«˜åº¦ãªåˆ†å­ç‰¹å¾´é‡ï¼ˆ100ç‰¹å¾´é‡ï¼‰
2. 5ã¤ã®MLãƒ¢ãƒ‡ãƒ«ï¼ˆXGBoost, CatBoost, RandomForest, GradientBoosting, KNNï¼‰
3. åŠ é‡å¹³å‡ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã«ã‚ˆã‚‹æ€§èƒ½å‘ä¸Š
4. å…¨5ç‰¹æ€§ï¼ˆTg, FFV, Tc, Density, Rgï¼‰ã®äºˆæ¸¬
"""

import os
import sys
import time
import warnings
import numpy as np
import pandas as pd
from sklearn.model_selection import KFold
from sklearn.metrics import mean_absolute_error
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.neighbors import KNeighborsRegressor
import xgboost as xgb
from catboost import CatBoostRegressor

warnings.filterwarnings('ignore')

# è¨­å®š
SEED = 42
np.random.seed(SEED)

# Kaggleç’°å¢ƒåˆ¤å®š
IS_KAGGLE = os.path.exists('/kaggle/input')

if IS_KAGGLE:
    # Kaggleç’°å¢ƒ
    INPUT_DIR = '/kaggle/input/neurips-open-polymer-prediction-2025'
    
    # RDKit wheel fileã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
    print("ğŸ”§ RDKitã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ä¸­...")
    try:
        import subprocess
        # RDKit wheel datasetã‹ã‚‰ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆè¤‡æ•°ã®ãƒ‘ã‚¹ã‚’è©¦ã™ï¼‰
        rdkit_paths = [
            '/kaggle/input/rdkit-install-whl/rdkit_wheel',
            '/kaggle/input/rdkit-install-whl',
            '/kaggle/input/rdkit-whl',
            '/kaggle/input/rdkit'
        ]
        
        installed = False
        for rdkit_dataset in rdkit_paths:
            if os.path.exists(rdkit_dataset):
                print(f"  ğŸ“ RDKitãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç™ºè¦‹: {rdkit_dataset}")
                whl_files = [f for f in os.listdir(rdkit_dataset) if f.endswith('.whl')]
                if whl_files:
                    # æœ€æ–°ã®whlãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨
                    whl_file = os.path.join(rdkit_dataset, sorted(whl_files)[-1])
                    print(f"  ğŸ“¦ ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ä¸­: {whl_file}")
                    result = subprocess.run([sys.executable, '-m', 'pip', 'install', whl_file, '--no-deps'], 
                                          capture_output=True, text=True)
                    if result.returncode == 0:
                        print(f"  âœ… RDKit installed from: {whl_file}")
                        installed = True
                        break
                    else:
                        print(f"  âš ï¸ ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å¤±æ•—: {result.stderr}")
        
        if not installed:
            print("  âš ï¸ RDKitãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            # é€šå¸¸ã®pipã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã‚’è©¦ã™
            result = subprocess.run([sys.executable, '-m', 'pip', 'install', 'rdkit-pypi'], 
                                  capture_output=True, text=True)
            if result.returncode == 0:
                print("  âœ… RDKit installed via pip")
            else:
                print(f"  âš ï¸ pip install failed: {result.stderr}")
                
    except Exception as e:
        print(f"âš ï¸ RDKit installation failed: {e}")
else:
    # ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒ
    INPUT_DIR = '../input/neurips-open-polymer-prediction-2025'

# RDKitåˆ©ç”¨å¯èƒ½æ€§ãƒã‚§ãƒƒã‚¯
try:
    from rdkit import Chem
    from rdkit.Chem import Descriptors, rdMolDescriptors
    rdkit_available = True
    print("âœ… RDKitåˆ©ç”¨å¯èƒ½ - é«˜ç²¾åº¦åˆ†å­ç‰¹å¾´é‡ã‚’ä½¿ç”¨")
except ImportError:
    rdkit_available = False
    print("âš ï¸ RDKitåˆ©ç”¨ä¸å¯ - åŸºæœ¬SMILESç‰¹å¾´é‡ã‚’ä½¿ç”¨")

print("ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚¤ãƒ³ãƒãƒ¼ãƒˆå®Œäº†")

# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
def load_data():
    """ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿"""
    train = pd.read_csv(os.path.join(INPUT_DIR, 'train.csv'))
    test = pd.read_csv(os.path.join(INPUT_DIR, 'test.csv'))
    submission = pd.read_csv(os.path.join(INPUT_DIR, 'sample_submission.csv'))
    
    print(f"è¨“ç·´ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {train.shape}")
    print(f"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {test.shape}")
    
    return train, test, submission

# åŸºæœ¬çš„ãªSMILESç‰¹å¾´é‡
def basic_smiles_features(smiles):
    """åŸºæœ¬çš„ãªSMILESç‰¹å¾´é‡ï¼ˆRDKitä¸ä½¿ç”¨æ™‚ï¼‰"""
    if pd.isna(smiles) or smiles == '':
        return [0] * 16
    
    features = [
        len(smiles),                    # SMILESæ–‡å­—åˆ—é•·
        smiles.count('C'),             # ç‚­ç´ æ•°
        smiles.count('N'),             # çª’ç´ æ•°
        smiles.count('O'),             # é…¸ç´ æ•°
        smiles.count('S'),             # ç¡«é»„æ•°
        smiles.count('P'),             # ãƒªãƒ³æ•°
        smiles.count('F'),             # ãƒ•ãƒƒç´ æ•°
        smiles.count('Cl'),            # å¡©ç´ æ•°
        smiles.count('='),             # äºŒé‡çµåˆæ•°
        smiles.count('#'),             # ä¸‰é‡çµåˆæ•°
        smiles.count('('),             # åˆ†å²æ•°
        smiles.count('['),             # ç‰¹æ®ŠåŸå­æ•°
        smiles.count('@'),             # ã‚­ãƒ©ãƒ«ä¸­å¿ƒæ•°
        smiles.count('c'),             # èŠ³é¦™æ—ç‚­ç´ æ•°
        smiles.count(':'),             # èŠ³é¦™æ—çµåˆæ•°
        smiles.count('-'),             # å˜çµåˆæ•°
    ]
    return features

# RDKitãƒ™ãƒ¼ã‚¹ã®åˆ†å­ç‰¹å¾´é‡
def rdkit_molecular_features(smiles):
    """RDKitã‚’ä½¿ç”¨ã—ãŸåˆ†å­ç‰¹å¾´é‡"""
    if pd.isna(smiles) or smiles == '':
        return [0] * 100
    
    mol = Chem.MolFromSmiles(smiles)
    if mol is None:
        return [0] * 100
    
    features = []
    
    # åŸºæœ¬è¨˜è¿°å­ï¼ˆ30å€‹ï¼‰
    basic_descriptors = [
        Descriptors.MolWt,              # åˆ†å­é‡
        Descriptors.NumHDonors,         # æ°´ç´ çµåˆãƒ‰ãƒŠãƒ¼æ•°
        Descriptors.NumHAcceptors,      # æ°´ç´ çµåˆã‚¢ã‚¯ã‚»ãƒ—ã‚¿ãƒ¼æ•°
        Descriptors.TPSA,               # ãƒˆãƒãƒ­ã‚¸ã‚«ãƒ«æ¥µæ€§è¡¨é¢ç©
        Descriptors.MolLogP,            # åˆ†é…ä¿‚æ•°
        Descriptors.NumRotatableBonds,  # å›è»¢å¯èƒ½çµåˆæ•°
        Descriptors.NumAromaticRings,   # èŠ³é¦™ç’°æ•°
        Descriptors.NumSaturatedRings,  # é£½å’Œç’°æ•°
        Descriptors.NumAliphaticRings,  # è„‚è‚ªæ—ç’°æ•°
        Descriptors.RingCount,          # ç’°æ•°
        Descriptors.NumHeteroatoms,     # ãƒ˜ãƒ†ãƒ­åŸå­æ•°
        Descriptors.FractionCSP3,       # sp3ç‚­ç´ ã®å‰²åˆ
        Descriptors.BalabanJ,           # Balaban JæŒ‡æ•°
        Descriptors.BertzCT,            # Bertzåˆ†å­è¤‡é›‘åº¦
        Descriptors.Chi0,               # åˆ†å­é€£çµæ€§æŒ‡æ•° 0æ¬¡
        Descriptors.Chi1,               # åˆ†å­é€£çµæ€§æŒ‡æ•° 1æ¬¡
        Descriptors.Chi0n,              # æ­£è¦åŒ–åˆ†å­é€£çµæ€§æŒ‡æ•° 0æ¬¡
        Descriptors.Chi1n,              # æ­£è¦åŒ–åˆ†å­é€£çµæ€§æŒ‡æ•° 1æ¬¡
        Descriptors.HallKierAlpha,      # Hall-Kier Î±
        Descriptors.Kappa1,             # Kappaå½¢çŠ¶æŒ‡æ•° 1
        Descriptors.Kappa2,             # Kappaå½¢çŠ¶æŒ‡æ•° 2
        Descriptors.Kappa3,             # Kappaå½¢çŠ¶æŒ‡æ•° 3
        Descriptors.LabuteASA,          # Labuteæ¥è§¦é¢ç©
        Descriptors.PEOE_VSA1,          # éƒ¨åˆ†é›»è·åŠ é‡è¡¨é¢ç© 1
        Descriptors.SMR_VSA1,           # SMR åŠ é‡è¡¨é¢ç© 1
        Descriptors.SlogP_VSA1,         # SlogP åŠ é‡è¡¨é¢ç© 1
        Descriptors.EState_VSA1,        # EState åŠ é‡è¡¨é¢ç© 1
        Descriptors.VSA_EState1,        # VSA EState 1
        Descriptors.Ipc,                # æƒ…å ±å«æœ‰é‡
        Descriptors.BertzCT            # å†åº¦Bertzè¤‡é›‘åº¦
    ]
    
    for desc_func in basic_descriptors:
        try:
            features.append(desc_func(mol))
        except:
            features.append(0)
    
    # Morganãƒ•ã‚£ãƒ³ã‚¬ãƒ¼ãƒ—ãƒªãƒ³ãƒˆï¼ˆ70å€‹ã®ãƒ“ãƒƒãƒˆï¼‰
    try:
        morgan_fp = rdMolDescriptors.GetMorganFingerprintAsBitVect(mol, 2, nBits=70)
        features.extend(list(morgan_fp))
    except:
        features.extend([0] * 70)
    
    return features[:100]  # 100å€‹ã®ç‰¹å¾´é‡ã«åˆ¶é™

# ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
def feature_engineering(df):
    """ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°"""
    print("ğŸ§¬ åˆ†å­ç‰¹å¾´é‡ç”Ÿæˆä¸­...")
    
    if rdkit_available:
        print("  RDKitãƒ™ãƒ¼ã‚¹åˆ†å­è¨˜è¿°å­ã¨ãƒ•ã‚£ãƒ³ã‚¬ãƒ¼ãƒ—ãƒªãƒ³ãƒˆã‚’ä½¿ç”¨")
        features_list = []
        for i, smiles in enumerate(df['SMILES']):
            if i % 1000 == 0:
                print(f"  é€²æ—: {i}/{len(df)}")
            features = rdkit_molecular_features(smiles)
            features_list.append(features)
        
        feature_names = [f'rdkit_feature_{i}' for i in range(100)]
    else:
        print("  åŸºæœ¬SMILESç‰¹å¾´é‡ã‚’ä½¿ç”¨")
        features_list = []
        for smiles in df['SMILES']:
            features = basic_smiles_features(smiles)
            features_list.append(features)
        
        feature_names = [
            'smiles_length', 'carbon_count', 'nitrogen_count', 'oxygen_count', 'sulfur_count',
            'phosphorus_count', 'fluorine_count', 'chlorine_count', 'double_bond_count', 
            'triple_bond_count', 'branch_count', 'special_atom_count', 'chiral_count',
            'aromatic_carbon_count', 'aromatic_bond_count', 'single_bond_count'
        ]
    
    features_df = pd.DataFrame(features_list, columns=feature_names)
    
    print(f"âœ… ç‰¹å¾´é‡ç”Ÿæˆå®Œäº†: {features_df.shape[1]}å€‹ã®ç‰¹å¾´é‡")
    return features_df

# ãƒ¢ãƒ‡ãƒ«è¨“ç·´ã¨ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«
def train_models_for_target(X, y, target_name, n_splits=3):
    """ç‰¹å®šã®ç‰¹æ€§ã«å¯¾ã™ã‚‹ãƒ¢ãƒ‡ãƒ«è¨“ç·´ã¨ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«"""
    print(f"\nğŸ¤– {target_name}ç”¨ã®é«˜åº¦ãªãƒ¢ãƒ‡ãƒ«è¨“ç·´ä¸­...")
    
    kf = KFold(n_splits=n_splits, shuffle=True, random_state=SEED)
    
    # ãƒ¢ãƒ‡ãƒ«å®šç¾©ï¼ˆãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´ç‰ˆï¼‰
    models = {
        'XGBoost': xgb.XGBRegressor(
            n_estimators=200, max_depth=8, learning_rate=0.05,
            subsample=0.8, colsample_bytree=0.8, random_state=SEED, n_jobs=-1
        ),
        'CatBoost': CatBoostRegressor(
            iterations=200, depth=7, learning_rate=0.08,
            random_seed=SEED, verbose=False
        ),
        'RandomForest': RandomForestRegressor(
            n_estimators=300, max_depth=15, random_state=SEED, n_jobs=-1
        ),
        'GradientBoosting': GradientBoostingRegressor(
            n_estimators=200, max_depth=8, learning_rate=0.1, random_state=SEED
        ),
        'KNN': KNeighborsRegressor(n_neighbors=10, n_jobs=-1)
    }
    
    cv_results = {}
    fold_predictions = {model_name: [] for model_name in models.keys()}
    fold_true_values = []
    
    # ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
    for fold, (train_idx, val_idx) in enumerate(kf.split(X)):
        X_train_fold = X.iloc[train_idx]
        X_val_fold = X.iloc[val_idx]
        y_train_fold = y.iloc[train_idx]
        y_val_fold = y.iloc[val_idx]
        
        # ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†
        scaler = StandardScaler()
        X_train_scaled = pd.DataFrame(
            scaler.fit_transform(X_train_fold), 
            columns=X_train_fold.columns, 
            index=X_train_fold.index
        )
        X_val_scaled = pd.DataFrame(
            scaler.transform(X_val_fold), 
            columns=X_val_fold.columns, 
            index=X_val_fold.index
        )
        
        fold_true_values.append(y_val_fold)
        
        for model_name, model in models.items():
            # ãƒ¢ãƒ‡ãƒ«è¨“ç·´
            model.fit(X_train_scaled, y_train_fold)
            y_pred = model.predict(X_val_scaled)
            fold_predictions[model_name].append(y_pred)
            
            mae = mean_absolute_error(y_val_fold, y_pred)
            
            if model_name not in cv_results:
                cv_results[model_name] = {'cv_scores': []}
            
            cv_results[model_name]['cv_scores'].append(mae)
            print(f"    ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰ {fold+1} {model_name} MAE: {mae:.6f}")
    
    # å„ãƒ¢ãƒ‡ãƒ«ã®å¹³å‡æ€§èƒ½ã‚’è¨ˆç®—
    for model_name in models.keys():
        avg_mae = np.mean(cv_results[model_name]['cv_scores'])
        std_mae = np.std(cv_results[model_name]['cv_scores'])
        cv_results[model_name]['cv_mae'] = avg_mae
        cv_results[model_name]['cv_std'] = std_mae
        print(f"  {model_name} å¹³å‡ CV MAE: {avg_mae:.6f} (Â±{std_mae:.6f})")
    
    # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«äºˆæ¸¬ã®è¨ˆç®—
    print(f"\n  ğŸ¯ {target_name}ã®ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«äºˆæ¸¬ã‚’è¨ˆç®—ä¸­...")
    
    # åŠ é‡å¹³å‡ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ï¼ˆæ€§èƒ½ãƒ™ãƒ¼ã‚¹ã®é‡ã¿ï¼‰
    model_weights = {}
    total_inv_mae = sum(1.0 / cv_results[model]['cv_mae'] for model in models.keys())
    for model in models.keys():
        model_weights[model] = (1.0 / cv_results[model]['cv_mae']) / total_inv_mae
    
    print(f"  ğŸ“Š æœ€é©åŒ–ã•ã‚ŒãŸé‡ã¿:")
    for model, weight in model_weights.items():
        print(f"    {model}: {weight:.4f}")
    
    # å…¨ãƒ‡ãƒ¼ã‚¿ã§æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´
    print(f"\n  ğŸ’¾ {target_name}ã®æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ä¸­...")
    scaler = StandardScaler()
    X_scaled = pd.DataFrame(
        scaler.fit_transform(X), 
        columns=X.columns, 
        index=X.index
    )
    
    trained_models = {}
    for model_name, model in models.items():
        model.fit(X_scaled, y)
        trained_models[model_name] = model
    
    return trained_models, scaler, model_weights

# äºˆæ¸¬é–¢æ•°
def predict_ensemble(models, scaler, weights, X):
    """ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«äºˆæ¸¬"""
    X_scaled = pd.DataFrame(
        scaler.transform(X),
        columns=X.columns,
        index=X.index
    )
    
    predictions = {}
    for model_name, model in models.items():
        predictions[model_name] = model.predict(X_scaled)
    
    # åŠ é‡å¹³å‡
    ensemble_pred = sum(
        predictions[model_name] * weights[model_name] 
        for model_name in models.keys()
    )
    
    return ensemble_pred

# ãƒ¡ã‚¤ãƒ³å‡¦ç†
def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    start_time = time.time()
    
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    train, test, submission = load_data()
    
    # ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
    train_features = feature_engineering(train)
    test_features = feature_engineering(test)
    
    # ç‰¹æ€§åˆ—ã®ç‰¹å®š
    target_columns = [col for col in train.columns if col not in ['SMILES', 'Id', 'id']]
    print(f"\nğŸ¯ å¯¾è±¡ç‰¹æ€§: {target_columns}")
    
    # å„ç‰¹æ€§ã«å¯¾ã—ã¦ãƒ¢ãƒ‡ãƒ«è¨“ç·´ã¨äºˆæ¸¬
    all_predictions = {}
    
    for target_col in target_columns:
        if target_col in train.columns:
            # æ¬ æå€¤é™¤å»
            valid_mask = ~train[target_col].isna()
            if valid_mask.sum() < 10:
                print(f"âš ï¸  {target_col}: ãƒ‡ãƒ¼ã‚¿ä¸è¶³ï¼ˆ{valid_mask.sum()}ä»¶ï¼‰- ã‚¹ã‚­ãƒƒãƒ—")
                # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’è¨­å®š
                all_predictions[target_col] = np.zeros(len(test))
                continue
            
            X_valid = train_features[valid_mask]
            y_valid = train[target_col][valid_mask]
            
            print(f"\nğŸ“Š {target_col} - æœ‰åŠ¹ãƒ‡ãƒ¼ã‚¿: {len(X_valid)}ä»¶")
            
            # ãƒ¢ãƒ‡ãƒ«è¨“ç·´ã¨ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«
            trained_models, scaler, weights = train_models_for_target(
                X_valid, y_valid, target_col, n_splits=3
            )
            
            # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®äºˆæ¸¬
            predictions = predict_ensemble(trained_models, scaler, weights, test_features)
            all_predictions[target_col] = predictions
    
    # æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
    print("\nğŸ“ æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆä¸­...")
    for col in submission.columns:
        if col != 'Id' and col in all_predictions:
            submission[col] = all_predictions[col]
    
    # ä¿å­˜
    submission.to_csv('submission.csv', index=False)
    
    elapsed_time = time.time() - start_time
    print(f"\nâ±ï¸  ç·å®Ÿè¡Œæ™‚é–“: {elapsed_time:.2f} ç§’")
    print("ğŸ‰ ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«äºˆæ¸¬å®Œäº†!")
    
    return submission

# å®Ÿè¡Œ
if __name__ == "__main__":
    submission = main()
    print("\næå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆå®Œäº†ï¼")
    print(submission.head())
{
 "cells": [
  {
   "cell_type": "code",
   "id": "894a11b7",
   "metadata": {},
   "outputs": [],
   "source": "# NeurIPS Open Polymer Prediction 2025 - Baseline Model\n# Kaggleç’°å¢ƒç”¨ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«ï¼ˆåŸºæœ¬çš„ãªMLæ‰‹æ³•ï¼‰\n\n# ä¾å­˜é–¢ä¿‚ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆKaggleç’°å¢ƒã§ã¯é€šå¸¸ä¸è¦ã ãŒå¿µã®ãŸã‚ï¼‰\nimport subprocess\nimport sys\n\ndef install_if_needed(package_name, import_name=None):\n    \"\"\"å¿…è¦ã«å¿œã˜ã¦ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\"\"\"\n    if import_name is None:\n        import_name = package_name\n    \n    try:\n        __import__(import_name)\n        print(f\"âœ… {package_name} ã¯æ—¢ã«ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ¸ˆã¿\")\n    except ImportError:\n        print(f\"ğŸ“¦ {package_name} ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ä¸­...\")\n        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package_name, \"--quiet\"])\n        print(f\"âœ… {package_name} ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å®Œäº†\")\n\n# åŸºæœ¬ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ç¢ºèª\ninstall_if_needed(\"xgboost\")\n\nimport numpy as np\nimport pandas as pd\n\n# å…¥åŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ç¢ºèª\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))"
  },
  {
   "cell_type": "code",
   "id": "4f40ffac",
   "metadata": {},
   "outputs": [],
   "source": "# åŸºæœ¬çš„ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ç”¨ï¼‰\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LinearRegression\nimport xgboost as xgb\nimport time\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprint(\"ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ç”¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚¤ãƒ³ãƒãƒ¼ãƒˆå®Œäº†\")"
  },
  {
   "cell_type": "code",
   "id": "42f9b638",
   "metadata": {},
   "outputs": [],
   "source": "# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨åŸºæœ¬åˆ†æï¼ˆãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ç‰ˆï¼‰\nprint(f\"ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒãƒªãƒãƒ¼ç‰¹æ€§äºˆæ¸¬ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³é–‹å§‹...\")\nprint(f\"å®Ÿè¡Œæ—¥æ™‚: 2025-06-24\")\nstart_time = time.time()\n\n# å†ç¾æ€§ã®ãŸã‚ã®ãƒ©ãƒ³ãƒ€ãƒ ã‚·ãƒ¼ãƒ‰è¨­å®š\nSEED = 42\nnp.random.seed(SEED)\n\n# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆKaggleç’°å¢ƒç”¨ãƒ‘ã‚¹ï¼‰\ntrain = pd.read_csv('/kaggle/input/neurips-open-polymer-prediction-2025/train.csv')\ntest = pd.read_csv('/kaggle/input/neurips-open-polymer-prediction-2025/test.csv')\nsubmission = pd.read_csv('/kaggle/input/neurips-open-polymer-prediction-2025/sample_submission.csv')\n\nprint(f\"è¨“ç·´ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {train.shape}\")\nprint(f\"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {test.shape}\")\n\n# æ¬ æå€¤ç¢ºèª\nprint(\"\\nè¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®æ¬ æå€¤:\")\nmissing_values = train.isnull().sum()\nprint(missing_values)\n\n# ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ—\ntarget_cols = ['Tg', 'FFV', 'Tc', 'Density', 'Rg']\n\n# å„ç‰¹æ€§ã®åˆ©ç”¨å¯èƒ½ã‚µãƒ³ãƒ—ãƒ«æ•°è¨ˆç®—\navailable_samples = {col: train.shape[0] - missing_values[col] for col in target_cols}\nprint(\"\\nå„ç‰¹æ€§ã®åˆ©ç”¨å¯èƒ½ã‚µãƒ³ãƒ—ãƒ«æ•°:\")\nfor col, count in available_samples.items():\n    print(f\"{col}: {count} ({count/train.shape[0]*100:.2f}%)\")\n\n# ã‚¿ãƒ¼ã‚²ãƒƒãƒˆç‰¹æ€§ã®çµ±è¨ˆæƒ…å ±\nprint(\"\\nã‚¿ãƒ¼ã‚²ãƒƒãƒˆç‰¹æ€§çµ±è¨ˆ:\")\nprint(train[target_cols].describe())"
  },
  {
   "cell_type": "code",
   "id": "53166e9f",
   "metadata": {},
   "outputs": [],
   "source": "# åŸºæœ¬çš„ãªç‰¹å¾´é‡æŠ½å‡ºé–¢æ•°ï¼ˆãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ç‰ˆï¼‰\ndef create_basic_features(df):\n    \"\"\"SMILESæ–‡å­—åˆ—ã‹ã‚‰åŸºæœ¬çš„ãªç‰¹å¾´é‡ã‚’ä½œæˆï¼ˆãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ç‰ˆï¼‰\"\"\"\n    features = pd.DataFrame()\n    \n    # æ–‡å­—åˆ—é•·\n    features['smiles_length'] = df['SMILES'].str.len()\n    \n    # åŸºæœ¬çš„ãªåŸå­ã‚«ã‚¦ãƒ³ãƒˆ\n    features['carbon_count'] = df['SMILES'].str.count('C')\n    features['nitrogen_count'] = df['SMILES'].str.count('N') \n    features['oxygen_count'] = df['SMILES'].str.count('O')\n    features['sulfur_count'] = df['SMILES'].str.count('S')\n    features['fluorine_count'] = df['SMILES'].str.count('F')\n    \n    # çµåˆæƒ…å ±\n    features['single_bond_count'] = df['SMILES'].str.count('-')\n    features['double_bond_count'] = df['SMILES'].str.count('=')\n    features['triple_bond_count'] = df['SMILES'].str.count('#')\n    \n    # ç’°æ§‹é€ \n    features['ring_count'] = df['SMILES'].str.count('1') + df['SMILES'].str.count('2')\n    features['aromatic_count'] = df['SMILES'].str.count('c')\n    \n    # åˆ†å²æ§‹é€ \n    features['branch_count'] = df['SMILES'].str.count('(')\n    \n    # åŸºæœ¬æ¯”ç‡\n    features['carbon_ratio'] = features['carbon_count'] / features['smiles_length']\n    features['hetero_ratio'] = (features['nitrogen_count'] + features['oxygen_count'] + features['sulfur_count']) / features['smiles_length']\n    \n    return features\n\nprint(\"åŸºæœ¬ç‰¹å¾´é‡æŠ½å‡ºé–¢æ•°å®šç¾©å®Œäº†ï¼ˆãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ç‰ˆï¼‰\")"
  },
  {
   "cell_type": "code",
   "id": "fe8639b4",
   "metadata": {},
   "outputs": [],
   "source": "# åŸºæœ¬ç‰¹å¾´é‡ç”Ÿæˆï¼ˆãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ç‰ˆï¼‰\nprint(\"\\nåŸºæœ¬ç‰¹å¾´é‡ç”Ÿæˆä¸­...\")\n\n# è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ç‰¹å¾´é‡ç”Ÿæˆ\ntrain_features = create_basic_features(train)\ntest_features = create_basic_features(test)\n\nprint(f\"ç‰¹å¾´é‡ç”Ÿæˆå®Œäº†:\")\nprint(f\"- è¨“ç·´ãƒ‡ãƒ¼ã‚¿ç‰¹å¾´é‡: {train_features.shape}\")\nprint(f\"- ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç‰¹å¾´é‡: {test_features.shape}\")\nprint(f\"- ç‰¹å¾´é‡æ•°: {train_features.shape[1]}\")\n\n# ç‰¹å¾´é‡ä¸€è¦§è¡¨ç¤º\nprint(f\"\\nç‰¹å¾´é‡ä¸€è¦§: {list(train_features.columns)}\")\n\n# åŸºæœ¬çµ±è¨ˆ\nprint(\"\\nç‰¹å¾´é‡åŸºæœ¬çµ±è¨ˆ:\")\nprint(train_features.describe())"
  },
  {
   "cell_type": "code",
   "id": "13a8fa69",
   "metadata": {},
   "outputs": [],
   "source": "# ãƒ‡ãƒ¼ã‚¿æº–å‚™ã¨ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°\nprint(\"\\nãƒ‡ãƒ¼ã‚¿æº–å‚™ä¸­...\")\n\n# ç‰¹å¾´é‡ã‚’numpy arrayã«å¤‰æ›\nX_train = train_features.values\nX_test = test_features.values\n\n# æ¬ æå€¤ã‚’0ã§è£œå®Œï¼ˆåŸºæœ¬çš„ãªæ‰‹æ³•ï¼‰\nX_train = np.nan_to_num(X_train, nan=0.0)\nX_test = np.nan_to_num(X_test, nan=0.0)\n\n# æ¨™æº–åŒ–\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nprint(f\"ãƒ‡ãƒ¼ã‚¿æº–å‚™å®Œäº†:\")\nprint(f\"- è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {X_train_scaled.shape}\")\nprint(f\"- ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {X_test_scaled.shape}\")"
  },
  {
   "cell_type": "code",
   "id": "7254dced",
   "metadata": {},
   "outputs": [],
   "source": "# ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«å®šç¾©\ndef train_baseline_model(X, y, target_name):\n    \"\"\"ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ï¼ˆã‚·ãƒ³ãƒ—ãƒ«ãªXGBoostï¼‰\"\"\"\n    \n    # æ¬ æå€¤ã®ãªã„è¡Œã«ãƒ•ã‚£ãƒ«ã‚¿\n    valid_idx = ~np.isnan(y)\n    X_valid = X[valid_idx]\n    y_valid = y[valid_idx]\n    \n    if len(y_valid) < 10:\n        print(f\"âš ï¸ {target_name}: ãƒ‡ãƒ¼ã‚¿ä¸è¶³ ({len(y_valid)} ã‚µãƒ³ãƒ—ãƒ«)\")\n        return None, 0.0\n    \n    print(f\"ğŸ”„ {target_name} ãƒ¢ãƒ‡ãƒ«è¨“ç·´ä¸­... ({len(y_valid)} ã‚µãƒ³ãƒ—ãƒ«)\")\n    \n    # 5-Fold CV\n    kf = KFold(n_splits=5, shuffle=True, random_state=SEED)\n    cv_scores = []\n    models = []\n    \n    for fold, (train_idx, val_idx) in enumerate(kf.split(X_valid)):\n        X_train_fold, X_val_fold = X_valid[train_idx], X_valid[val_idx]\n        y_train_fold, y_val_fold = y_valid[train_idx], y_valid[val_idx]\n        \n        # ã‚·ãƒ³ãƒ—ãƒ«ãªXGBoostãƒ¢ãƒ‡ãƒ«\n        model = xgb.XGBRegressor(\n            n_estimators=100,\n            max_depth=6,\n            learning_rate=0.1,\n            random_state=SEED+fold,\n            verbosity=0\n        )\n        model.fit(X_train_fold, y_train_fold)\n        \n        # æ¤œè¨¼\n        val_pred = model.predict(X_val_fold)\n        score = mean_absolute_error(y_val_fold, val_pred)\n        cv_scores.append(score)\n        models.append(model)\n        \n        print(f\"  Fold {fold+1}: MAE = {score:.4f}\")\n    \n    avg_score = np.mean(cv_scores)\n    print(f\"âœ… {target_name} CV MAE: {avg_score:.4f}\")\n    \n    return models, avg_score\n\nprint(\"ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«å®šç¾©å®Œäº†\")"
  },
  {
   "cell_type": "code",
   "id": "ac648258",
   "metadata": {},
   "outputs": [],
   "source": "# å…¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆç‰¹æ€§ã®ãƒ¢ãƒ‡ãƒ«è¨“ç·´ï¼ˆãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼‰\nprint(\"ğŸš€ ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«è¨“ç·´é–‹å§‹\")\n\n# å„ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®ãƒ¢ãƒ‡ãƒ«è¨“ç·´\ntrained_models = {}\ncv_scores = {}\n\nfor target_col in target_cols:\n    y = train[target_col].values\n    models, score = train_baseline_model(X_train_scaled, y, target_col)\n    trained_models[target_col] = models\n    cv_scores[target_col] = score\n\nprint(\"\\nğŸ“Š ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³çµæœ:\")\nfor target_col, score in cv_scores.items():\n    print(f\"{target_col}: {score:.4f}\")\n\noverall_score = np.mean([s for s in cv_scores.values() if s > 0])\nprint(f\"\\nğŸ¯ å…¨ä½“å¹³å‡MAE: {overall_score:.4f}\")\n\nprint(\"\\nãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«è¨“ç·´å®Œäº†\")"
  },
  {
   "cell_type": "code",
   "id": "41165125",
   "metadata": {},
   "outputs": [],
   "source": "# ãƒ†ã‚¹ãƒˆäºˆæ¸¬ã¨ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†\nprint(\"ğŸ”® ãƒ†ã‚¹ãƒˆäºˆæ¸¬ä¸­...\")\n\ntest_predictions = {}\n\nfor target_col in target_cols:\n    if trained_models[target_col] is not None:\n        # å„ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰ã®äºˆæ¸¬ã‚’å¹³å‡\n        fold_preds = []\n        for model in trained_models[target_col]:\n            pred = model.predict(X_test_scaled)\n            fold_preds.append(pred)\n        \n        test_predictions[target_col] = np.mean(fold_preds, axis=0)\n        print(f\"âœ… {target_col} äºˆæ¸¬å®Œäº†\")\n    else:\n        # ãƒ‡ãƒ¼ã‚¿ä¸è¶³ã®å ´åˆã¯è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®å¹³å‡å€¤ã‚’ä½¿ç”¨\n        mean_val = train[target_col].mean()\n        if np.isnan(mean_val):\n            # å®Œå…¨ã«æ¬ æã—ã¦ã„ã‚‹å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å€¤\n            fallback_values = {\n                'Tg': 400,\n                'FFV': 0.2,\n                'Tc': 0.2,\n                'Density': 1.0,\n                'Rg': 10.0\n            }\n            mean_val = fallback_values.get(target_col, 0.0)\n        \n        test_predictions[target_col] = np.full(len(test), mean_val)\n        print(f\"âš ï¸ {target_col} å¹³å‡å€¤ã§äºˆæ¸¬ (å€¤: {mean_val:.3f})\")\n\nprint(\"ãƒ†ã‚¹ãƒˆäºˆæ¸¬å®Œäº†\")"
  },
  {
   "cell_type": "code",
   "id": "6b2340fd",
   "metadata": {},
   "outputs": [],
   "source": "# æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ\nprint(\"ğŸ“„ æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆä¸­...\")\n\n# æå‡ºãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ä½œæˆ\nsubmission_final = pd.DataFrame({'id': test['id']})\nfor target_col in target_cols:\n    submission_final[target_col] = test_predictions[target_col]\n\n# æå‡ºãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆç¢ºèª\nprint(\"æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼:\")\nprint(submission_final.head())\n\nprint(f\"\\næå‡ºãƒ•ã‚¡ã‚¤ãƒ«çµ±è¨ˆ:\")\nfor target_col in target_cols:\n    values = submission_final[target_col]\n    print(f\"{target_col}: å¹³å‡={values.mean():.3f}, æ¨™æº–åå·®={values.std():.3f}\")\n\n# æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜\nsubmission_final.to_csv('submission.csv', index=False)\nprint(\"\\nâœ… æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜å®Œäº†: submission.csv\")\n\nelapsed_time = time.time() - start_time\nprint(f\"\\nâ±ï¸ ç·å®Ÿè¡Œæ™‚é–“: {elapsed_time/60:.2f} åˆ†\")\n\nprint(\"\\nğŸ‰ ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«å®Ÿè¡Œå®Œäº†ï¼\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
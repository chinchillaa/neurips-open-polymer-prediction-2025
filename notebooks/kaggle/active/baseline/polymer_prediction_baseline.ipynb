{
 "cells": [
  {
   "cell_type": "code",
   "id": "894a11b7",
   "metadata": {},
   "outputs": [],
   "source": "# NeurIPS Open Polymer Prediction 2025 - Baseline Model\n# Kaggle環境用ベースラインモデル（基本的なML手法）\n\n# 依存関係インストール（Kaggle環境では通常不要だが念のため）\nimport subprocess\nimport sys\n\ndef install_if_needed(package_name, import_name=None):\n    \"\"\"必要に応じてパッケージをインストール\"\"\"\n    if import_name is None:\n        import_name = package_name\n    \n    try:\n        __import__(import_name)\n        print(f\"✅ {package_name} は既にインストール済み\")\n    except ImportError:\n        print(f\"📦 {package_name} をインストール中...\")\n        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package_name, \"--quiet\"])\n        print(f\"✅ {package_name} インストール完了\")\n\n# 基本パッケージのインストール確認\ninstall_if_needed(\"xgboost\")\n\nimport numpy as np\nimport pandas as pd\n\n# 入力ディレクトリの確認\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))"
  },
  {
   "cell_type": "code",
   "id": "4f40ffac",
   "metadata": {},
   "outputs": [],
   "source": "# 基本的なライブラリのインポート（ベースライン用）\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LinearRegression\nimport xgboost as xgb\nimport time\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprint(\"ベースライン用ライブラリインポート完了\")"
  },
  {
   "cell_type": "code",
   "id": "42f9b638",
   "metadata": {},
   "outputs": [],
   "source": "# データ読み込みと基本分析（ベースライン版）\nprint(f\"ベースラインポリマー特性予測パイプライン開始...\")\nprint(f\"実行日時: 2025-06-24\")\nstart_time = time.time()\n\n# 再現性のためのランダムシード設定\nSEED = 42\nnp.random.seed(SEED)\n\n# データ読み込み（Kaggle環境用パス）\ntrain = pd.read_csv('/kaggle/input/neurips-open-polymer-prediction-2025/train.csv')\ntest = pd.read_csv('/kaggle/input/neurips-open-polymer-prediction-2025/test.csv')\nsubmission = pd.read_csv('/kaggle/input/neurips-open-polymer-prediction-2025/sample_submission.csv')\n\nprint(f\"訓練データ形状: {train.shape}\")\nprint(f\"テストデータ形状: {test.shape}\")\n\n# 欠損値確認\nprint(\"\\n訓練データの欠損値:\")\nmissing_values = train.isnull().sum()\nprint(missing_values)\n\n# ターゲット列\ntarget_cols = ['Tg', 'FFV', 'Tc', 'Density', 'Rg']\n\n# 各特性の利用可能サンプル数計算\navailable_samples = {col: train.shape[0] - missing_values[col] for col in target_cols}\nprint(\"\\n各特性の利用可能サンプル数:\")\nfor col, count in available_samples.items():\n    print(f\"{col}: {count} ({count/train.shape[0]*100:.2f}%)\")\n\n# ターゲット特性の統計情報\nprint(\"\\nターゲット特性統計:\")\nprint(train[target_cols].describe())"
  },
  {
   "cell_type": "code",
   "id": "53166e9f",
   "metadata": {},
   "outputs": [],
   "source": "# 基本的な特徴量抽出関数（ベースライン版）\ndef create_basic_features(df):\n    \"\"\"SMILES文字列から基本的な特徴量を作成（ベースライン版）\"\"\"\n    features = pd.DataFrame()\n    \n    # 文字列長\n    features['smiles_length'] = df['SMILES'].str.len()\n    \n    # 基本的な原子カウント\n    features['carbon_count'] = df['SMILES'].str.count('C')\n    features['nitrogen_count'] = df['SMILES'].str.count('N') \n    features['oxygen_count'] = df['SMILES'].str.count('O')\n    features['sulfur_count'] = df['SMILES'].str.count('S')\n    features['fluorine_count'] = df['SMILES'].str.count('F')\n    \n    # 結合情報\n    features['single_bond_count'] = df['SMILES'].str.count('-')\n    features['double_bond_count'] = df['SMILES'].str.count('=')\n    features['triple_bond_count'] = df['SMILES'].str.count('#')\n    \n    # 環構造\n    features['ring_count'] = df['SMILES'].str.count('1') + df['SMILES'].str.count('2')\n    features['aromatic_count'] = df['SMILES'].str.count('c')\n    \n    # 分岐構造\n    features['branch_count'] = df['SMILES'].str.count('(')\n    \n    # 基本比率\n    features['carbon_ratio'] = features['carbon_count'] / features['smiles_length']\n    features['hetero_ratio'] = (features['nitrogen_count'] + features['oxygen_count'] + features['sulfur_count']) / features['smiles_length']\n    \n    return features\n\nprint(\"基本特徴量抽出関数定義完了（ベースライン版）\")"
  },
  {
   "cell_type": "code",
   "id": "fe8639b4",
   "metadata": {},
   "outputs": [],
   "source": "# 基本特徴量生成（ベースライン版）\nprint(\"\\n基本特徴量生成中...\")\n\n# 訓練データとテストデータの特徴量生成\ntrain_features = create_basic_features(train)\ntest_features = create_basic_features(test)\n\nprint(f\"特徴量生成完了:\")\nprint(f\"- 訓練データ特徴量: {train_features.shape}\")\nprint(f\"- テストデータ特徴量: {test_features.shape}\")\nprint(f\"- 特徴量数: {train_features.shape[1]}\")\n\n# 特徴量一覧表示\nprint(f\"\\n特徴量一覧: {list(train_features.columns)}\")\n\n# 基本統計\nprint(\"\\n特徴量基本統計:\")\nprint(train_features.describe())"
  },
  {
   "cell_type": "code",
   "id": "13a8fa69",
   "metadata": {},
   "outputs": [],
   "source": "# データ準備とスケーリング\nprint(\"\\nデータ準備中...\")\n\n# 特徴量をnumpy arrayに変換\nX_train = train_features.values\nX_test = test_features.values\n\n# 欠損値を0で補完（基本的な手法）\nX_train = np.nan_to_num(X_train, nan=0.0)\nX_test = np.nan_to_num(X_test, nan=0.0)\n\n# 標準化\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nprint(f\"データ準備完了:\")\nprint(f\"- 訓練データ: {X_train_scaled.shape}\")\nprint(f\"- テストデータ: {X_test_scaled.shape}\")"
  },
  {
   "cell_type": "code",
   "id": "7254dced",
   "metadata": {},
   "outputs": [],
   "source": "# ベースラインモデル定義\ndef train_baseline_model(X, y, target_name):\n    \"\"\"ベースラインモデルの訓練（シンプルなXGBoost）\"\"\"\n    \n    # 欠損値のない行にフィルタ\n    valid_idx = ~np.isnan(y)\n    X_valid = X[valid_idx]\n    y_valid = y[valid_idx]\n    \n    if len(y_valid) < 10:\n        print(f\"⚠️ {target_name}: データ不足 ({len(y_valid)} サンプル)\")\n        return None, 0.0\n    \n    print(f\"🔄 {target_name} モデル訓練中... ({len(y_valid)} サンプル)\")\n    \n    # 5-Fold CV\n    kf = KFold(n_splits=5, shuffle=True, random_state=SEED)\n    cv_scores = []\n    models = []\n    \n    for fold, (train_idx, val_idx) in enumerate(kf.split(X_valid)):\n        X_train_fold, X_val_fold = X_valid[train_idx], X_valid[val_idx]\n        y_train_fold, y_val_fold = y_valid[train_idx], y_valid[val_idx]\n        \n        # シンプルなXGBoostモデル\n        model = xgb.XGBRegressor(\n            n_estimators=100,\n            max_depth=6,\n            learning_rate=0.1,\n            random_state=SEED+fold,\n            verbosity=0\n        )\n        model.fit(X_train_fold, y_train_fold)\n        \n        # 検証\n        val_pred = model.predict(X_val_fold)\n        score = mean_absolute_error(y_val_fold, val_pred)\n        cv_scores.append(score)\n        models.append(model)\n        \n        print(f\"  Fold {fold+1}: MAE = {score:.4f}\")\n    \n    avg_score = np.mean(cv_scores)\n    print(f\"✅ {target_name} CV MAE: {avg_score:.4f}\")\n    \n    return models, avg_score\n\nprint(\"ベースラインモデル定義完了\")"
  },
  {
   "cell_type": "code",
   "id": "ac648258",
   "metadata": {},
   "outputs": [],
   "source": "# 全ターゲット特性のモデル訓練（ベースライン）\nprint(\"🚀 ベースラインモデル訓練開始\")\n\n# 各ターゲットのモデル訓練\ntrained_models = {}\ncv_scores = {}\n\nfor target_col in target_cols:\n    y = train[target_col].values\n    models, score = train_baseline_model(X_train_scaled, y, target_col)\n    trained_models[target_col] = models\n    cv_scores[target_col] = score\n\nprint(\"\\n📊 クロスバリデーション結果:\")\nfor target_col, score in cv_scores.items():\n    print(f\"{target_col}: {score:.4f}\")\n\noverall_score = np.mean([s for s in cv_scores.values() if s > 0])\nprint(f\"\\n🎯 全体平均MAE: {overall_score:.4f}\")\n\nprint(\"\\nベースラインモデル訓練完了\")"
  },
  {
   "cell_type": "code",
   "id": "41165125",
   "metadata": {},
   "outputs": [],
   "source": "# テスト予測とフォールバック処理\nprint(\"🔮 テスト予測中...\")\n\ntest_predictions = {}\n\nfor target_col in target_cols:\n    if trained_models[target_col] is not None:\n        # 各フォールドの予測を平均\n        fold_preds = []\n        for model in trained_models[target_col]:\n            pred = model.predict(X_test_scaled)\n            fold_preds.append(pred)\n        \n        test_predictions[target_col] = np.mean(fold_preds, axis=0)\n        print(f\"✅ {target_col} 予測完了\")\n    else:\n        # データ不足の場合は訓練データの平均値を使用\n        mean_val = train[target_col].mean()\n        if np.isnan(mean_val):\n            # 完全に欠損している場合のフォールバック値\n            fallback_values = {\n                'Tg': 400,\n                'FFV': 0.2,\n                'Tc': 0.2,\n                'Density': 1.0,\n                'Rg': 10.0\n            }\n            mean_val = fallback_values.get(target_col, 0.0)\n        \n        test_predictions[target_col] = np.full(len(test), mean_val)\n        print(f\"⚠️ {target_col} 平均値で予測 (値: {mean_val:.3f})\")\n\nprint(\"テスト予測完了\")"
  },
  {
   "cell_type": "code",
   "id": "6b2340fd",
   "metadata": {},
   "outputs": [],
   "source": "# 提出ファイル作成\nprint(\"📄 提出ファイル作成中...\")\n\n# 提出データフレーム作成\nsubmission_final = pd.DataFrame({'id': test['id']})\nfor target_col in target_cols:\n    submission_final[target_col] = test_predictions[target_col]\n\n# 提出フォーマット確認\nprint(\"提出ファイルプレビュー:\")\nprint(submission_final.head())\n\nprint(f\"\\n提出ファイル統計:\")\nfor target_col in target_cols:\n    values = submission_final[target_col]\n    print(f\"{target_col}: 平均={values.mean():.3f}, 標準偏差={values.std():.3f}\")\n\n# 提出ファイル保存\nsubmission_final.to_csv('submission.csv', index=False)\nprint(\"\\n✅ 提出ファイル保存完了: submission.csv\")\n\nelapsed_time = time.time() - start_time\nprint(f\"\\n⏱️ 総実行時間: {elapsed_time/60:.2f} 分\")\n\nprint(\"\\n🎉 ベースラインモデル実行完了！\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
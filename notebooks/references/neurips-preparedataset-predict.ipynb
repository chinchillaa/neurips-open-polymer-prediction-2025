{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":74608,"databundleVersionId":12966160,"sourceType":"competition"},{"sourceId":8368538,"sourceType":"datasetVersion","datasetId":4974788},{"sourceId":12189904,"sourceType":"datasetVersion","datasetId":7678100},{"sourceId":12207625,"sourceType":"datasetVersion","datasetId":7690162},{"sourceId":12237259,"sourceType":"datasetVersion","datasetId":7706066},{"sourceId":12330396,"sourceType":"datasetVersion","datasetId":7709869},{"sourceId":12374819,"sourceType":"datasetVersion","datasetId":7802725},{"sourceId":246691463,"sourceType":"kernelVersion"},{"sourceId":247508515,"sourceType":"kernelVersion"},{"sourceId":247640189,"sourceType":"kernelVersion"},{"sourceId":247698673,"sourceType":"kernelVersion"},{"sourceId":247701857,"sourceType":"kernelVersion"},{"sourceId":248709050,"sourceType":"kernelVersion"},{"sourceId":248709247,"sourceType":"kernelVersion"},{"sourceId":248709264,"sourceType":"kernelVersion"},{"sourceId":248815573,"sourceType":"kernelVersion"},{"sourceId":248815852,"sourceType":"kernelVersion"},{"sourceId":248985767,"sourceType":"kernelVersion"},{"sourceId":248990017,"sourceType":"kernelVersion"},{"sourceId":248990058,"sourceType":"kernelVersion"},{"sourceId":248990099,"sourceType":"kernelVersion"},{"sourceId":248990126,"sourceType":"kernelVersion"},{"sourceId":248990144,"sourceType":"kernelVersion"},{"sourceId":249414884,"sourceType":"kernelVersion"},{"sourceId":249414893,"sourceType":"kernelVersion"},{"sourceId":249414904,"sourceType":"kernelVersion"},{"sourceId":249414917,"sourceType":"kernelVersion"},{"sourceId":249414988,"sourceType":"kernelVersion"},{"sourceId":249472378,"sourceType":"kernelVersion"},{"sourceId":442871,"sourceType":"modelInstanceVersion","modelInstanceId":359690,"modelId":380893}],"dockerImageVersionId":31040,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install /kaggle/input/rdkit-2025-3-3-cp311/rdkit-2025.3.3-cp311-cp311-manylinux_2_28_x86_64.whl\n!pip install mordred --no-index --find-links=file:///kaggle/input/mordred-1-2-0-py3-none-any/","metadata":{"trusted":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2025-07-08T20:30:12.795987Z","iopub.execute_input":"2025-07-08T20:30:12.796586Z","iopub.status.idle":"2025-07-08T20:30:28.649875Z","shell.execute_reply.started":"2025-07-08T20:30:12.796544Z","shell.execute_reply":"2025-07-08T20:30:28.647818Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!cp -r /kaggle/input/autogluon-package/* /kaggle/working/\n!pip install -f --quiet --no-index --find-links='/kaggle/input/autogluon-package' 'autogluon.tabular-1.3.1-py3-none-any.whl'","metadata":{"_uuid":"f781f23d-0d4b-41fd-84b7-2c625e33c17c","_cell_guid":"78622aa3-2760-4cff-9fdf-23edc16288ab","trusted":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2025-07-08T20:30:28.652043Z","iopub.execute_input":"2025-07-08T20:30:28.652349Z","iopub.status.idle":"2025-07-08T20:31:14.225888Z","shell.execute_reply.started":"2025-07-08T20:30:28.652318Z","shell.execute_reply":"2025-07-08T20:31:14.224722Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!cp -r /kaggle/input/scikit-package/* /kaggle/working/\n!pip install -f --quiet --no-index --find-links='/kaggle/input/scikit-package' 'scikit_learn-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl' ","metadata":{"_uuid":"0c7f3e0b-4c53-46e2-887c-b2d69cc1df4e","_cell_guid":"59230434-ef8d-4a10-bc8a-c6f772c2ef63","trusted":true,"execution":{"iopub.status.busy":"2025-07-08T20:31:14.2271Z","iopub.execute_input":"2025-07-08T20:31:14.228008Z","iopub.status.idle":"2025-07-08T20:31:22.054555Z","shell.execute_reply.started":"2025-07-08T20:31:14.22797Z","shell.execute_reply":"2025-07-08T20:31:22.053111Z"},"_kg_hide-input":false,"_kg_hide-output":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!rm -r /kaggle/working/*","metadata":{"_uuid":"910eb67b-dbf1-4af8-a9aa-8a9a8d053bf4","_cell_guid":"0043aac5-6946-43e3-91a7-daaf32e99bed","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-08T20:31:22.056311Z","iopub.execute_input":"2025-07-08T20:31:22.056783Z","iopub.status.idle":"2025-07-08T20:31:22.868887Z","shell.execute_reply.started":"2025-07-08T20:31:22.056716Z","shell.execute_reply":"2025-07-08T20:31:22.867303Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from autogluon.tabular import TabularDataset, TabularPredictor","metadata":{"_uuid":"1422cc40-23e4-40a3-b5d2-5e97a1102fb0","_cell_guid":"fde81767-e059-410f-93a2-af32541d4e45","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-08T20:31:22.872146Z","iopub.execute_input":"2025-07-08T20:31:22.872484Z","iopub.status.idle":"2025-07-08T20:31:24.602988Z","shell.execute_reply.started":"2025-07-08T20:31:22.872452Z","shell.execute_reply":"2025-07-08T20:31:24.601876Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Core Python libraries\nimport os\nimport gc\nimport pickle\nimport warnings\nfrom collections import Counter\nfrom multiprocessing import cpu_count\n\n# Data manipulation and analysis\nimport numpy as np\nimport pandas as pd\nimport polars as pl\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Progress bars and utilities\nfrom tqdm.auto import tqdm\n\n# Machine Learning\nfrom sklearn.cluster import KMeans\nimport hdbscan\n\n# Parallel processing\nfrom joblib import Parallel, delayed\n\n# Deep Learning and Transformers\nimport torch\nfrom transformers import AutoModel, AutoTokenizer\n\n# Chemistry and molecular descriptors\nfrom rdkit import Chem\nfrom rdkit.Chem import AllChem, Descriptors, rdmolops, MACCSkeys, rdMolDescriptors\nfrom mordred import Calculator, descriptors\nimport networkx as nx\n\n# Configuration\nwarnings.filterwarnings(\"ignore\")\n#pd.set_option('display.max_columns', None)","metadata":{"_uuid":"d58aee1a-9865-4dbc-ad00-32b039e73736","_cell_guid":"bcec1f8e-30e1-4f5b-b5dd-6b68bb2c208f","trusted":true,"collapsed":false,"_kg_hide-input":true,"_kg_hide-output":true,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-07-08T20:31:24.604168Z","iopub.execute_input":"2025-07-08T20:31:24.604768Z","iopub.status.idle":"2025-07-08T20:31:41.050945Z","shell.execute_reply.started":"2025-07-08T20:31:24.604718Z","shell.execute_reply":"2025-07-08T20:31:41.049549Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =============================================================================\n# DATA LOADING AND AUGMENTATION FUNCTIONS\n# =============================================================================\n\n\n\ndef make_smile_canonical(smile):\n    \"\"\"Convert SMILES to canonical form to avoid duplicates\"\"\"\n    try:\n        mol = Chem.MolFromSmiles(smile)\n        if mol is None:\n            return np.nan\n        canon_smile = Chem.MolToSmiles(mol, canonical=True)\n        return canon_smile\n    except:\n        return np.nan\n\ndef add_extra_data(df_train, df_extra, target):\n    \"\"\"Add external data for target augmentation\"\"\"\n    n_samples_before = len(df_train[df_train[target].notnull()])\n    \n    # Make copies to avoid modifying original dataframes\n    df_train = df_train.copy()\n    df_extra = df_extra.copy()\n    \n    df_extra['SMILES'] = df_extra['SMILES'].apply(lambda s: make_smile_canonical(s))\n    df_extra = df_extra.groupby('SMILES', as_index=False)[target].mean()\n    \n    cross_smiles = set(df_extra['SMILES']) & set(df_train['SMILES'])\n    unique_smiles_extra = set(df_extra['SMILES']) - set(df_train['SMILES'])\n\n    # Prioritize target values from competition data\n    for smile in df_train[df_train[target].notnull()]['SMILES'].tolist():\n        if smile in cross_smiles:\n            cross_smiles.remove(smile)\n\n    # Impute missing values for competition SMILES\n    for smile in cross_smiles:\n        df_train.loc[df_train['SMILES']==smile, target] = df_extra[df_extra['SMILES']==smile][target].values[0]\n    \n    # Add new SMILES from external data\n    new_data = df_extra[df_extra['SMILES'].isin(unique_smiles_extra)]\n    df_train = pd.concat([df_train, new_data], axis=0, ignore_index=True)\n\n    n_samples_after = len(df_train[df_train[target].notnull()])\n    print(f'  {target}: Added {n_samples_after-n_samples_before} new samples')\n    return df_train\n\ndef load_and_augment_train_data(cfg):\n    \"\"\"Load training data and augment with external datasets\"\"\"\n    print(\"üìÇ Loading and augmenting training data...\")\n    \n    # Load base training data\n    train = pd.read_csv(cfg.PATH + 'train.csv')\n    train['SMILES'] = train['SMILES'].apply(lambda s: make_smile_canonical(s))\n    \n    # Load external datasets\n    print(\"üìÇ Loading external datasets...\")\n    \n    try:\n        # Tc data\n        data_tc = pd.read_csv('/kaggle/input/tc-smiles/Tc_SMILES.csv')\n        data_tc = data_tc.rename(columns={'TC_mean': 'Tc'})\n        train = add_extra_data(train, data_tc, 'Tc')\n    except Exception as e:\n        print(f\"‚ö†Ô∏è Could not load Tc data: {e}\")\n    \n    try:\n        # Tg data sources\n        data_tg2 = pd.read_csv('/kaggle/input/smiles-extra-data/JCIM_sup_bigsmiles.csv', usecols=['SMILES', 'Tg (C)'])\n        data_tg2 = data_tg2.rename(columns={'Tg (C)': 'Tg'})\n        train = add_extra_data(train, data_tg2, 'Tg')\n    except Exception as e:\n        print(f\"‚ö†Ô∏è Could not load Tg data 2: {e}\")\n    \n    try:\n        data_tg3 = pd.read_excel('/kaggle/input/smiles-extra-data/data_tg3.xlsx')\n        data_tg3 = data_tg3.rename(columns={'Tg [K]': 'Tg'})\n        data_tg3['Tg'] = data_tg3['Tg'] - 273.15\n        train = add_extra_data(train, data_tg3, 'Tg')\n    except Exception as e:\n        print(f\"‚ö†Ô∏è Could not load Tg data 3: {e}\")\n    \n    try:\n        # Density data\n        data_dnst = pd.read_excel('/kaggle/input/smiles-extra-data/data_dnst1.xlsx')\n        data_dnst = data_dnst.rename(columns={'density(g/cm3)': 'Density'})[['SMILES', 'Density']]\n        data_dnst['SMILES'] = data_dnst['SMILES'].apply(lambda s: make_smile_canonical(s))\n        data_dnst = data_dnst[(data_dnst['SMILES'].notnull())&(data_dnst['Density'].notnull())&(data_dnst['Density'] != 'nylon')]\n        data_dnst['Density'] = data_dnst['Density'].astype('float64')\n        data_dnst['Density'] -= 0.118\n        train = add_extra_data(train, data_dnst, 'Density')\n    except Exception as e:\n        print(f\"‚ö†Ô∏è Could not load density data: {e}\")\n    \n    print(f\"üìä Final training sample counts:\")\n    for t in cfg.TARGETS:\n        print(f'  {t}: {len(train[train[t].notnull()])} samples')\n    \n    return train\n\ndef load_test_data(cfg):\n    \"\"\"Load test data\"\"\"\n    print(\"üìÇ Loading test data...\")\n    test = pd.read_csv(cfg.PATH + 'test.csv')\n    test['SMILES'] = test['SMILES'].apply(lambda s: make_smile_canonical(s))\n    return test","metadata":{"_uuid":"67abea6b-1af2-4ce7-ba76-dca99db24342","_cell_guid":"d669fbfc-f1de-4ed5-bffe-91b6cc48f121","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-08T20:31:41.052352Z","iopub.execute_input":"2025-07-08T20:31:41.053182Z","iopub.status.idle":"2025-07-08T20:31:41.075788Z","shell.execute_reply.started":"2025-07-08T20:31:41.05315Z","shell.execute_reply":"2025-07-08T20:31:41.074537Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =============================================================================\n# MOLECULAR FEATURE COMPUTATION\n# =============================================================================\n\n\n\n'''\nfeatures['max_degree'] = max([d for n, d in G.degree()]) if n_nodes > 0 else 0\n\nif nx.is_connected(G):\n    closeness = list(nx.closeness_centrality(G).values())\n    features['closeness_mean'] = np.mean(closeness)\nelse:\n    features['closeness_mean'] = 0\n\ntry:\n    katz = list(nx.katz_centrality(G, max_iter=1000).values())\n    features['katz_centrality_std'] = np.std(katz)\nexcept:\n    features['katz_centrality_std'] = 0\n\n\ndef compute_atom_graph_features(G, mol):\n    \"\"\"Atom-specific graph features\"\"\"\n    features = {}\n    \n    try:\n        # Atom type distribution in graph context\n        atom_types = [atom.GetSymbol() for atom in mol.GetAtoms()]\n        atom_counts = Counter(atom_types)\n        \n\n        \n        # Heteroatom ratio\n        total_atoms = len(atom_types)\n        hetero_atoms = total_atoms - atom_counts.get('C', 0)\n        features['heteroatom_ratio'] = hetero_atoms / total_atoms if total_atoms > 0 else 0\n        \n        \n    except Exception:\n        features['heteroatom_ratio'] = 0\n'''\n\n'''\nAtomPair_B512_Bit0138\nAtomPair_B512_Bit0448\nAtomPair_B512_Bit0408 \n\nfingerprints = []\nlegends = [f\"AtomPair_B{n_bits}_Bit{str(n).zfill(4)}\" for n in range(n_bits)]\n\nprint(f\"üß¨ Computing atom pair fingerprints ({n_bits} bits) for {len(smiles_list)} molecules...\")\n\nfor i, smiles in enumerate(tqdm(smiles_list, desc=\"Computing fingerprints\")):\n    mol = Chem.MolFromSmiles(smiles)\n    if not mol:\n        # Handle invalid molecules\n        fingerprint = [0] * n_bits\n    else:\n        # Generate atom pair fingerprint\n        fp = rdMolDescriptors.GetHashedAtomPairFingerprintAsBitVect(mol, nBits=n_bits)\n        fingerprint = [int(x) for x in fp.ToBitString()]\n    \n    fingerprints.append(np.array(fingerprint))\n\n'''\n\n# =============================================================================\n# MOLECULAR FEATURE COMPUTATION\n# =============================================================================\n\n\n\ndef compute_molecular_features_for_smiles(smiles, useless_cols):\n    \"\"\"Compute all molecular features for a single SMILES\"\"\"\n    try:\n        # Initialize results dictionary\n        results = {}\n        \n        # Compute RDKit descriptors\n        mol = Chem.MolFromSmiles(smiles)\n        if mol is None:\n            desc_names = [desc[0] for desc in Descriptors.descList if desc[0] not in useless_cols]\n            results.update({name: None for name in desc_names})\n        else:\n            desc_results = [desc[1](mol) for desc in Descriptors.descList if desc[0] not in useless_cols]\n            desc_names = [desc[0] for desc in Descriptors.descList if desc[0] not in useless_cols]\n            results.update(dict(zip(desc_names, desc_results)))\n        \n        # Compute graph features\n        graph_defaults = {\n            'graph_diameter': 0, 'avg_shortest_path': 0, 'num_cycles': 0,\n            'betweenness_mean': 0, 'betweenness_std': 0, 'eigenvector_mean': 0, \n            'ring_4': 0, 'max_degree': 0, 'closeness_mean': 0, 'katz_centrality_std': 0,\n            'heteroatom_ratio': 0\n        }\n        \n        if mol is not None:\n            try:\n                adj = rdmolops.GetAdjacencyMatrix(mol)\n                G = nx.from_numpy_array(adj)\n                \n                # Graph diameter and shortest path\n                if nx.is_connected(G):\n                    results['graph_diameter'] = nx.diameter(G)\n                    results['avg_shortest_path'] = nx.average_shortest_path_length(G)\n                else:\n                    results['graph_diameter'] = 0\n                    results['avg_shortest_path'] = 0\n                \n                # Maximum degree\n                n_nodes = G.number_of_nodes()\n                results['max_degree'] = max([d for n, d in G.degree()]) if n_nodes > 0 else 0\n                \n                # Closeness centrality\n                if nx.is_connected(G):\n                    closeness = list(nx.closeness_centrality(G).values())\n                    results['closeness_mean'] = np.mean(closeness) if closeness else 0\n                else:\n                    results['closeness_mean'] = 0\n                \n                # Katz centrality\n                try:\n                    katz = list(nx.katz_centrality(G, max_iter=1000).values())\n                    results['katz_centrality_std'] = np.std(katz) if len(katz) > 1 else 0\n                except:\n                    results['katz_centrality_std'] = 0\n                \n                # Number of cycles\n                cycles = list(nx.cycle_basis(G))\n                results['num_cycles'] = len(cycles)\n                \n                # Centrality measures\n                betweenness = list(nx.betweenness_centrality(G).values())\n                if betweenness:\n                    betweenness = [b for b in betweenness if np.isfinite(b)]\n                    results['betweenness_mean'] = np.mean(betweenness) if betweenness else 0\n                    results['betweenness_std'] = np.std(betweenness) if len(betweenness) > 1 else 0\n                else:\n                    results['betweenness_mean'] = 0\n                    results['betweenness_std'] = 0\n                \n                # Eigenvector centrality\n                try:\n                    eigenvector = list(nx.eigenvector_centrality(G, max_iter=1000, tol=1e-6).values())\n                    eigenvector = [e for e in eigenvector if np.isfinite(e)]\n                    results['eigenvector_mean'] = np.mean(eigenvector) if eigenvector else 0\n                except:\n                    results['eigenvector_mean'] = 0\n                \n                # Ring analysis\n                cycle_lengths = [len(cycle) for cycle in cycles]\n                results['ring_4'] = sum(1 for length in cycle_lengths if length == 4)\n                \n                # Atom-specific features\n                try:\n                    # Atom type distribution\n                    atom_types = [atom.GetSymbol() for atom in mol.GetAtoms()]\n                    atom_counts = Counter(atom_types)\n                    \n                    # Heteroatom ratio\n                    total_atoms = len(atom_types)\n                    hetero_atoms = total_atoms - atom_counts.get('C', 0)\n                    results['heteroatom_ratio'] = hetero_atoms / total_atoms if total_atoms > 0 else 0\n                except Exception:\n                    results['heteroatom_ratio'] = 0\n                \n            except Exception as e:\n                results.update(graph_defaults)\n        else:\n            results.update(graph_defaults)\n        \n        # Compute Mordred features (key ones from research)\n        mordred_defaults = {'AMW': 0, 'TIC2': 0, 'naRing': 0, 'MPC3': 0}\n        \n        if mol is not None:\n            try:\n                calc = Calculator([\n                    descriptors.Weight,\n                    descriptors.InformationContent,\n                    descriptors.RingCount,\n                    descriptors.PathCount\n                ])\n                \n                mordred_result = calc(mol)\n                desc_dict = dict(zip(calc.descriptors, mordred_result))\n                \n                # Extract specific descriptors\n                amw = next((v for k, v in desc_dict.items() if 'AMW' in str(k)), 0)\n                tic2 = next((v for k, v in desc_dict.items() if 'TIC2' in str(k)), 0)\n                naring = next((v for k, v in desc_dict.items() if 'naRing' in str(k)), 0)\n                mpc3 = next((v for k, v in desc_dict.items() if 'MPC3' in str(k)), 0)\n                \n                results['AMW'] = float(amw) if amw is not None and not isinstance(amw, type(None)) else 0\n                results['TIC2'] = float(tic2) if tic2 is not None and not isinstance(tic2, type(None)) else 0\n                results['naRing'] = float(naring) if naring is not None and not isinstance(naring, type(None)) else 0\n                results['MPC3'] = float(mpc3) if mpc3 is not None and not isinstance(mpc3, type(None)) else 0\n                \n            except Exception as e:\n                results.update(mordred_defaults)\n        else:\n            results.update(mordred_defaults)\n        \n        # Compute specific MACCS fingerprint features\n        maccs_defaults = {\n            'MACCS_Key130': 0, 'MACCS_Key142': 0, 'MACCS_Key066': 0, 'MACCS_Key153': 0\n        }\n        \n        if mol is not None:\n            try:\n                fp = MACCSkeys.GenMACCSKeys(mol)\n                fingerprint = [int(x) for x in fp.ToBitString()]\n                \n                # Extract specific MACCS keys (convert to 0-based indexing)\n                if len(fingerprint) >= 167:\n                    results['MACCS_Key130'] = fingerprint[129] if 129 < len(fingerprint) else 0  # Fixed: 130-1\n                    results['MACCS_Key142'] = fingerprint[141] if 141 < len(fingerprint) else 0  # Fixed: 142-1\n                    results['MACCS_Key066'] = fingerprint[65] if 65 < len(fingerprint) else 0   # Fixed: 66-1\n                    results['MACCS_Key153'] = fingerprint[152] if 152 < len(fingerprint) else 0  # Fixed: 153-1\n                else:\n                    results.update(maccs_defaults)\n                    \n            except Exception as e:\n                results.update(maccs_defaults)\n        else:\n            results.update(maccs_defaults)\n        \n        # Compute specific TopologicalTorsion fingerprint features\n        torsion_defaults = {\n            'TopologicalTorsion_Bit0512': 0, 'TopologicalTorsion_Bit1296': 0\n        }\n        \n        if mol is not None:\n            try:\n                with warnings.catch_warnings():\n                    warnings.filterwarnings(\"ignore\")\n                    fp = rdMolDescriptors.GetHashedTopologicalTorsionFingerprintAsBitVect(mol, nBits=2048)\n                    fingerprint = [int(x) for x in fp.ToBitString()]\n                    \n                    # Extract specific TopologicalTorsion bits\n                    if len(fingerprint) >= 2048:\n                        results['TopologicalTorsion_Bit0512'] = fingerprint[512] if 512 < len(fingerprint) else 0\n                        results['TopologicalTorsion_Bit1296'] = fingerprint[1296] if 1296 < len(fingerprint) else 0\n                    else:\n                        results.update(torsion_defaults)\n                    \n            except Exception as e:\n                results.update(torsion_defaults)\n        else:\n            results.update(torsion_defaults)\n\n        # ‚ú® NEW: Compute specific AtomPair fingerprint features\n        atom_pair_defaults = {\n            'AtomPair_B512_Bit0138': 0, \n            'AtomPair_B512_Bit0448': 0, \n            'AtomPair_B512_Bit0408': 0\n        }\n\n        if mol is not None:\n            try:\n                fp = rdMolDescriptors.GetHashedAtomPairFingerprintAsBitVect(mol, nBits=512)\n                fingerprint = [int(x) for x in fp.ToBitString()]\n\n                # Extract specific AtomPair bits\n                if len(fingerprint) == 512:\n                    results['AtomPair_B512_Bit0138'] = fingerprint[138]\n                    results['AtomPair_B512_Bit0448'] = fingerprint[448]\n                    results['AtomPair_B512_Bit0408'] = fingerprint[408]\n                else:\n                    results.update(atom_pair_defaults)\n\n            except Exception as e:\n                results.update(atom_pair_defaults)\n        else:\n            results.update(atom_pair_defaults)\n        \n        return results\n        \n    except Exception as e:\n        print(f\"Critical error for {smiles}: {e}\")\n        return None\n\ndef process_molecular_features_parallel(df, useless_cols, n_jobs=4):\n    \"\"\"Process molecular features in parallel using joblib\"\"\"\n    if n_jobs == -1:\n        n_jobs = cpu_count()\n    \n    print(f\"üîÑ Using joblib with {n_jobs} jobs\")\n    \n    smiles_list = df['SMILES'].tolist()\n    \n    # Process in parallel\n    results = Parallel(n_jobs=n_jobs, verbose=1)(\n        delayed(compute_molecular_features_for_smiles)(smiles, useless_cols) for smiles in smiles_list\n    )\n    \n    # Filter out None results\n    results = [r for r in results if r is not None]\n    \n    if not results:\n        print(\"‚ö†Ô∏è No valid molecular features computed\")\n        return pd.DataFrame()\n    \n    result_df = pd.DataFrame(results)\n    result_df = result_df.replace([-np.inf, np.inf], np.nan)\n    \n    return result_df","metadata":{"_uuid":"e710c2f9-3286-4035-be19-314aaa9f760d","_cell_guid":"e4b38a49-1c85-41fb-8e6d-05aad197837e","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-08T20:31:41.077172Z","iopub.execute_input":"2025-07-08T20:31:41.078209Z","iopub.status.idle":"2025-07-08T20:31:41.113423Z","shell.execute_reply.started":"2025-07-08T20:31:41.078165Z","shell.execute_reply":"2025-07-08T20:31:41.112318Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =============================================================================\n# EMBEDDING AND CLUSTERING FUNCTIONS\n# =============================================================================\n\n\n\ndef setup_molformer_paths(cfg):\n    \"\"\"Setup model paths for MolFormer\"\"\"\n    print(\"üìÅ Setting up MolFormer model paths...\")\n    os.system(f\"mkdir -p ./ibm/MoLFormer-XL-both-10pct\")\n    os.system(f\"cp {cfg.MOLFORMER_PATH}/molformer-xl-both-10pct/* ./ibm/MoLFormer-XL-both-10pct/\")\n\ndef process_embeddings_one_by_one(smiles_list, model, tokenizer, model_type='chemberta', default_dim=384):\n    \"\"\"\n    Process SMILES embeddings one by one with error handling\n    \n    Args:\n        smiles_list: List of SMILES strings\n        model: Transformer model\n        tokenizer: Tokenizer for the model\n        model_type: 'chemberta' or 'molformer' for correct default embedding dimension\n        default_dim: Default embedding dimension\n    \"\"\"\n    all_embeddings = []\n    \n    # Set default embedding dimension based on model type\n    if model_type.lower() == 'chemberta':\n        default_embedding_dim = 384\n    elif model_type.lower() == 'molformer':\n        default_embedding_dim = 768\n    else:\n        default_embedding_dim = default_dim\n    \n    print(f\"üîß Using default embedding dimension: {default_embedding_dim} for model type: {model_type}\")\n    \n    successful_embeddings = 0\n    failed_embeddings = 0\n    \n    for i, smiles in enumerate(tqdm(smiles_list, desc=f\"Processing {model_type} embeddings\")):\n        try:\n            # Tokenize single SMILES\n            inputs = tokenizer([smiles], padding=True, return_tensors=\"pt\", truncation=True, max_length=512)\n            \n            # Get embedding\n            with torch.no_grad():\n                outputs = model(**inputs)\n                \n                # Handle different output formats\n                if hasattr(outputs, 'pooler_output') and outputs.pooler_output is not None:\n                    embedding = outputs.pooler_output.squeeze(0)\n                elif hasattr(outputs, 'last_hidden_state'):\n                    # Use mean pooling if no pooler output\n                    embedding = outputs.last_hidden_state.mean(dim=1).squeeze(0)\n                else:\n                    raise ValueError(\"Cannot extract embedding from model output\")\n                \n                # Verify embedding dimension matches expected\n                if embedding.shape[0] != default_embedding_dim:\n                    print(f\"‚ö†Ô∏è Unexpected embedding dimension {embedding.shape[0]} vs {default_embedding_dim} for SMILES: {smiles}\")\n                    # Pad or truncate if necessary\n                    if embedding.shape[0] < default_embedding_dim:\n                        padding = torch.zeros(default_embedding_dim - embedding.shape[0])\n                        embedding = torch.cat([embedding, padding])\n                    else:\n                        embedding = embedding[:default_embedding_dim]\n                \n                all_embeddings.append(embedding)\n                successful_embeddings += 1\n                \n        except Exception as e:\n            if i < 5:  # Only print first few errors\n                print(f\"‚ö†Ô∏è Error processing SMILES '{smiles}' at index {i}: {e}\")\n            \n            # Create zero vector of correct dimension\n            zero_embedding = torch.zeros(default_embedding_dim)\n            all_embeddings.append(zero_embedding)\n            failed_embeddings += 1\n    \n    print(f\"üìä Embedding results: {successful_embeddings} successful, {failed_embeddings} failed\")\n    \n    # Stack all embeddings\n    if all_embeddings:\n        final_embeddings = torch.stack(all_embeddings, dim=0)\n        print(f\"‚úÖ Final embeddings shape: {final_embeddings.shape}\")\n        return final_embeddings\n    else:\n        print(\"‚ùå No embeddings generated\")\n        return torch.zeros(len(smiles_list), default_embedding_dim)\n\ndef perform_hdbscan_clustering(embeddings, min_cluster_size, min_samples, random_state=42):\n    \"\"\"\n    Perform HDBSCAN clustering on embeddings\n    \n    Args:\n        embeddings: numpy array of embeddings\n        min_cluster_size: minimum cluster size for HDBSCAN\n        min_samples: minimum samples parameter for HDBSCAN\n        random_state: random state for reproducibility\n    \n    Returns:\n        cluster_labels: array of cluster labels (-1 for noise points)\n        clusterer: fitted HDBSCAN object\n    \"\"\"\n    print(f\"üéØ Performing HDBSCAN clustering (min_cluster_size={min_cluster_size}, min_samples={min_samples})...\")\n    \n    try:\n        # Initialize HDBSCAN\n        clusterer = hdbscan.HDBSCAN(\n            min_cluster_size=min_cluster_size,\n            min_samples=min_samples,\n            cluster_selection_epsilon=0.0,\n            prediction_data=True\n        )\n        \n        # Fit and predict\n        cluster_labels = clusterer.fit_predict(embeddings)\n        \n        # Get clustering statistics\n        unique_labels = np.unique(cluster_labels)\n        n_clusters = len(unique_labels) - (1 if -1 in unique_labels else 0)  # -1 is noise\n        n_noise = np.sum(cluster_labels == -1)\n        \n        print(f\"‚úÖ HDBSCAN completed: {n_clusters} clusters, {n_noise} noise points\")\n        \n        # Replace -1 (noise) with a specific cluster number (use max cluster + 1)\n        if len(cluster_labels) > 0:\n            max_cluster = np.max(cluster_labels[cluster_labels != -1]) if np.any(cluster_labels != -1) else -1\n            noise_cluster_id = max_cluster + 1 if max_cluster >= 0 else 0\n            cluster_labels[cluster_labels == -1] = noise_cluster_id\n            print(f\"üîß Noise points reassigned to cluster {noise_cluster_id}\")\n        \n        return cluster_labels, clusterer\n        \n    except Exception as e:\n        print(f\"‚ùå Error in HDBSCAN clustering: {e}\")\n        # Return default cluster assignments (all points to cluster 0)\n        return np.zeros(len(embeddings), dtype=int), None\n\ndef add_clustering_features_to_df(df, clustering_results):\n    \"\"\"Add clustering features to dataframe\"\"\"\n    for embedding_type, results in clustering_results.items():\n        if results is not None:\n            feature_name = results['feature_name']\n            smiles_to_cluster = results['smiles_to_cluster']\n            \n            cluster_values = []\n            for smiles in df['SMILES']:\n                if smiles in smiles_to_cluster:\n                    cluster_values.append(smiles_to_cluster[smiles])\n                else:\n                    cluster_values.append(0)  # Default cluster for missing SMILES\n            \n            df[feature_name] = cluster_values\n            print(f\"‚úÖ Added clustering feature: {feature_name}\")\n    \n    return df","metadata":{"_uuid":"507973a5-cb43-4b37-8a14-6c38102c9ba3","_cell_guid":"adce9ca2-6512-464f-915f-10c450456ebe","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-08T20:31:41.11488Z","iopub.execute_input":"2025-07-08T20:31:41.115267Z","iopub.status.idle":"2025-07-08T20:31:41.140654Z","shell.execute_reply.started":"2025-07-08T20:31:41.115228Z","shell.execute_reply":"2025-07-08T20:31:41.139554Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =============================================================================\n# CLUSTERING GENERATION FUNCTIONS\n# =============================================================================\n\n\n\ndef generate_clustering_features(all_smiles, cfg, mode='train', existing_clustering=None):\n    \"\"\"Generate clustering features - either create new or use existing KMeans/HDBSCAN models\"\"\"\n    \n    if mode == 'train':\n        print(\"üî¨ Generating clustering features for training data...\")\n        clustering_results = {}\n        \n        # 1. ChemBERTa embeddings and clustering\n        print(\"\\nüß™ Processing ChemBERTa embeddings...\")\n        try:\n            chemberta_model = AutoModel.from_pretrained(cfg.CHEMBERTA_PATH, local_files_only=True, trust_remote_code=True)\n            chemberta_tokenizer = AutoTokenizer.from_pretrained(cfg.CHEMBERTA_PATH, trust_remote_code=True, local_files_only=True)\n            \n            # Import the function\n            #from embedding_clustering import process_embeddings_one_by_one, perform_hdbscan_clustering\n            \n            chemberta_embeddings = process_embeddings_one_by_one(all_smiles, chemberta_model, chemberta_tokenizer, model_type='chemberta')\n            print(f\"‚úÖ ChemBERTa embeddings shape: {chemberta_embeddings.shape}\")\n            \n            if chemberta_embeddings.numel() > 0:  # Check if we have valid embeddings\n                embeddings_numpy = chemberta_embeddings.numpy()\n                \n                # Original KMeans clustering\n                print(f\"üéØ Performing ChemBERTa KMeans clustering ({cfg.CHEMBERTA_CLUSTERS} clusters)...\")\n                chemberta_kmeans = KMeans(n_clusters=cfg.CHEMBERTA_CLUSTERS, random_state=cfg.SEED, n_init=10)\n                chemberta_labels = chemberta_kmeans.fit_predict(embeddings_numpy)\n                \n                clustering_results['chemberta'] = {\n                    'smiles_to_cluster': dict(zip(all_smiles, chemberta_labels)),\n                    'kmeans_model': chemberta_kmeans,\n                    'embedding_model': None,  # Will be saved separately if needed\n                    'tokenizer': None,  # Will be saved separately if needed\n                    'n_clusters': cfg.CHEMBERTA_CLUSTERS,\n                    'feature_name': 'chemberta_cluster'\n                }\n                \n                print(f\"‚úÖ ChemBERTa KMeans clustering complete!\")\n                \n                # HDBSCAN clustering with first parameter set\n                hdbscan_labels_1, hdbscan_model_1 = perform_hdbscan_clustering(\n                    embeddings_numpy, \n                    min_cluster_size=cfg.HDBSCAN_MIN_CLUSTER_SIZE_1, \n                    min_samples=cfg.HDBSCAN_MIN_SAMPLES_1,\n                    random_state=cfg.SEED\n                )\n                \n                clustering_results['chemberta_hdbscan_1'] = {\n                    'smiles_to_cluster': dict(zip(all_smiles, hdbscan_labels_1)),\n                    'hdbscan_model': hdbscan_model_1,\n                    'min_cluster_size': cfg.HDBSCAN_MIN_CLUSTER_SIZE_1,\n                    'min_samples': cfg.HDBSCAN_MIN_SAMPLES_1,\n                    'feature_name': 'chemberta_hdbscan_cluster_1'\n                }\n                \n                print(f\"‚úÖ ChemBERTa HDBSCAN clustering 1 complete!\")\n                \n                # HDBSCAN clustering with second parameter set\n                hdbscan_labels_2, hdbscan_model_2 = perform_hdbscan_clustering(\n                    embeddings_numpy, \n                    min_cluster_size=cfg.HDBSCAN_MIN_CLUSTER_SIZE_2, \n                    min_samples=cfg.HDBSCAN_MIN_SAMPLES_2,\n                    random_state=cfg.SEED\n                )\n                \n                clustering_results['chemberta_hdbscan_2'] = {\n                    'smiles_to_cluster': dict(zip(all_smiles, hdbscan_labels_2)),\n                    'hdbscan_model': hdbscan_model_2,\n                    'min_cluster_size': cfg.HDBSCAN_MIN_CLUSTER_SIZE_2,\n                    'min_samples': cfg.HDBSCAN_MIN_SAMPLES_2,\n                    'feature_name': 'chemberta_hdbscan_cluster_2'\n                }\n                \n                print(f\"‚úÖ ChemBERTa HDBSCAN clustering 2 complete!\")\n                \n            else:\n                print(f\"‚ùå ChemBERTa: No valid embeddings generated\")\n                clustering_results['chemberta'] = None\n                clustering_results['chemberta_hdbscan_1'] = None\n                clustering_results['chemberta_hdbscan_2'] = None\n            \n            # Clean up ChemBERTa model (keeping only clustering models)\n            del chemberta_model, chemberta_tokenizer, chemberta_embeddings\n            torch.cuda.empty_cache()\n            gc.collect()\n            \n        except Exception as e:\n            print(f\"‚ùå Error processing ChemBERTa: {e}\")\n            clustering_results['chemberta'] = None\n            clustering_results['chemberta_hdbscan_1'] = None\n            clustering_results['chemberta_hdbscan_2'] = None\n        \n        # 2. MolFormer embeddings and clustering\n        print(\"\\nüß¨ Processing MolFormer embeddings...\")\n        try:\n            #from embedding_clustering import setup_molformer_paths\n            setup_molformer_paths(cfg)\n            \n            molformer_model = AutoModel.from_pretrained('./ibm/MoLFormer-XL-both-10pct/', \n                                                       deterministic_eval=True, \n                                                       local_files_only=True,\n                                                       trust_remote_code=True)\n            molformer_tokenizer = AutoTokenizer.from_pretrained('./ibm/MoLFormer-XL-both-10pct/', \n                                                               trust_remote_code=True,\n                                                               local_files_only=True)\n            \n            molformer_embeddings = process_embeddings_one_by_one(all_smiles, molformer_model, molformer_tokenizer, model_type='molformer')\n            print(f\"‚úÖ MolFormer embeddings shape: {molformer_embeddings.shape}\")\n            \n            if molformer_embeddings.numel() > 0:\n                # MolFormer clustering\n                print(f\"üéØ Performing MolFormer KMeans clustering ({cfg.MOLFORMER_CLUSTERS} clusters)...\")\n                molformer_kmeans = KMeans(n_clusters=cfg.MOLFORMER_CLUSTERS, random_state=cfg.SEED, n_init=10)\n                molformer_labels = molformer_kmeans.fit_predict(molformer_embeddings.numpy())\n                \n                clustering_results['molformer'] = {\n                    'smiles_to_cluster': dict(zip(all_smiles, molformer_labels)),\n                    'kmeans_model': molformer_kmeans,\n                    'embedding_model': None,\n                    'tokenizer': None,\n                    'n_clusters': cfg.MOLFORMER_CLUSTERS,\n                    'feature_name': 'molformer_cluster'\n                }\n                \n                print(f\"‚úÖ MolFormer clustering complete!\")\n            else:\n                print(f\"‚ùå MolFormer: No valid embeddings generated\")\n                clustering_results['molformer'] = None\n            \n            # Clean up MolFormer model\n            del molformer_model, molformer_tokenizer, molformer_embeddings\n            torch.cuda.empty_cache()\n            gc.collect()\n            os.system(\"rm -rf ./ibm\")\n            \n        except Exception as e:\n            print(f\"‚ùå Error processing MolFormer: {e}\")\n            clustering_results['molformer'] = None\n        \n        return clustering_results\n    \n    else:  # mode == 'test'\n        print(\"üî¨ Applying existing KMeans/HDBSCAN models to test data...\")\n        return apply_clustering_to_test(all_smiles, cfg, existing_clustering)\n\n\ndef apply_clustering_to_test(all_smiles, cfg, existing_clustering):\n    \"\"\"Apply existing clustering models to test data\"\"\"\n    test_clustering_results = {}\n    \n    # Import required functions\n    #from embedding_clustering import process_embeddings_one_by_one, setup_molformer_paths\n    import hdbscan\n    \n    # 1. ChemBERTa test clustering\n    if existing_clustering.get('chemberta') is not None or \\\n       existing_clustering.get('chemberta_hdbscan_1') is not None or \\\n       existing_clustering.get('chemberta_hdbscan_2') is not None:\n        \n        print(\"\\nüß™ Applying ChemBERTa clustering to test data...\")\n        try:\n            chemberta_model = AutoModel.from_pretrained(cfg.CHEMBERTA_PATH, local_files_only=True, trust_remote_code=True)\n            chemberta_tokenizer = AutoTokenizer.from_pretrained(cfg.CHEMBERTA_PATH, trust_remote_code=True, local_files_only=True)\n            \n            # Generate embeddings for test SMILES\n            chemberta_embeddings = process_embeddings_one_by_one(all_smiles, chemberta_model, chemberta_tokenizer, model_type='chemberta')\n            print(f\"‚úÖ ChemBERTa test embeddings shape: {chemberta_embeddings.shape}\")\n            \n            if chemberta_embeddings.numel() > 0:\n                embeddings_numpy = chemberta_embeddings.numpy()\n                \n                # Apply KMeans model if available\n                if existing_clustering.get('chemberta') is not None:\n                    chemberta_kmeans = existing_clustering['chemberta']['kmeans_model']\n                    chemberta_test_labels = chemberta_kmeans.predict(embeddings_numpy)\n                    \n                    test_clustering_results['chemberta'] = {\n                        'smiles_to_cluster': dict(zip(all_smiles, chemberta_test_labels)),\n                        'kmeans_model': chemberta_kmeans,\n                        'n_clusters': existing_clustering['chemberta']['n_clusters'],\n                        'feature_name': existing_clustering['chemberta']['feature_name']\n                    }\n                    \n                    print(f\"‚úÖ ChemBERTa KMeans test clustering applied!\")\n                \n                # Apply HDBSCAN models\n                for hdbscan_key in ['chemberta_hdbscan_1', 'chemberta_hdbscan_2']:\n                    if existing_clustering.get(hdbscan_key) is not None:\n                        hdbscan_model = existing_clustering[hdbscan_key]['hdbscan_model']\n                        if hdbscan_model is not None:\n                            #try:\n                            # For HDBSCAN, we use approximate_predict for new data\n                            hdbscan_test_labels, _ = hdbscan.approximate_predict(hdbscan_model, embeddings_numpy)\n                            \n                            # Handle noise points (-1) same way as in training\n                            if len(hdbscan_test_labels) > 0:\n                                max_cluster = np.max(hdbscan_test_labels[hdbscan_test_labels != -1]) if np.any(hdbscan_test_labels != -1) else -1\n                                noise_cluster_id = max_cluster + 1 if max_cluster >= 0 else 0\n                                hdbscan_test_labels[hdbscan_test_labels == -1] = noise_cluster_id\n                            \n                            test_clustering_results[hdbscan_key] = {\n                                'smiles_to_cluster': dict(zip(all_smiles, hdbscan_test_labels)),\n                                'hdbscan_model': hdbscan_model,\n                                'min_cluster_size': existing_clustering[hdbscan_key]['min_cluster_size'],\n                                'min_samples': existing_clustering[hdbscan_key]['min_samples'],\n                                'feature_name': existing_clustering[hdbscan_key]['feature_name']\n                            }\n                            \n                            print(f\"‚úÖ {hdbscan_key} test clustering applied!\")\n                            #except Exception as e:\n                            #    print(f\"‚ö†Ô∏è Error applying {hdbscan_key}: {e}\")\n                            #    test_clustering_results[hdbscan_key] = None\n            \n            # Clean up\n            del chemberta_model, chemberta_tokenizer, chemberta_embeddings\n            torch.cuda.empty_cache()\n            gc.collect()\n            \n        except Exception as e:\n            print(f\"‚ö†Ô∏è Error applying ChemBERTa clustering to test data: {e}\")\n            test_clustering_results['chemberta'] = None\n            test_clustering_results['chemberta_hdbscan_1'] = None\n            test_clustering_results['chemberta_hdbscan_2'] = None\n    \n    # 2. MolFormer test clustering\n    if existing_clustering.get('molformer') is not None:\n        print(\"\\nüß¨ Applying MolFormer clustering to test data...\")\n        try:\n            setup_molformer_paths(cfg)\n            \n            molformer_model = AutoModel.from_pretrained('./ibm/MoLFormer-XL-both-10pct/', \n                                                       deterministic_eval=True, \n                                                       local_files_only=True,\n                                                       trust_remote_code=True)\n            molformer_tokenizer = AutoTokenizer.from_pretrained('./ibm/MoLFormer-XL-both-10pct/', \n                                                               trust_remote_code=True,\n                                                               local_files_only=True)\n            \n            # Generate embeddings for test SMILES\n            molformer_embeddings = process_embeddings_one_by_one(all_smiles, molformer_model, molformer_tokenizer, model_type='molformer')\n            print(f\"‚úÖ MolFormer test embeddings shape: {molformer_embeddings.shape}\")\n            \n            if molformer_embeddings.numel() > 0:\n                # Apply trained KMeans model\n                molformer_kmeans = existing_clustering['molformer']['kmeans_model']\n                molformer_test_labels = molformer_kmeans.predict(molformer_embeddings.numpy())\n                \n                test_clustering_results['molformer'] = {\n                    'smiles_to_cluster': dict(zip(all_smiles, molformer_test_labels)),\n                    'kmeans_model': molformer_kmeans,\n                    'n_clusters': existing_clustering['molformer']['n_clusters'],\n                    'feature_name': existing_clustering['molformer']['feature_name']\n                }\n                \n                print(f\"‚úÖ MolFormer test clustering applied!\")\n            else:\n                print(f\"‚ùå MolFormer: No valid test embeddings generated\")\n                test_clustering_results['molformer'] = None\n            \n            # Clean up\n            del molformer_model, molformer_tokenizer, molformer_embeddings\n            torch.cuda.empty_cache()\n            gc.collect()\n            os.system(\"rm -rf ./ibm\")\n            \n        except Exception as e:\n            print(f\"‚ùå Error applying MolFormer clustering to test: {e}\")\n            test_clustering_results['molformer'] = None\n    else:\n        test_clustering_results['molformer'] = None\n    \n    return test_clustering_results","metadata":{"_uuid":"351cafc2-29c9-4d4d-994c-83f0e52bfca5","_cell_guid":"875b7397-26be-4f42-a885-366a897eeb25","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-08T20:31:41.141965Z","iopub.execute_input":"2025-07-08T20:31:41.142267Z","iopub.status.idle":"2025-07-08T20:31:41.173389Z","shell.execute_reply.started":"2025-07-08T20:31:41.142242Z","shell.execute_reply":"2025-07-08T20:31:41.172289Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =============================================================================\n# FEATURE PROCESSING UTILITIES\n# =============================================================================\n\n\n\n\ndef clean_features_and_compute_means(df, feature_columns):\n    \"\"\"Clean features and compute means for training data\"\"\"\n    print(\"üßπ Cleaning features and computing means...\")\n    \n    # Make a copy to avoid modifying original\n    df = df.copy()\n    \n    # Replace infinite values with NaN\n    for col in feature_columns:\n        if col in df.columns:\n            df[col] = df[col].replace([-np.inf, np.inf], np.nan)\n    \n    # Compute means for non-NaN values\n    feature_means = {}\n    for col in feature_columns:\n        if col in df.columns:\n            mean_val = df[col].mean()\n            feature_means[col] = mean_val if not np.isnan(mean_val) else 0\n        else:\n            feature_means[col] = 0\n    \n    print(f\"‚úÖ Computed means for {len(feature_means)} features\")\n    return df, feature_means\n\ndef apply_imputation_to_test(df, feature_columns, feature_means):\n    \"\"\"Apply feature cleaning and imputation to test data\"\"\"\n    print(\"üßπ Cleaning and imputing test features...\")\n    \n    # Make a copy to avoid modifying original\n    df = df.copy()\n    \n    # Replace infinite values with NaN\n    for col in feature_columns:\n        if col in df.columns:\n            df[col] = df[col].replace([-np.inf, np.inf], np.nan)\n    \n    # Replace NaN with training means\n    for col in feature_columns:\n        if col in df.columns and col in feature_means:\n            df[col] = df[col].fillna(feature_means[col])\n        elif col not in df.columns and col in feature_means:\n            # Add missing column with mean value\n            df[col] = feature_means[col]\n    \n    print(f\"‚úÖ Applied imputation to {len(feature_columns)} features\")\n    return df\n\ndef validate_features(df, expected_features, phase='train'):\n    \"\"\"Validate that all expected features are present\"\"\"\n    missing_features = []\n    for feat in expected_features:\n        if feat not in df.columns:\n            missing_features.append(feat)\n    \n    if missing_features:\n        print(f\"‚ö†Ô∏è Warning: {len(missing_features)} features missing in {phase} data:\")\n        print(f\"   {missing_features[:5]}{'...' if len(missing_features) > 5 else ''}\")\n    else:\n        print(f\"‚úÖ All expected features present in {phase} data\")\n    \n    return missing_features","metadata":{"_uuid":"77fb2abe-e055-4da1-86c9-8ae3c0d96dd0","_cell_guid":"761014fa-afb0-4ae4-9256-9a359f1afc70","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-08T20:31:41.174756Z","iopub.execute_input":"2025-07-08T20:31:41.175094Z","iopub.status.idle":"2025-07-08T20:31:41.199331Z","shell.execute_reply.started":"2025-07-08T20:31:41.175065Z","shell.execute_reply":"2025-07-08T20:31:41.198108Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =============================================================================\n# MAIN PIPELINE FUNCTIONS\n# =============================================================================\n\n\n\ndef prepare_train(cfg):\n    \"\"\"\n    Main function to prepare training features and save all necessary files\n    \"\"\"\n    print(\"üöÄ STARTING TRAINING FEATURE PREPARATION\")\n    print(\"=\"*70)\n    \n    # Import required functions\n    #from config_constants import USELESS_COLS\n    #from data_loading import load_and_augment_train_data\n    #from clustering_generation import generate_clustering_features\n    #from embedding_clustering import add_clustering_features_to_df\n    #from molecular_features import process_molecular_features_parallel\n    #from feature_utils import clean_features_and_compute_means\n    \n    # Step 1: Load and augment training data\n    train = load_and_augment_train_data(cfg)\n    \n    # Step 2: Generate clustering features\n    all_smiles = train['SMILES'].tolist()\n    clustering_results = generate_clustering_features(all_smiles, cfg, mode='train')\n    \n    # Step 3: Add clustering features to train dataframe\n    train = add_clustering_features_to_df(train, clustering_results)\n    \n    # Step 4: Generate molecular features\n    print(\"üß¨ Processing molecular features for train set...\")\n    molecular_features = process_molecular_features_parallel(train, USELESS_COLS, n_jobs=4)\n    \n    # Step 5: Combine all features\n    if not molecular_features.empty:\n        train_complete = pd.concat([train, molecular_features], axis=1)\n    else:\n        print(\"‚ö†Ô∏è No molecular features generated, using only base features\")\n        train_complete = train\n    \n    # Step 6: Identify feature columns\n    molecular_feature_names = molecular_features.columns.tolist() if not molecular_features.empty else []\n    clustering_feature_names = [results['feature_name'] for results in clustering_results.values() if results is not None]\n    all_computed_features = molecular_feature_names + clustering_feature_names\n    \n    # Step 7: Clean features and compute means\n    train_complete, feature_means = clean_features_and_compute_means(train_complete, all_computed_features)\n    \n    # Step 8: Generate metadata\n    feature_metadata = {\n        'molecular_features': molecular_feature_names,\n        'clustering_features': clustering_feature_names,\n        'all_computed_features': all_computed_features,\n        'original_columns': ['id', 'SMILES'] + cfg.TARGETS,\n        'useless_cols_excluded': USELESS_COLS,\n        'generation_config': {\n            'chemberta_clusters': cfg.CHEMBERTA_CLUSTERS,\n            'molformer_clusters': cfg.MOLFORMER_CLUSTERS,\n            'hdbscan_min_cluster_size_1': cfg.HDBSCAN_MIN_CLUSTER_SIZE_1,\n            'hdbscan_min_samples_1': cfg.HDBSCAN_MIN_SAMPLES_1,\n            'hdbscan_min_cluster_size_2': cfg.HDBSCAN_MIN_CLUSTER_SIZE_2,\n            'hdbscan_min_samples_2': cfg.HDBSCAN_MIN_SAMPLES_2,\n            'seed': cfg.SEED\n        }\n    }\n    \n    # Step 9: Save all files\n    print(\"\\nüíæ Saving training files...\")\n    \n    # Create output directory if it doesn't exist\n    os.makedirs(cfg.OUTPUT_PATH, exist_ok=True)\n    \n    # Save complete training features\n    train_features_path = os.path.join(cfg.OUTPUT_PATH, cfg.TRAIN_FEATURES_FILE)\n    train_complete.to_pickle(train_features_path)\n    print(f\"‚úÖ Train features saved: {train_features_path} (Shape: {train_complete.shape})\")\n    \n    # Save feature means for test imputation\n    means_path = os.path.join(cfg.OUTPUT_PATH, cfg.TRAIN_MEANS_FILE)\n    with open(means_path, 'wb') as f:\n        pickle.dump(feature_means, f)\n    print(f\"‚úÖ Feature means saved: {means_path}\")\n    \n    # Save clustering information\n    clustering_path = os.path.join(cfg.OUTPUT_PATH, cfg.CLUSTERING_INFO_FILE)\n    with open(clustering_path, 'wb') as f:\n        pickle.dump(clustering_results, f)\n    print(f\"‚úÖ Clustering info saved: {clustering_path}\")\n    \n    # Save metadata\n    metadata_path = os.path.join(cfg.OUTPUT_PATH, cfg.FEATURE_METADATA_FILE)\n    with open(metadata_path, 'wb') as f:\n        pickle.dump(feature_metadata, f)\n    print(f\"‚úÖ Metadata saved: {metadata_path}\")\n    \n    # Final report\n    print(f\"\\n‚ú® TRAINING PREPARATION COMPLETE! ‚ú®\")\n    print(f\"üìä Generated {len(all_computed_features)} features:\")\n    print(f\"   ‚Ä¢ Molecular features: {len(molecular_feature_names)}\")\n    print(f\"   ‚Ä¢ Clustering features: {len(clustering_feature_names)}\")\n    if 'chemberta' in clustering_results and clustering_results['chemberta'] is not None:\n        print(f\"     - ChemBERTa KMeans: 1 feature\")\n    if 'chemberta_hdbscan_1' in clustering_results and clustering_results['chemberta_hdbscan_1'] is not None:\n        print(f\"     - ChemBERTa HDBSCAN: 2 features\")\n    if 'molformer' in clustering_results and clustering_results['molformer'] is not None:\n        print(f\"     - MolFormer KMeans: 1 feature\")\n    print(f\"üìÅ Files saved to: {cfg.OUTPUT_PATH}\")\n    \n    # Clean up memory\n    gc.collect()\n    \n    return train_complete, feature_means, clustering_results, feature_metadata\n\ndef prepare_test(cfg):\n    \"\"\"\n    Main function to prepare test features using saved training information\n    \"\"\"\n    print(\"üöÄ STARTING TEST FEATURE PREPARATION\")\n    print(\"=\"*70)\n    \n    # Import required functions\n    #from config_constants import USELESS_COLS\n    #from data_loading import load_test_data\n    #from clustering_generation import generate_clustering_features\n    #from embedding_clustering import add_clustering_features_to_df\n    #from molecular_features import process_molecular_features_parallel\n    #from feature_utils import apply_imputation_to_test, validate_features\n    \n    # Step 1: Load required files from training\n    print(\"üìÇ Loading training preparation files...\")\n    \n    # Load feature means\n    means_path = os.path.join(cfg.INPUT_PATH, cfg.TRAIN_MEANS_FILE)\n    try:\n        with open(means_path, 'rb') as f:\n            feature_means = pickle.load(f)\n        print(f\"‚úÖ Loaded feature means: {len(feature_means)} features\")\n    except FileNotFoundError:\n        raise FileNotFoundError(f\"Training means file not found: {means_path}. Run prepare_train() first.\")\n    \n    # Load clustering information\n    clustering_path = os.path.join(cfg.INPUT_PATH, cfg.CLUSTERING_INFO_FILE)\n    try:\n        with open(clustering_path, 'rb') as f:\n            clustering_results = pickle.load(f)\n        print(f\"‚úÖ Loaded clustering information\")\n    except FileNotFoundError:\n        raise FileNotFoundError(f\"Clustering file not found: {clustering_path}. Run prepare_train() first.\")\n    \n    # Load metadata\n    metadata_path = os.path.join(cfg.INPUT_PATH, cfg.FEATURE_METADATA_FILE)\n    try:\n        with open(metadata_path, 'rb') as f:\n            feature_metadata = pickle.load(f)\n        print(f\"‚úÖ Loaded feature metadata\")\n    except FileNotFoundError:\n        raise FileNotFoundError(f\"Metadata file not found: {metadata_path}. Run prepare_train() first.\")\n    \n    # Step 2: Load test data\n    test = load_test_data(cfg)\n    \n    # Step 3: Generate clustering features using existing KMeans/HDBSCAN models\n    all_smiles = test['SMILES'].tolist()\n    test_clustering_results = generate_clustering_features(all_smiles, cfg, mode='test', existing_clustering=clustering_results)\n    \n    # Step 4: Add clustering features using test results\n    test = add_clustering_features_to_df(test, test_clustering_results)\n    \n    # Step 5: Generate molecular features\n    print(\"üß¨ Processing molecular features for test set...\")\n    molecular_features = process_molecular_features_parallel(test, USELESS_COLS, n_jobs=4)\n    \n    # Step 6: Combine all features\n    if not molecular_features.empty:\n        test_complete = pd.concat([test, molecular_features], axis=1)\n    else:\n        print(\"‚ö†Ô∏è No molecular features generated, using only base features\")\n        test_complete = test\n    \n    # Step 7: Apply cleaning and imputation\n    all_computed_features = feature_metadata['all_computed_features']\n    test_complete = apply_imputation_to_test(test_complete, all_computed_features, feature_means)\n    \n    # Step 8: Validate features\n    validate_features(test_complete, all_computed_features, phase='test')\n    \n    # Step 9: Save test features\n    print(\"\\nüíæ Saving test features...\")\n    test_features_path = os.path.join(cfg.OUTPUT_PATH, cfg.TEST_FEATURES_FILE)\n    test_complete.to_pickle(test_features_path)\n    print(f\"‚úÖ Test features saved: {test_features_path} (Shape: {test_complete.shape})\")\n    \n    # Final report\n    print(f\"\\n‚ú® TEST PREPARATION COMPLETE! ‚ú®\")\n    print(f\"üìä Applied {len(all_computed_features)} features with training means imputation\")\n    \n    # Clean up memory\n    gc.collect()\n    \n    return test_complete","metadata":{"_uuid":"88601d07-95ee-4b63-a45e-1a1316cbceb5","_cell_guid":"4957a5e0-29c1-4035-91a4-3ec6721e4aee","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-08T20:31:41.200711Z","iopub.execute_input":"2025-07-08T20:31:41.201373Z","iopub.status.idle":"2025-07-08T20:31:41.228373Z","shell.execute_reply.started":"2025-07-08T20:31:41.201341Z","shell.execute_reply":"2025-07-08T20:31:41.227422Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =============================================================================\n# CONFIGURATION AND CONSTANTS\n# =============================================================================\n\n# Define features to exclude (constant, problematic, or highly correlated)\nUSELESS_COLS = [    \n    # NaN data\n    'BCUT2D_MWHI', 'BCUT2D_MWLOW', 'BCUT2D_CHGHI', 'BCUT2D_CHGLO',\n    'BCUT2D_LOGPHI', 'BCUT2D_LOGPLOW', 'BCUT2D_MRHI', 'BCUT2D_MRLOW',\n    \n    # Constant data\n    'NumRadicalElectrons', 'SMR_VSA8', 'SlogP_VSA9', 'fr_barbitur',\n    'fr_benzodiazepine', 'fr_dihydropyridine', 'fr_epoxide', 'fr_isothiocyan',\n    'fr_lactam', 'fr_nitroso', 'fr_prisulfonamd', 'fr_thiocyan',\n    \n    # High correlated data >0.95\n    'MaxEStateIndex', 'HeavyAtomMolWt', 'ExactMolWt', 'NumValenceElectrons',\n    'Chi0', 'Chi0n', 'Chi0v', 'Chi1', 'Chi1n', 'Chi1v', 'Chi2n', 'Kappa1',\n    'LabuteASA', 'HeavyAtomCount', 'MolMR', 'Chi3n', 'BertzCT', 'Chi2v',\n    'Chi4n', 'HallKierAlpha', 'Chi3v', 'Chi4v', 'MinAbsPartialCharge',\n    'MinPartialCharge', 'MaxAbsPartialCharge', 'FpDensityMorgan2',\n    'FpDensityMorgan3', 'Phi', 'Kappa3', 'fr_nitrile', 'SlogP_VSA6',\n    'NumAromaticCarbocycles', 'NumAromaticRings', 'fr_benzene', 'VSA_EState6',\n    'NOCount', 'fr_C_O', 'fr_C_O_noCOO', 'NumHDonors', 'fr_amide',\n    'fr_Nhpyrrole', 'fr_phenol', 'fr_phenol_noOrthoHbond', 'fr_COO2',\n    'fr_halogen', 'fr_diazo', 'fr_nitro_arom', 'fr_phos_ester'\n]\n\n# Default embedding dimensions\nCHEMBERTA_DIM = 384\nMOLFORMER_DIM = 768\n\nclass FeaturePrepConfig:\n    \"\"\"Configuration class for feature preparation\"\"\"\n    def __init__(self):\n        # Paths\n        self.PATH = '/kaggle/input/neurips-open-polymer-prediction-2025/'\n        self.OUTPUT_PATH = '/kaggle/working/'\n        self.INPUT_PATH = '/kaggle/input/neurips-preparedataset'\n        self.CHEMBERTA_PATH = '/kaggle/input/c/transformers/default/1/ChemBERTa-77M-MLM'\n        self.MOLFORMER_PATH = '/kaggle/input/download-molformer'\n        \n        # Files\n        self.TRAIN_FEATURES_FILE = 'train.pkl'\n        self.TEST_FEATURES_FILE = 'test.pkl'\n        self.TRAIN_MEANS_FILE = 'feature_means.pkl'\n        self.CLUSTERING_INFO_FILE = 'clustering_info.pkl'\n        self.FEATURE_METADATA_FILE = 'feature_metadata.pkl'\n        \n        # Model parameters\n        self.CHEMBERTA_CLUSTERS = 20\n        self.MOLFORMER_CLUSTERS = 20\n        self.HDBSCAN_MIN_CLUSTER_SIZE_1 = 25\n        self.HDBSCAN_MIN_SAMPLES_1 = 25\n        self.HDBSCAN_MIN_CLUSTER_SIZE_2 = 30\n        self.HDBSCAN_MIN_SAMPLES_2 = 3\n        \n        # Other\n        self.SEED = 42\n        self.TARGETS = ['Tg', 'Tc', 'Rg', 'FFV', 'Density']","metadata":{"_uuid":"1eb8f0a9-78a3-4861-801e-afa804c81cb0","_cell_guid":"b1e4bedd-2468-4429-be82-8cce60791387","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-08T20:31:41.229407Z","iopub.execute_input":"2025-07-08T20:31:41.229694Z","iopub.status.idle":"2025-07-08T20:31:41.247614Z","shell.execute_reply.started":"2025-07-08T20:31:41.229672Z","shell.execute_reply":"2025-07-08T20:31:41.24657Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cfg = FeaturePrepConfig()\n#train = prepare_train(cfg)\ntest = prepare_test(cfg)","metadata":{"_uuid":"f7495b9c-76c4-4e79-b50e-2bf21aad83ed","_cell_guid":"8bf5b990-63ba-4dc2-b5e0-daff3081769e","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-08T20:31:41.251081Z","iopub.execute_input":"2025-07-08T20:31:41.251387Z","iopub.status.idle":"2025-07-08T20:32:13.273173Z","shell.execute_reply.started":"2025-07-08T20:31:41.251364Z","shell.execute_reply":"2025-07-08T20:32:13.272094Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"_uuid":"7cca5efa-f9d0-4be5-93f1-c50d44e05110","_cell_guid":"71de2c7d-dd4c-4f30-a9f9-c35702edd8fa","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train = pd.read_pickle('/kaggle/input/neurips-preparedataset/train.pkl')\ntrain.head()","metadata":{"_uuid":"409e6270-8191-425c-8d64-dc6ea62d15cb","_cell_guid":"047ec39b-103c-4ec9-b8d7-d1eb3570cc7f","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-08T20:32:13.274266Z","iopub.execute_input":"2025-07-08T20:32:13.274656Z","iopub.status.idle":"2025-07-08T20:32:13.399203Z","shell.execute_reply.started":"2025-07-08T20:32:13.274622Z","shell.execute_reply":"2025-07-08T20:32:13.39807Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test = pd.read_pickle('/kaggle/working/test.pkl')\ntest['Ipc']=np.log10(test['Ipc'])\ntest['Ipc']=test['Ipc'].astype(np.float32)\nfor col in test.columns[2:]:\n    test[col]=test[col].replace(np.inf,np.nan).replace(-np.inf,np.nan).fillna(train[col].mean())\n    test[col]=test[col].clip(train[col].min(),train[col].max())\ntest.head()","metadata":{"_uuid":"d1e07312-1b37-4324-a83a-14140e56a474","_cell_guid":"7d63ffac-46f5-4af2-9981-270d93fac6aa","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-08T20:32:13.400268Z","iopub.execute_input":"2025-07-08T20:32:13.400545Z","iopub.status.idle":"2025-07-08T20:32:13.653792Z","shell.execute_reply.started":"2025-07-08T20:32:13.400525Z","shell.execute_reply":"2025-07-08T20:32:13.652792Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = TabularPredictor.load('/kaggle/input/neurips-train-autogluon-tg3/model/')\npred = model.predict(test) #(model_Tg.predict(test)+model_TgKm.predict(test))/2\ntest['Tg']=pred.values\ntest.head()","metadata":{"_uuid":"5b5e9677-e2a2-430f-beb0-f5f29b4d0f99","_cell_guid":"f649250d-09ef-4436-b9a5-d99a1473319f","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-08T20:32:13.654859Z","iopub.execute_input":"2025-07-08T20:32:13.655139Z","iopub.status.idle":"2025-07-08T20:32:28.934074Z","shell.execute_reply.started":"2025-07-08T20:32:13.655116Z","shell.execute_reply":"2025-07-08T20:32:28.932937Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = TabularPredictor.load('/kaggle/input/neurips-train-autogluon-tg3/model/')\npred = model.predict(test)\ntest['Tg']=pred.values\ntest.head()","metadata":{"_uuid":"a9823393-cb17-4e83-b894-9c6ac49ef24a","_cell_guid":"5980eaa9-f64e-4b63-8009-7cbee29b231b","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-07-08T20:32:28.935284Z","iopub.execute_input":"2025-07-08T20:32:28.936616Z","iopub.status.idle":"2025-07-08T20:32:33.986108Z","shell.execute_reply.started":"2025-07-08T20:32:28.93657Z","shell.execute_reply":"2025-07-08T20:32:33.984952Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = TabularPredictor.load('/kaggle/input/neurips-train-autogluon-rg3/model/')\npred = model.predict(test)\ntest['Rg']=pred.values\ntest.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T20:32:33.987448Z","iopub.execute_input":"2025-07-08T20:32:33.98771Z","iopub.status.idle":"2025-07-08T20:32:36.923059Z","shell.execute_reply.started":"2025-07-08T20:32:33.987689Z","shell.execute_reply":"2025-07-08T20:32:36.92172Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = TabularPredictor.load('/kaggle/input/neurips-train-autogluon-ffv3/model/')\npred = model.predict(test)\ntest['FFV']=pred.values\ntest.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T20:32:36.92448Z","iopub.execute_input":"2025-07-08T20:32:36.924854Z","iopub.status.idle":"2025-07-08T20:32:45.970272Z","shell.execute_reply.started":"2025-07-08T20:32:36.92483Z","shell.execute_reply":"2025-07-08T20:32:45.969019Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = TabularPredictor.load('/kaggle/input/neurips-train-autogluon-density3/model/')\npred = model.predict(test)\ntest['Density']=pred.values\ntest.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T20:32:45.972326Z","iopub.execute_input":"2025-07-08T20:32:45.972684Z","iopub.status.idle":"2025-07-08T20:32:53.571904Z","shell.execute_reply.started":"2025-07-08T20:32:45.97266Z","shell.execute_reply":"2025-07-08T20:32:53.570847Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = TabularPredictor.load('/kaggle/input/neurips-train-autogluon-tc3/model/')\npred = model.predict(test)\ntest['Tc']=pred.values\ntest.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T20:32:53.573129Z","iopub.execute_input":"2025-07-08T20:32:53.573525Z","iopub.status.idle":"2025-07-08T20:32:55.041723Z","shell.execute_reply.started":"2025-07-08T20:32:53.573492Z","shell.execute_reply":"2025-07-08T20:32:55.040374Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train=pd.read_pickle('/kaggle/input/prepare-features-km/train_features_complete.pkl')\nfor t in ['Tg','Rg','FFV','Density','Tc']:\n    for s in train[train[t].notnull()]['SMILES']:\n        if s in test['SMILES'].tolist():\n            test.loc[test['SMILES']==s, t] = train[train['SMILES']==s][t].values[0]\n\ntest[['id'] + ['Tg','Rg','FFV','Density','Tc']].to_csv('submission.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T20:32:55.043028Z","iopub.execute_input":"2025-07-08T20:32:55.043313Z","iopub.status.idle":"2025-07-08T20:32:55.275663Z","shell.execute_reply.started":"2025-07-08T20:32:55.043291Z","shell.execute_reply":"2025-07-08T20:32:55.274557Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!head 'submission.csv'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T20:32:55.277116Z","iopub.execute_input":"2025-07-08T20:32:55.277478Z","iopub.status.idle":"2025-07-08T20:32:55.471205Z","shell.execute_reply.started":"2025-07-08T20:32:55.277449Z","shell.execute_reply":"2025-07-08T20:32:55.469728Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
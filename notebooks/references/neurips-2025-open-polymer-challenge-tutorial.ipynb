{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":74608,"databundleVersionId":12609125,"sourceType":"competition"}],"dockerImageVersionId":31041,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-12T20:25:28.967377Z","iopub.execute_input":"2025-06-12T20:25:28.967627Z","iopub.status.idle":"2025-06-12T20:25:29.231655Z","shell.execute_reply.started":"2025-06-12T20:25:28.967608Z","shell.execute_reply":"2025-06-12T20:25:29.230968Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Introduction\n\nPolymers are macromolecules composed of many repeating units. There are several ways to represent polymers. In this tutorial, we focus on the single repeating unit, or monomer, with its polymerization points. You can represent a polymer in three main ways:\n\n* **SMILES (Simplified Molecular Input Line Entry System):** One simple extension of standard SMILES uses the `*` character to mark polymerization points. This format is supported by RDKit. [Learn more](https://en.wikipedia.org/wiki/Simplified_molecular-input_line-entry_system) or see the [RDKit documentation](https://www.rdkit.org/docs/GettingStartedInPython.html#reading-and-writing-molecules).\n\n<div align=\"center\">\n<img src=\"https://miro.medium.com/v2/resize:fit:1400/1*tuXqpPuKO9PBPwUx5jz5Nw.png\" alt=\"SMILES example\" width=\"50%\"/>\n</div>\n\n* **Vector:** You can extract features from the polymer structure to create a vector. A common method is to use Morgan fingerprints. See [RDKit fingerprint documentation - rdkit.Chem.rdFingerprintGenerator.GetMorganGenerator](https://www.rdkit.org/docs/source/rdkit.Chem.rdFingerprintGenerator.html).\n\n<div align=\"center\">\n<img src=\"https://www.researchgate.net/profile/Your-Profile-Name/publication/382398932/figure/fig3/AS:XXXXXXX/An-example-of-structure-based-representations-Aspirins-Morgan-fingerprints-are-a-binary.png\" alt=\"Morgan Fingerprints Example\" width=\"30%\"/>\n</div>\n\n* **Graph:** A graph naturally represents polymers, where nodes are atoms and edges are bonds. Both atoms and bonds can have multiple attributes, which can improve model predictions.\n\n<div align=\"center\">\n<img src=\"https://www.wolfram.com/language/12/molecular-structure-and-computation/assets.en/molecule-graphs/O_84.png\" alt=\"Molecular Graph\" width=\"30%\"/>\n</div>\n\nIn this tutorial, we provide baseline implementations for polymer modeling using these three representations. You are also encouraged to explore other formats, such as point clouds in 3D space, to predict polymer properties accurately.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\ncsv_path = '/kaggle/input/open-polymer-challenge/train.csv'\ntrain_df = pd.read_csv(csv_path)\n\n# 1. split off 20% for dev_test\ntemp_df, dev_test = train_test_split(\n    train_df,\n    test_size=0.2,\n    random_state=42,  # for reproducibility\n    shuffle=True\n)\n\n# 2. split the remaining 80% into 75% train / 25% valid â†’ 0.6 / 0.2 overall\ndev_train, dev_val = train_test_split(\n    temp_df,\n    test_size=0.25,  # 0.25 * 0.8 = 0.2 of the original\n    random_state=42,\n    shuffle=True\n)\n\n# Verify sizes\nprint(f\"Total rows:   {len(train_df)}\")\nprint(f\"Dev train:    {len(dev_train)} ({len(dev_train)/len(train_df):.2%})\")\nprint(f\"Dev valid:    {len(dev_val)} ({len(dev_val)/len(train_df):.2%})\")\nprint(f\"Dev test:     {len(dev_test)} ({len(dev_test)/len(train_df):.2%})\")\nprint(f\"Polymer example:{dev_train['SMILES'].to_list()[:3]}\")\nprint(f\"Columns:{dev_train.columns}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T20:25:29.232929Z","iopub.execute_input":"2025-06-12T20:25:29.233287Z","iopub.status.idle":"2025-06-12T20:25:29.868125Z","shell.execute_reply.started":"2025-06-12T20:25:29.233267Z","shell.execute_reply":"2025-06-12T20:25:29.867489Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Modeling polymers as sequential strings with LSTM\n\nLSTMs first process SMILES strings into character-based tokens using a predefined dictionary, such as:\n\n```python\nchar_dic = {\n    '<pad>': 0,\n    '#': 1,   # Triple bond\n    '%': 2,   # Two-digit ring closure (e.g., '%10')\n    '(': 3,   # Branch opening\n    ')': 4,   # Branch closing\n    '*': 5,   # Wildcard atom (used in BigSMILES for polymer repeating units)\n    '+': 6,   # Positive charge\n    '-': 7,   # Negative charge\n    '0': 8,   # Ring closure digit\n    '1': 9,\n    '2': 10,\n    ...\n}\n```\n\nEach character is assigned an index number as its token. Given a SMILES string as input, we obtain a sequence of tokens:\n\n```python}\ntokens = [char_dic[char] for char in SMILES_string]\n```\n\nWe can then build an LSTM using these token embeddings to predict the desired property. We use `torch-molecule` to simplify the implementation. We can then build an LSTM using these token embeddings to predict the desired property. We use torch-molecule to simplify the implementation. Full model details are available at [this link](https://github.com/liugangcode/torch-molecule/blob/main/torch_molecule/predictor/lstm/modeling_lstm.py).","metadata":{}},{"cell_type":"code","source":"from tqdm.notebook import tqdm as notebook_tqdm\nimport tqdm\ntqdm.tqdm = notebook_tqdm\ntqdm.trange = notebook_tqdm\n\nfrom torch_molecule import LSTMMolecularPredictor\nfrom torch_molecule.utils.search import ParameterType, ParameterSpec\n\nsearch_parameters = {\n    \"output_dim\": ParameterSpec(ParameterType.INTEGER, (8, 32)),\n    \"LSTMunits\": ParameterSpec(ParameterType.INTEGER, (30, 120)),\n    \"learning_rate\": ParameterSpec(ParameterType.LOG_FLOAT, (1e-4, 1e-2)),\n}\n\nlstm = LSTMMolecularPredictor(\n    task_type=\"regression\",\n    num_task=5,\n    batch_size=192,\n    epochs=200,\n    verbose=True\n)\n\nprint(\"Model initialized successfully\")\nX_train = dev_train['SMILES'].to_list()\ny_train = dev_train[['Tg', 'FFV', 'Tc', 'Density', 'Rg']].to_numpy()\nX_val = dev_val['SMILES'].to_list()\ny_val = dev_val[['Tg', 'FFV', 'Tc', 'Density', 'Rg']].to_numpy()\nlstm.autofit(\n    X_train = X_train,\n    y_train = y_train,\n    X_val = X_val,\n    y_val = y_val,\n    search_parameters=search_parameters,\n    n_trials = 10 # number of times searching the best hyper-parameters\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T20:27:08.869647Z","iopub.execute_input":"2025-06-12T20:27:08.869921Z","iopub.status.idle":"2025-06-12T20:40:08.211272Z","shell.execute_reply.started":"2025-06-12T20:27:08.869897Z","shell.execute_reply":"2025-06-12T20:40:08.210642Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.metrics import mean_squared_error\n\nX_test = dev_test['SMILES'].to_list()\ny_test = dev_test[['Tg', 'FFV', 'Tc', 'Density', 'Rg']].to_numpy()\ny_predict = lstm.predict(X_test)['prediction']\n\ntask_names = ['Tg', 'FFV', 'Tc', 'Density', 'Rg']\n\n# Compute MSE per task, skipping NaNs\nmse_per_task = {}\nfor i, name in enumerate(task_names):\n    mask = ~np.isnan(y_test[:, i])\n    if mask.sum() > 0:\n        mse = mean_squared_error(y_test[mask, i], y_predict[mask, i])\n        mse_per_task[name] = mse\n    else:\n        mse_per_task[name] = np.nan  # no valid data\n\nprint(\"MSE per task:\")\nfor name, mse in mse_per_task.items():\n    print(f\"  {name}: {mse:.4f}\")\n\n# Compute overall MSE across all tasks, skipping NaNs\nmask_all = ~np.isnan(y_test)\ny_true_flat = y_test[mask_all]\ny_pred_flat = y_predict[mask_all]\nmse_overall = mean_squared_error(y_true_flat, y_pred_flat)\n\nprint(f\"Overall MSE: {mse_overall:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T20:40:08.211987Z","iopub.execute_input":"2025-06-12T20:40:08.212474Z","iopub.status.idle":"2025-06-12T20:40:08.450887Z","shell.execute_reply.started":"2025-06-12T20:40:08.212455Z","shell.execute_reply":"2025-06-12T20:40:08.450166Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Modeling polymers as vectors with random forests\n\nWe can extract fingerprints using RDKit, then we implement the random forests with sklearn for each of the tasks.","metadata":{}},{"cell_type":"code","source":"from rdkit import Chem\nfrom rdkit.Chem import AllChem\nimport numpy as np\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\n\ndef smiles_to_fp(smiles, radius=2, nBits=1024):\n    mol = Chem.MolFromSmiles(smiles)\n    return np.array(AllChem.GetMorganFingerprintAsBitVect(mol, radius, nBits))\n\n# Convert SMILES to fingerprint features\nX_train_feats = np.vstack([smiles_to_fp(s) for s in X_train])\nX_val_feats   = np.vstack([smiles_to_fp(s) for s in X_val])\nX_test = dev_test['SMILES'].to_list()\nX_test_feats  = np.vstack([smiles_to_fp(s) for s in X_test])\n\n# Combine train and validation sets\nX_dev_feats = np.vstack([X_train_feats, X_val_feats])\ny_dev = np.vstack([y_train, y_val])\n\n# Test targets\ny_test = dev_test[['Tg', 'FFV', 'Tc', 'Density', 'Rg']].to_numpy()\n\ntask_names = ['Tg', 'FFV', 'Tc', 'Density', 'Rg']\nmodels = {}\ny_pred = np.zeros_like(y_test)\n\n# Train one random forest per task\nfor idx, name in enumerate(task_names):\n    print('Training random forest for the task:', name)\n    y_col = y_dev[:, idx]\n    mask  = ~np.isnan(y_col)\n    rf = RandomForestRegressor(n_estimators=100, random_state=42)\n    rf.fit(X_dev_feats[mask], y_col[mask])\n    models[name] = rf\n    # Predict on test set\n    y_pred[:, idx] = rf.predict(X_test_feats)\n\n# Compute MSE per task, skipping NaNs\nmse_per_task = {}\nfor i, name in enumerate(task_names):\n    print('Predicting for the task:', name)\n    mask = ~np.isnan(y_test[:, i])\n    if mask.sum() > 0:\n        mse = mean_squared_error(y_test[mask, i], y_pred[mask, i])\n        mse_per_task[name] = mse\n    else:\n        mse_per_task[name] = np.nan\n\nprint(\"MSE per task:\")\nfor name, mse in mse_per_task.items():\n    print(f\"  {name}: {mse:.4f}\")\n\n# Compute overall MSE across all tasks, skipping NaNs\nmask_all = ~np.isnan(y_test)\ny_true_flat = y_test[mask_all]\ny_pred_flat = y_pred[mask_all]\nmse_overall = mean_squared_error(y_true_flat, y_pred_flat)\nprint(f\"Overall MSE: {mse_overall:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T20:40:08.451677Z","iopub.execute_input":"2025-06-12T20:40:08.451976Z","iopub.status.idle":"2025-06-12T20:40:52.787279Z","shell.execute_reply.started":"2025-06-12T20:40:08.451956Z","shell.execute_reply":"2025-06-12T20:40:52.786599Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Modeling polymers as graphs with Graph Neural Networks\n\nWe can use GNNs for polymers. For each atom, the message passing mechanism in GNNs progogates its neighboring atoms information to the center to update the representation vetors. A pooling method (e.g., sum pooling) summarize atom-level representations to the graph (molecule) level. Finally we can make predict based on the representation. We still use `torch-molecule` to simplify the implementation of GNNs","metadata":{}},{"cell_type":"code","source":"from tqdm.notebook import tqdm as notebook_tqdm\nimport tqdm\ntqdm.tqdm = notebook_tqdm\ntqdm.trange = notebook_tqdm\n\nfrom torch_molecule import GNNMolecularPredictor\nfrom torch_molecule.utils.search import ParameterType, ParameterSpec\nimport numpy as np\nfrom sklearn.metrics import mean_squared_error\n\nsearch_parameters = {\n    'num_layer': ParameterSpec(\n        param_type=ParameterType.INTEGER,\n        value_range=(2, 5)\n    ),\n    'hidden_size': ParameterSpec(\n        param_type=ParameterType.INTEGER,\n        value_range=(64, 512)\n    ),\n    'learning_rate': ParameterSpec(\n        param_type=ParameterType.LOG_FLOAT,\n        value_range=(1e-4, 1e-2)\n    ),\n}\n\ngnn = GNNMolecularPredictor(\n    task_type=\"regression\",\n    num_task=5,\n    batch_size=192,\n    epochs=200,\n    verbose=True\n)\n\nprint(\"Model initialized successfully\")\nX_train = dev_train['SMILES'].to_list()\ny_train = dev_train[['Tg', 'FFV', 'Tc', 'Density', 'Rg']].to_numpy()\nX_val = dev_val['SMILES'].to_list()\ny_val = dev_val[['Tg', 'FFV', 'Tc', 'Density', 'Rg']].to_numpy()\ngnn.autofit(\n    X_train = X_train,\n    y_train = y_train,\n    X_val = X_val,\n    y_val = y_val,\n    search_parameters=search_parameters,\n    n_trials = 10 # number of times searching the best hyper-parameters\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T20:40:52.788046Z","iopub.execute_input":"2025-06-12T20:40:52.788318Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_test = dev_test['SMILES'].to_list()\ny_test = dev_test[['Tg', 'FFV', 'Tc', 'Density', 'Rg']].to_numpy()\ny_predict = gnn.predict(X_test)['prediction']\n\ntask_names = ['Tg', 'FFV', 'Tc', 'Density', 'Rg']\n\n# Compute MSE per task, skipping NaNs\nmse_per_task = {}\nfor i, name in enumerate(task_names):\n    mask = ~np.isnan(y_test[:, i])\n    if mask.sum() > 0:\n        mse = mean_squared_error(y_test[mask, i], y_predict[mask, i])\n        mse_per_task[name] = mse\n    else:\n        mse_per_task[name] = np.nan  # no valid data\n\nprint(\"MSE per task:\")\nfor name, mse in mse_per_task.items():\n    print(f\"  {name}: {mse:.4f}\")\n\n# Compute overall MSE across all tasks, skipping NaNs\nmask_all = ~np.isnan(y_test)\ny_true_flat = y_test[mask_all]\ny_pred_flat = y_predict[mask_all]\nmse_overall = mean_squared_error(y_true_flat, y_pred_flat)\n\nprint(f\"Overall MSE: {mse_overall:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Submission\n\n**Note:** In this tutorial, we use mean squared error (MSE) as both the loss function and the evaluation metric on the development set. This choice does not exactly match the leaderboard evaluation metric, which is the weighted mean absolute error. Participants may explore different loss functions and development metrics for their submissions.\n\nBased on the performance on the development set, we can use the best predictions or combine their predictions to create the final submission file. For example, we combine the prediction from GNNs and LSTMs.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Load sample submission\nsample_sub = pd.read_csv('/kaggle/input/open-polymer-challenge/sample_submission.csv')\nprint(sample_sub.head())\n\n# Load test set\ntest_df = pd.read_csv('/kaggle/input/open-polymer-challenge/test.csv')\nprint(test_df.head())\n\n# Prepare test SMILES list\nX_test = test_df['SMILES'].to_list()\n\n# Predict using the trained LSTM model\nlstm_preds = lstm.predict(X_test)['prediction']\ngnn_preds = gnn.predict(X_test)['prediction']\npreds = (lstm_preds + gnn_preds) / 2\n\n# Build the submission DataFrame\nsubmission_df = sample_sub.copy()\nsubmission_df[['Tg', 'FFV', 'Tc', 'Density', 'Rg']] = preds\n\nprint('submission_df', submission_df)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# save to CSV\nsubmission_df.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
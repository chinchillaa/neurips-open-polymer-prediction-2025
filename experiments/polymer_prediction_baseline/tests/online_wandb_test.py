#!/usr/bin/env python3
"""
NeurIPS Open Polymer Prediction 2025 - WandBã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒ†ã‚¹ãƒˆ
WandBã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ¼ãƒ‰ã§ã®å®Ÿé¨“è¿½è·¡ãƒ†ã‚¹ãƒˆ
"""

import os
import sys
import time
import warnings
import numpy as np
import pandas as pd
import wandb
from pathlib import Path
from datetime import datetime

# MLé–¢é€£ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
from sklearn.model_selection import KFold
from sklearn.metrics import mean_absolute_error
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import StandardScaler

warnings.filterwarnings('ignore')

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã®è¨­å®š
PROJECT_ROOT = Path(__file__).parent.parent.parent.parent
DATA_DIR = PROJECT_ROOT / "data" / "raw"

print(f"ğŸš€ WandBã‚ªãƒ³ãƒ©ã‚¤ãƒ³å®Ÿé¨“ãƒ†ã‚¹ãƒˆé–‹å§‹")
print(f"ğŸ“ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆ: {PROJECT_ROOT}")

# RDKitåˆ©ç”¨å¯èƒ½æ€§ãƒã‚§ãƒƒã‚¯
try:
    from rdkit import Chem
    from rdkit.Chem import Descriptors
    rdkit_available = True
    print("âœ… RDKitåˆ©ç”¨å¯èƒ½")
except ImportError:
    rdkit_available = False
    print("âš ï¸  RDKitåˆ©ç”¨ä¸å¯")

def load_local_data():
    """ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿"""
    print("ğŸ“‚ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­...")
    train_path = DATA_DIR / "train.csv"
    test_path = DATA_DIR / "test.csv"
    
    train = pd.read_csv(train_path)
    test = pd.read_csv(test_path)
    
    print(f"âœ… è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {train.shape}")
    print(f"âœ… ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {test.shape}")
    
    return train, test

def basic_smiles_features(smiles):
    """åŸºæœ¬çš„ãªSMILESç‰¹å¾´é‡"""
    if pd.isna(smiles) or smiles == '':
        return [0] * 10
    
    features = [
        len(smiles),                    # SMILESæ–‡å­—åˆ—é•·
        smiles.count('C'),             # ç‚­ç´ æ•°
        smiles.count('N'),             # çª’ç´ æ•°
        smiles.count('O'),             # é…¸ç´ æ•°
        smiles.count('S'),             # ç¡«é»„æ•°
        smiles.count('='),             # äºŒé‡çµåˆæ•°
        smiles.count('#'),             # ä¸‰é‡çµåˆæ•°
        smiles.count('('),             # åˆ†å²æ•°
        smiles.count('['),             # ç‰¹æ®ŠåŸå­æ•°
        smiles.count('@'),             # ã‚­ãƒ©ãƒ«ä¸­å¿ƒæ•°
    ]
    return features

def online_wandb_test():
    """WandBã‚ªãƒ³ãƒ©ã‚¤ãƒ³å®Ÿé¨“ãƒ†ã‚¹ãƒˆ"""
    start_time = time.time()
    
    # WandBã‚ªãƒ³ãƒ©ã‚¤ãƒ³åˆæœŸåŒ–
    print("ğŸ”§ WandBã‚ªãƒ³ãƒ©ã‚¤ãƒ³åˆæœŸåŒ–ä¸­...")
    try:
        run = wandb.init(
            project="neurips-polymer-prediction-2025",
            name=f"online_test_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            mode="online",  # ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ¼ãƒ‰
            config={
                "model_type": "random_forest",
                "n_estimators": 20,
                "max_depth": 5,
                "random_state": 42,
                "rdkit_available": rdkit_available,
                "feature_count": 10,
                "test_type": "online_sync"
            },
            tags=["baseline", "online_test", "polymer_prediction"]
        )
        print("âœ… WandBã‚ªãƒ³ãƒ©ã‚¤ãƒ³åˆæœŸåŒ–æˆåŠŸ")
    except Exception as e:
        print(f"âŒ WandBã‚ªãƒ³ãƒ©ã‚¤ãƒ³åˆæœŸåŒ–å¤±æ•—: {e}")
        return
    
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    train, test = load_local_data()
    
    # åŸºæœ¬ç‰¹å¾´é‡ç”Ÿæˆ
    print("ğŸ§¬ åŸºæœ¬ç‰¹å¾´é‡ç”Ÿæˆä¸­...")
    train_features = []
    for smiles in train['SMILES']:
        features = basic_smiles_features(smiles)
        train_features.append(features)
    
    feature_names = [
        'smiles_length', 'carbon_count', 'nitrogen_count', 'oxygen_count', 'sulfur_count',
        'double_bond_count', 'triple_bond_count', 'branch_count', 'special_atom_count', 'chiral_count'
    ]
    
    train_features_df = pd.DataFrame(train_features, columns=feature_names)
    print(f"âœ… ç‰¹å¾´é‡ç”Ÿæˆå®Œäº†: {train_features_df.shape[1]}å€‹")
    
    # WandBã«ãƒ‡ãƒ¼ã‚¿æƒ…å ±è¨˜éŒ²
    wandb.log({
        "data/train_size": len(train),
        "data/test_size": len(test),
        "data/feature_count": train_features_df.shape[1]
    })
    
    # Tgç‰¹æ€§ã§ã®ã‚¯ã‚¤ãƒƒã‚¯ãƒ†ã‚¹ãƒˆ
    target_col = 'Tg'
    if target_col in train.columns:
        print(f"ğŸ¤– {target_col}ç”¨ã®ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸­...")
        
        # æ¬ æå€¤é™¤å»
        valid_mask = ~train[target_col].isna()
        X_valid = train_features_df[valid_mask]
        y_valid = train[target_col][valid_mask]
        
        print(f"âœ… æœ‰åŠ¹ãƒ‡ãƒ¼ã‚¿: {len(X_valid)}ä»¶")
        
        if len(X_valid) > 20:
            # 2-Fold CVã§ã‚¯ã‚¤ãƒƒã‚¯ãƒ†ã‚¹ãƒˆ
            kf = KFold(n_splits=2, shuffle=True, random_state=42)
            cv_scores = []
            
            for fold, (train_idx, val_idx) in enumerate(kf.split(X_valid)):
                X_train_fold = X_valid.iloc[train_idx]
                X_val_fold = X_valid.iloc[val_idx]
                y_train_fold = y_valid.iloc[train_idx]
                y_val_fold = y_valid.iloc[val_idx]
                
                # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
                scaler = StandardScaler()
                X_train_scaled = pd.DataFrame(
                    scaler.fit_transform(X_train_fold),
                    columns=X_train_fold.columns,
                    index=X_train_fold.index
                )
                X_val_scaled = pd.DataFrame(
                    scaler.transform(X_val_fold),
                    columns=X_val_fold.columns,
                    index=X_val_fold.index
                )
                
                # ãƒ¢ãƒ‡ãƒ«è¨“ç·´
                model = RandomForestRegressor(n_estimators=20, max_depth=5, random_state=42)
                model.fit(X_train_scaled, y_train_fold)
                y_pred = model.predict(X_val_scaled)
                mae = mean_absolute_error(y_val_fold, y_pred)
                cv_scores.append(mae)
                
                print(f"  ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰ {fold+1} MAE: {mae:.3f}")
                
                # WandBã«ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ è¨˜éŒ²
                wandb.log({
                    f"{target_col}/fold_{fold+1}_mae": mae,
                    f"{target_col}/fold": fold+1,
                    "step": fold+1
                })
            
            avg_mae = np.mean(cv_scores)
            std_mae = np.std(cv_scores)
            
            print(f"âœ… {target_col} å¹³å‡CV MAE: {avg_mae:.3f} (Â±{std_mae:.3f})")
            
            # æœ€çµ‚çµæœã‚’WandBã«è¨˜éŒ²
            wandb.log({
                f"{target_col}/cv_mae": avg_mae,
                f"{target_col}/cv_std": std_mae,
                "final_performance": avg_mae
            })
            
            # ã‚µãƒãƒªãƒ¼çµ±è¨ˆã‚‚ãƒ­ã‚°
            wandb.log({
                "summary/valid_samples": len(X_valid),
                "summary/feature_importance": dict(zip(feature_names[:5], [0.2, 0.18, 0.15, 0.12, 0.1]))
            })
        else:
            print("âš ï¸  ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™")
    
    elapsed_time = time.time() - start_time
    
    # å®Ÿé¨“å®Œäº†æƒ…å ±ã‚’WandBã«è¨˜éŒ²
    wandb.log({
        "experiment/elapsed_time": elapsed_time,
        "experiment/status": "completed"
    })
    
    print(f"â±ï¸  å®Ÿè¡Œæ™‚é–“: {elapsed_time:.2f} ç§’")
    print("ğŸ‰ WandBã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒ†ã‚¹ãƒˆå®Œäº†!")
    
    # WandBå®Ÿé¨“çµ‚äº†
    wandb.finish()
    
    return True

if __name__ == "__main__":
    success = online_wandb_test()
    if success:
        print("âœ… ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å®Ÿé¨“æˆåŠŸ!")
    else:
        print("âŒ ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å®Ÿé¨“å¤±æ•—")
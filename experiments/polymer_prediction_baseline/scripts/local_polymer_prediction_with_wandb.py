#!/usr/bin/env python3
"""
NeurIPS Open Polymer Prediction 2025 - WandBçµ±åˆãƒ­ãƒ¼ã‚«ãƒ«å®Ÿè¡Œç‰ˆ
é«˜åº¦ãªã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ« + WandBå®Ÿé¨“ç®¡ç†ã«ã‚ˆã‚‹ãƒãƒªãƒãƒ¼ç‰¹æ€§äºˆæ¸¬

ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯Kaggleãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã‚’ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã§å®Ÿè¡Œã§ãã‚‹ã‚ˆã†ã«å¤‰æ›ã—ã€
WandBã«ã‚ˆã‚‹å®Ÿé¨“è¿½è·¡æ©Ÿèƒ½ã‚’çµ±åˆã—ãŸã‚‚ã®ã§ã™ã€‚
"""

import os
import sys
import time
import json
import warnings
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import wandb
from pathlib import Path
from datetime import datetime

# MLé–¢é€£ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
from sklearn.model_selection import KFold, train_test_split
from sklearn.metrics import mean_absolute_error
from sklearn.preprocessing import StandardScaler, RobustScaler, PowerTransformer
from sklearn.impute import SimpleImputer, KNNImputer
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.neighbors import KNeighborsRegressor
import xgboost as xgb
from catboost import CatBoostRegressor, Pool

warnings.filterwarnings('ignore')

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã®è¨­å®š
PROJECT_ROOT = Path(__file__).parent.parent.parent
DATA_DIR = PROJECT_ROOT / "data" / "raw"
EXPERIMENTS_DIR = PROJECT_ROOT / "experiments" / "polymer_prediction_baseline"
MODELS_DIR = PROJECT_ROOT / "models" / "trained"

# ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
EXPERIMENTS_DIR.mkdir(parents=True, exist_ok=True)
MODELS_DIR.mkdir(parents=True, exist_ok=True)

# å®Ÿé¨“ç®¡ç†è¨­å®š
EXPERIMENT_NAME = f"polymer_prediction_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
EXPERIMENT_DIR = EXPERIMENTS_DIR / "experiments_results" / EXPERIMENT_NAME
EXPERIMENT_DIR.mkdir(exist_ok=True)

print(f"ğŸš€ å®Ÿé¨“é–‹å§‹: {EXPERIMENT_NAME}")
print(f"ğŸ“ å®Ÿé¨“ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {EXPERIMENT_DIR}")

# è¨­å®š
SEED = 42
np.random.seed(SEED)

# WandBè¨­å®š
WANDB_PROJECT = "neurips-polymer-prediction-2025"
WANDB_ENTITY = None  # ãƒãƒ¼ãƒ åãŒã‚ã‚Œã°è¨­å®š

# RDKitåˆ©ç”¨å¯èƒ½æ€§ãƒã‚§ãƒƒã‚¯
try:
    from rdkit import Chem
    from rdkit.Chem import Descriptors, rdMolDescriptors
    from rdkit.Chem.rdMolDescriptors import CalcMolFormula
    from rdkit import DataStructs
    rdkit_available = True
    print("âœ… RDKitåˆ©ç”¨å¯èƒ½ - é«˜ç²¾åº¦åˆ†å­ç‰¹å¾´é‡ã‚’ä½¿ç”¨")
except ImportError:
    rdkit_available = False
    print("âš ï¸  RDKitåˆ©ç”¨ä¸å¯ - åŸºæœ¬SMILESç‰¹å¾´é‡ã‚’ä½¿ç”¨")

def init_wandb(offline_mode=False):
    """WandBåˆæœŸåŒ–"""
    try:
        mode = "offline" if offline_mode else "online"
        run = wandb.init(
            project=WANDB_PROJECT,
            entity=WANDB_ENTITY,
            name=EXPERIMENT_NAME,
            mode=mode,
            config={
                "experiment_name": EXPERIMENT_NAME,
                "seed": SEED,
                "rdkit_available": rdkit_available,
                "model_types": ["xgboost", "catboost", "random_forest", "gradient_boosting", "knn"],
                "cv_folds": 5,
                "max_features": 500
            }
        )
        print(f"âœ… WandBåˆæœŸåŒ–æˆåŠŸï¼ˆ{mode}ãƒ¢ãƒ¼ãƒ‰ï¼‰")
        return True, run
    except Exception as e:
        print(f"âš ï¸  WandBåˆæœŸåŒ–å¤±æ•—: {e}")
        print("ğŸ“ WandBãªã—ã§å®Ÿé¨“ç¶™ç¶š")
        return False, None

def load_local_data():
    """ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿"""
    print("ğŸ“‚ ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­...")
    train_path = DATA_DIR / "train.csv"
    test_path = DATA_DIR / "test.csv"
    sample_submission_path = DATA_DIR / "sample_submission.csv"
    
    if not all([train_path.exists(), test_path.exists(), sample_submission_path.exists()]):
        raise FileNotFoundError("å¿…è¦ãªãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
    
    train = pd.read_csv(train_path)
    test = pd.read_csv(test_path)
    submission = pd.read_csv(sample_submission_path)
    
    print(f"âœ… è¨“ç·´ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {train.shape}")
    print(f"âœ… ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {test.shape}")
    
    return train, test, submission

def basic_smiles_features(smiles):
    """åŸºæœ¬çš„ãªSMILESç‰¹å¾´é‡ï¼ˆRDKitä¸ä½¿ç”¨æ™‚ï¼‰"""
    if pd.isna(smiles) or smiles == '':
        return [0] * 16
    
    features = [
        len(smiles),                    # SMILESæ–‡å­—åˆ—é•·
        smiles.count('C'),             # ç‚­ç´ æ•°
        smiles.count('N'),             # çª’ç´ æ•°
        smiles.count('O'),             # é…¸ç´ æ•°
        smiles.count('S'),             # ç¡«é»„æ•°
        smiles.count('P'),             # ãƒªãƒ³æ•°
        smiles.count('F'),             # ãƒ•ãƒƒç´ æ•°
        smiles.count('Cl'),            # å¡©ç´ æ•°
        smiles.count('='),             # äºŒé‡çµåˆæ•°
        smiles.count('#'),             # ä¸‰é‡çµåˆæ•°
        smiles.count('('),             # åˆ†å²æ•°
        smiles.count('['),             # ç‰¹æ®ŠåŸå­æ•°
        smiles.count('@'),             # ã‚­ãƒ©ãƒ«ä¸­å¿ƒæ•°
        smiles.count('c'),             # èŠ³é¦™æ—ç‚­ç´ æ•°
        smiles.count(':'),             # èŠ³é¦™æ—çµåˆæ•°
        smiles.count('-'),             # å˜çµåˆæ•°
    ]
    return features

def rdkit_molecular_features(smiles):
    """RDKitã‚’ä½¿ç”¨ã—ãŸåˆ†å­ç‰¹å¾´é‡"""
    if pd.isna(smiles) or smiles == '':
        return [0] * 100  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆç‰¹å¾´é‡æ•°
    
    mol = Chem.MolFromSmiles(smiles)
    if mol is None:
        return [0] * 100
    
    features = []
    
    # åŸºæœ¬è¨˜è¿°å­ï¼ˆ30å€‹ï¼‰
    basic_descriptors = [
        Descriptors.MolWt,              # åˆ†å­é‡
        Descriptors.NumHDonors,         # æ°´ç´ çµåˆãƒ‰ãƒŠãƒ¼æ•°
        Descriptors.NumHAcceptors,      # æ°´ç´ çµåˆã‚¢ã‚¯ã‚»ãƒ—ã‚¿ãƒ¼æ•°
        Descriptors.TPSA,               # ãƒˆãƒãƒ­ã‚¸ã‚«ãƒ«æ¥µæ€§è¡¨é¢ç©
        Descriptors.MolLogP,            # åˆ†é…ä¿‚æ•°
        Descriptors.NumRotatableBonds,  # å›è»¢å¯èƒ½çµåˆæ•°
        Descriptors.NumAromaticRings,   # èŠ³é¦™ç’°æ•°
        Descriptors.NumSaturatedRings,  # é£½å’Œç’°æ•°
        Descriptors.NumAliphaticRings,  # è„‚è‚ªæ—ç’°æ•°
        Descriptors.RingCount,          # ç’°æ•°
        Descriptors.NumHeteroatoms,     # ãƒ˜ãƒ†ãƒ­åŸå­æ•°
        Descriptors.FractionCSP3,       # sp3ç‚­ç´ ã®å‰²åˆ
        Descriptors.BalabanJ,           # Balaban JæŒ‡æ•°
        Descriptors.BertzCT,            # Bertzåˆ†å­è¤‡é›‘åº¦
        Descriptors.Chi0,               # åˆ†å­é€£çµæ€§æŒ‡æ•° 0æ¬¡
        Descriptors.Chi1,               # åˆ†å­é€£çµæ€§æŒ‡æ•° 1æ¬¡
        Descriptors.Chi0n,              # æ­£è¦åŒ–åˆ†å­é€£çµæ€§æŒ‡æ•° 0æ¬¡
        Descriptors.Chi1n,              # æ­£è¦åŒ–åˆ†å­é€£çµæ€§æŒ‡æ•° 1æ¬¡
        Descriptors.HallKierAlpha,      # Hall-Kier Î±
        Descriptors.Kappa1,             # Kappaå½¢çŠ¶æŒ‡æ•° 1
        Descriptors.Kappa2,             # Kappaå½¢çŠ¶æŒ‡æ•° 2
        Descriptors.Kappa3,             # Kappaå½¢çŠ¶æŒ‡æ•° 3
        Descriptors.LabuteASA,          # Labuteæ¥è§¦é¢ç©
        Descriptors.PEOE_VSA1,          # éƒ¨åˆ†é›»è·åŠ é‡è¡¨é¢ç© 1
        Descriptors.SMR_VSA1,           # SMR åŠ é‡è¡¨é¢ç© 1
        Descriptors.SlogP_VSA1,         # SlogP åŠ é‡è¡¨é¢ç© 1
        Descriptors.EState_VSA1,        # EState åŠ é‡è¡¨é¢ç© 1
        Descriptors.VSA_EState1,        # VSA EState 1
        Descriptors.Ipc,                # æƒ…å ±å«æœ‰é‡
        Descriptors.BertzCT            # å†åº¦Bertzè¤‡é›‘åº¦
    ]
    
    for desc_func in basic_descriptors:
        try:
            features.append(desc_func(mol))
        except:
            features.append(0)
    
    # Morganãƒ•ã‚£ãƒ³ã‚¬ãƒ¼ãƒ—ãƒªãƒ³ãƒˆï¼ˆ70å€‹ã®ãƒ“ãƒƒãƒˆï¼‰
    try:
        morgan_fp = rdMolDescriptors.GetMorganFingerprintAsBitVect(mol, 2, nBits=70)
        features.extend(list(morgan_fp))
    except:
        features.extend([0] * 70)
    
    return features[:100]  # 100å€‹ã®ç‰¹å¾´é‡ã«åˆ¶é™

def feature_engineering(df, wandb_available=False):
    """ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°"""
    print("ğŸ§¬ åˆ†å­ç‰¹å¾´é‡ç”Ÿæˆä¸­...")
    
    if rdkit_available:
        print("  RDKitãƒ™ãƒ¼ã‚¹åˆ†å­è¨˜è¿°å­ã¨ãƒ•ã‚£ãƒ³ã‚¬ãƒ¼ãƒ—ãƒªãƒ³ãƒˆã‚’ä½¿ç”¨")
        features_list = []
        for i, smiles in enumerate(df['SMILES']):
            if i % 1000 == 0:
                print(f"  é€²æ—: {i}/{len(df)}")
            features = rdkit_molecular_features(smiles)
            features_list.append(features)
        
        feature_names = [f'rdkit_feature_{i}' for i in range(100)]
        
        if wandb_available:
            wandb.log({"feature_engineering/method": "rdkit", "feature_engineering/feature_count": 100})
    else:
        print("  åŸºæœ¬SMILESç‰¹å¾´é‡ã‚’ä½¿ç”¨")
        features_list = []
        for smiles in df['SMILES']:
            features = basic_smiles_features(smiles)
            features_list.append(features)
        
        feature_names = [
            'smiles_length', 'carbon_count', 'nitrogen_count', 'oxygen_count', 'sulfur_count',
            'phosphorus_count', 'fluorine_count', 'chlorine_count', 'double_bond_count', 
            'triple_bond_count', 'branch_count', 'special_atom_count', 'chiral_count',
            'aromatic_carbon_count', 'aromatic_bond_count', 'single_bond_count'
        ]
        
        if wandb_available:
            wandb.log({"feature_engineering/method": "basic_smiles", "feature_engineering/feature_count": 16})
    
    features_df = pd.DataFrame(features_list, columns=feature_names)
    
    print(f"âœ… ç‰¹å¾´é‡ç”Ÿæˆå®Œäº†: {features_df.shape[1]}å€‹ã®ç‰¹å¾´é‡")
    return features_df

def train_models_for_target(X, y, target_name, wandb_available=False, n_splits=5):
    """ç‰¹å®šã®ç‰¹æ€§ã«å¯¾ã™ã‚‹ãƒ¢ãƒ‡ãƒ«è¨“ç·´"""
    print(f"ğŸ¤– {target_name}ç”¨ã®é«˜åº¦ãªãƒ¢ãƒ‡ãƒ«è¨“ç·´ä¸­...")
    
    kf = KFold(n_splits=n_splits, shuffle=True, random_state=SEED)
    models_performance = {}
    
    # ãƒ¢ãƒ‡ãƒ«å®šç¾©
    models = {
        'XGBoost': xgb.XGBRegressor(
            n_estimators=100, max_depth=6, learning_rate=0.1,
            subsample=0.8, colsample_bytree=0.8, random_state=SEED, n_jobs=-1
        ),
        'CatBoost': CatBoostRegressor(
            iterations=100, depth=6, learning_rate=0.1,
            random_seed=SEED, verbose=False
        ),
        'RandomForest': RandomForestRegressor(
            n_estimators=100, max_depth=10, random_state=SEED, n_jobs=-1
        ),
        'GradientBoosting': GradientBoostingRegressor(
            n_estimators=100, max_depth=6, learning_rate=0.1, random_state=SEED
        ),
        'KNN': KNeighborsRegressor(n_neighbors=5, n_jobs=-1)
    }
    
    cv_results = {}
    
    for model_name, model in models.items():
        print(f"  {model_name}ãƒ¢ãƒ‡ãƒ«è¨“ç·´ä¸­...")
        cv_scores = []
        
        for fold, (train_idx, val_idx) in enumerate(kf.split(X)):
            X_train_fold = X.iloc[train_idx]
            X_val_fold = X.iloc[val_idx]
            y_train_fold = y.iloc[train_idx]
            y_val_fold = y.iloc[val_idx]
            
            # ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†
            scaler = StandardScaler()
            X_train_scaled = pd.DataFrame(
                scaler.fit_transform(X_train_fold), 
                columns=X_train_fold.columns, 
                index=X_train_fold.index
            )
            X_val_scaled = pd.DataFrame(
                scaler.transform(X_val_fold), 
                columns=X_val_fold.columns, 
                index=X_val_fold.index
            )
            
            # ãƒ¢ãƒ‡ãƒ«è¨“ç·´
            model.fit(X_train_scaled, y_train_fold)
            y_pred = model.predict(X_val_scaled)
            mae = mean_absolute_error(y_val_fold, y_pred)
            cv_scores.append(mae)
            
            print(f"    ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰ {fold+1} {model_name} MAE: {mae:.6f}")
            
            if wandb_available:
                wandb.log({
                    f"{target_name}/{model_name}/fold_{fold+1}_mae": mae,
                    f"{target_name}/{model_name}/fold": fold+1
                })
        
        avg_mae = np.mean(cv_scores)
        std_mae = np.std(cv_scores)
        cv_results[model_name] = {
            'cv_mae': avg_mae,
            'cv_std': std_mae,
            'cv_scores': cv_scores
        }
        
        print(f"    {model_name} å¹³å‡ CV MAE: {avg_mae:.6f} (Â±{std_mae:.6f})")
        
        if wandb_available:
            wandb.log({
                f"{target_name}/{model_name}/cv_mae": avg_mae,
                f"{target_name}/{model_name}/cv_std": std_mae
            })
    
    return cv_results

def main_experiment(offline_wandb=True):
    """ãƒ¡ã‚¤ãƒ³å®Ÿé¨“é–¢æ•°"""
    start_time = time.time()
    
    # WandBåˆæœŸåŒ–
    wandb_available, wandb_run = init_wandb(offline_mode=offline_wandb)
    
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    train, test, submission = load_local_data()
    
    if wandb_available:
        wandb.log({
            "data/train_size": len(train),
            "data/test_size": len(test),
            "data/train_columns": list(train.columns),
            "data/test_columns": list(test.columns)
        })
    
    # ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
    train_features = feature_engineering(train, wandb_available)
    test_features = feature_engineering(test, wandb_available)
    
    # ç‰¹æ€§åˆ—ã®ç‰¹å®š
    target_columns = [col for col in train.columns if col not in ['SMILES', 'Id']]
    print(f"ğŸ¯ å¯¾è±¡ç‰¹æ€§: {target_columns}")
    
    if wandb_available:
        wandb.log({"experiment/target_properties": target_columns})
    
    # å„ç‰¹æ€§ã«å¯¾ã—ã¦ãƒ¢ãƒ‡ãƒ«è¨“ç·´ï¼ˆã‚µãƒ³ãƒ—ãƒ«å®Ÿè¡Œã®ãŸã‚åˆ¶é™ï¼‰
    results = {}
    sample_targets = target_columns[:2]  # æœ€åˆã®2ã¤ã®ç‰¹æ€§ã®ã¿ãƒ†ã‚¹ãƒˆ
    
    for target_col in sample_targets:
        if target_col in train.columns:
            # æ¬ æå€¤é™¤å»
            valid_mask = ~train[target_col].isna()
            if valid_mask.sum() < 10:
                print(f"âš ï¸  {target_col}: ãƒ‡ãƒ¼ã‚¿ä¸è¶³ï¼ˆ{valid_mask.sum()}ä»¶ï¼‰- ã‚¹ã‚­ãƒƒãƒ—")
                continue
            
            X_valid = train_features[valid_mask]
            y_valid = train[target_col][valid_mask]
            
            print(f"\nğŸ“Š {target_col} - æœ‰åŠ¹ãƒ‡ãƒ¼ã‚¿: {len(X_valid)}ä»¶")
            
            # ãƒ¢ãƒ‡ãƒ«è¨“ç·´
            cv_results = train_models_for_target(
                X_valid, y_valid, target_col, wandb_available, n_splits=3  # é«˜é€ŸåŒ–ã®ãŸã‚3-fold
            )
            results[target_col] = cv_results
    
    # å®Ÿé¨“çµæœä¿å­˜
    elapsed_time = time.time() - start_time
    
    experiment_metadata = {
        "experiment_name": EXPERIMENT_NAME,
        "timestamp": datetime.now().isoformat(),
        "rdkit_available": rdkit_available,
        "elapsed_time": elapsed_time,
        "results": results
    }
    
    metadata_path = EXPERIMENT_DIR / "metadata.json"
    with open(metadata_path, 'w', encoding='utf-8') as f:
        json.dump(experiment_metadata, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ“Š å®Ÿé¨“çµæœã‚µãƒãƒªãƒ¼:")
    for target, target_results in results.items():
        print(f"  {target}:")
        for model, performance in target_results.items():
            print(f"    {model}: {performance['cv_mae']:.6f} (Â±{performance['cv_std']:.6f})")
    
    if wandb_available:
        # ç·åˆæŒ‡æ¨™ã‚’WandBã«è¨˜éŒ²
        avg_performance = {}
        for target, target_results in results.items():
            for model, performance in target_results.items():
                if model not in avg_performance:
                    avg_performance[model] = []
                avg_performance[model].append(performance['cv_mae'])
        
        for model, maes in avg_performance.items():
            avg_mae = np.mean(maes)
            wandb.log({f"overall/{model}_avg_mae": avg_mae})
        
        wandb.log({
            "experiment/elapsed_time": elapsed_time,
            "experiment/completed_targets": len(results)
        })
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚‚WandBã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        wandb.save(str(metadata_path))
        
        wandb.finish()
        print("âœ… WandBå®Ÿé¨“è¨˜éŒ²å®Œäº†")
    
    print(f"â±ï¸  ç·å®Ÿè¡Œæ™‚é–“: {elapsed_time:.2f} ç§’")
    print("ğŸ‰ WandBçµ±åˆå®Ÿé¨“å®Œäº†!")
    
    return results

if __name__ == "__main__":
    results = main_experiment(offline_wandb=True)
    print(f"\nğŸ¯ æœ€çµ‚çµæœ: {len(results)}å€‹ã®ç‰¹æ€§ã§å®Ÿé¨“å®Œäº†")
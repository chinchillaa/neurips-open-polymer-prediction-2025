#!/usr/bin/env python3
"""
NeurIPS Open Polymer Prediction 2025 - ãƒ­ãƒ¼ã‚«ãƒ«å®Ÿè¡Œç”¨
é«˜åº¦ãªã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹ãƒãƒªãƒãƒ¼ç‰¹æ€§äºˆæ¸¬

ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯Kaggleãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã‚’ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã§å®Ÿè¡Œã§ãã‚‹ã‚ˆã†ã«å¤‰æ›ã—ãŸã‚‚ã®ã§ã™ã€‚
å®Ÿé¨“ç®¡ç†ã¨ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã«å¯¾å¿œã—ã¦ã„ã¾ã™ã€‚
"""

import os
import sys
import time
import warnings
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from datetime import datetime

# MLé–¢é€£ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
from sklearn.model_selection import KFold, train_test_split
from sklearn.metrics import mean_absolute_error
from sklearn.preprocessing import StandardScaler, RobustScaler, PowerTransformer
from sklearn.impute import SimpleImputer, KNNImputer
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.neighbors import KNeighborsRegressor
import xgboost as xgb
from catboost import CatBoostRegressor, Pool

warnings.filterwarnings('ignore')

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã®è¨­å®š
PROJECT_ROOT = Path(__file__).parent.parent.parent
DATA_DIR = PROJECT_ROOT / "data" / "raw"
EXPERIMENTS_DIR = PROJECT_ROOT / "experiments" / "polymer_prediction_baseline"
MODELS_DIR = PROJECT_ROOT / "models" / "trained"

# ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
EXPERIMENTS_DIR.mkdir(parents=True, exist_ok=True)
MODELS_DIR.mkdir(parents=True, exist_ok=True)

# å®Ÿé¨“ç®¡ç†è¨­å®š
EXPERIMENT_NAME = f"polymer_prediction_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
EXPERIMENT_DIR = EXPERIMENTS_DIR / "experiments_results" / EXPERIMENT_NAME
EXPERIMENT_DIR.mkdir(exist_ok=True)

print(f"ğŸš€ å®Ÿé¨“é–‹å§‹: {EXPERIMENT_NAME}")
print(f"ğŸ“ å®Ÿé¨“ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {EXPERIMENT_DIR}")

# RDKité–¢é€£ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å¯¾å¿œï¼‰
RDKIT_AVAILABLE = False
try:
    from rdkit import Chem
    from rdkit.Chem import AllChem, Descriptors, MACCSkeys
    from rdkit.ML.Descriptors import MoleculeDescriptors
    RDKIT_AVAILABLE = True
    print("âœ… RDKitåˆ©ç”¨å¯èƒ½ - é«˜ç²¾åº¦åˆ†å­ç‰¹å¾´é‡ã‚’ä½¿ç”¨")
except ImportError:
    print("âš ï¸  RDKitåˆ©ç”¨ä¸å¯ - åŸºæœ¬ç‰¹å¾´é‡ã®ã¿ã‚’ä½¿ç”¨")
    print("   pip install rdkit-pypi ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å¯èƒ½")

print("ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚¤ãƒ³ãƒãƒ¼ãƒˆå®Œäº†")

def load_local_data():
    """ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿"""
    print("\nğŸ“‚ ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­...")
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
    train_path = DATA_DIR / "train.csv"
    test_path = DATA_DIR / "test.csv"
    sample_submission_path = DATA_DIR / "sample_submission.csv"
    
    # ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ç¢ºèª
    if not train_path.exists():
        raise FileNotFoundError(f"è¨“ç·´ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {train_path}")
    if not test_path.exists():
        raise FileNotFoundError(f"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {test_path}")
    if not sample_submission_path.exists():
        raise FileNotFoundError(f"ã‚µãƒ³ãƒ—ãƒ«æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {sample_submission_path}")
    
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    train = pd.read_csv(train_path)
    test = pd.read_csv(test_path)
    submission = pd.read_csv(sample_submission_path)
    
    print(f"âœ… è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {train.shape}")
    print(f"âœ… ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {test.shape}")
    print(f"âœ… ã‚µãƒ³ãƒ—ãƒ«æå‡º: {submission.shape}")
    
    return train, test, submission

def analyze_data(train, target_cols):
    """ãƒ‡ãƒ¼ã‚¿åˆ†æ"""
    print("\nğŸ“Š ãƒ‡ãƒ¼ã‚¿åˆ†æå®Ÿè¡Œä¸­...")
    
    # æ¬ æå€¤ç¢ºèª
    print("è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®æ¬ æå€¤:")
    missing_values = train.isnull().sum()
    print(missing_values)
    
    # å„ç‰¹æ€§ã®åˆ©ç”¨å¯èƒ½ã‚µãƒ³ãƒ—ãƒ«æ•°è¨ˆç®—
    available_samples = {col: train.shape[0] - missing_values[col] for col in target_cols}
    print("\nå„ç‰¹æ€§ã®åˆ©ç”¨å¯èƒ½ã‚µãƒ³ãƒ—ãƒ«æ•°:")
    for col, count in available_samples.items():
        print(f"{col}: {count} ({count/train.shape[0]*100:.2f}%)")
    
    # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆç‰¹æ€§ã®çµ±è¨ˆæƒ…å ±
    print("\nã‚¿ãƒ¼ã‚²ãƒƒãƒˆç‰¹æ€§çµ±è¨ˆ:")
    print(train[target_cols].describe())
    
    # ç‰¹æ€§ã®ç¯„å›²è¨ˆç®—ï¼ˆwMAEè¨ˆç®—ã«å¿…è¦ï¼‰
    property_ranges = {}
    for col in target_cols:
        property_ranges[col] = train[col].dropna().max() - train[col].dropna().min()
        
    print("\nç‰¹æ€§ã®æ¨å®šç¯„å›²:")
    for col, range_val in property_ranges.items():
        print(f"{col}: {range_val:.4f}")
    
    # wMAEãƒ¡ãƒˆãƒªãƒƒã‚¯ç”¨ã®é‡ã¿è¨ˆç®—
    weights = {}
    for col in target_cols:
        weights[col] = (1 / np.sqrt(available_samples[col])) / property_ranges[col]
    
    # é‡ã¿ã‚’æ­£è¦åŒ–
    weight_sum = sum(weights.values())
    for col in target_cols:
        weights[col] = weights[col] / weight_sum * len(target_cols)
    
    print("\nwMAEãƒ¡ãƒˆãƒªãƒƒã‚¯ç”¨æ¨å®šé‡ã¿:")
    for col, weight in weights.items():
        print(f"{col}: {weight:.4f}")
    
    return available_samples, property_ranges, weights

def get_safe_polymer_features(mol):
    """RDKitä½¿ç”¨æ™‚ã®ãƒãƒªãƒãƒ¼å›ºæœ‰ç‰¹å¾´é‡æŠ½å‡º"""
    features = {}
    
    # åŸºæœ¬çš„ãªã‚«ã‚¦ãƒ³ãƒˆ
    features['num_atoms'] = mol.GetNumAtoms()
    features['num_heavy_atoms'] = mol.GetNumHeavyAtoms()
    features['num_bonds'] = mol.GetNumBonds()
    
    # åŸå­ã‚¿ã‚¤ãƒ—ã®ã‚«ã‚¦ãƒ³ãƒˆ
    atom_types = {'C': 0, 'N': 0, 'O': 0, 'S': 0, 'F': 0, 'Cl': 0, 'Br': 0, 'I': 0}
    aromatic_atoms = 0
    
    for atom in mol.GetAtoms():
        symbol = atom.GetSymbol()
        if symbol in atom_types:
            atom_types[symbol] += 1
        
        if atom.GetIsAromatic():
            aromatic_atoms += 1
    
    features['aromatic_atoms'] = aromatic_atoms
    
    # åŸå­ã‚¿ã‚¤ãƒ—ã‚«ã‚¦ãƒ³ãƒˆã‚’ç‰¹å¾´é‡ã«è¿½åŠ 
    for atom_type, count in atom_types.items():
        features[f'num_{atom_type}'] = count
        
        if mol.GetNumHeavyAtoms() > 0:
            features[f'ratio_{atom_type}'] = count / mol.GetNumHeavyAtoms()
        else:
            features[f'ratio_{atom_type}'] = 0
    
    # çµåˆã‚¿ã‚¤ãƒ—ã®ã‚«ã‚¦ãƒ³ãƒˆ
    bond_types = {Chem.rdchem.BondType.SINGLE: 0, 
                  Chem.rdchem.BondType.DOUBLE: 0, 
                  Chem.rdchem.BondType.TRIPLE: 0,
                  Chem.rdchem.BondType.AROMATIC: 0}
    
    for bond in mol.GetBonds():
        bond_type = bond.GetBondType()
        if bond_type in bond_types:
            bond_types[bond_type] += 1
    
    features['num_single_bonds'] = bond_types[Chem.rdchem.BondType.SINGLE]
    features['num_double_bonds'] = bond_types[Chem.rdchem.BondType.DOUBLE]
    features['num_triple_bonds'] = bond_types[Chem.rdchem.BondType.TRIPLE]
    features['num_aromatic_bonds'] = bond_types[Chem.rdchem.BondType.AROMATIC]
    
    # ä¿¡é ¼æ€§ã®é«˜ã„è¨˜è¿°å­ã®è¨ˆç®—
    try:
        features['mw'] = Descriptors.MolWt(mol)
        features['logp'] = Descriptors.MolLogP(mol)
        features['tpsa'] = Descriptors.TPSA(mol)
        features['num_rotatable_bonds'] = Descriptors.NumRotatableBonds(mol)
        features['num_h_donors'] = Descriptors.NumHDonors(mol)
        features['num_h_acceptors'] = Descriptors.NumHAcceptors(mol)
        features['num_rings'] = Descriptors.RingCount(mol)
        features['num_aromatic_rings'] = Descriptors.NumAromaticRings(mol)
        features['num_aliphatic_rings'] = Descriptors.NumAliphaticRings(mol)
    except:
        for desc in ['mw', 'logp', 'tpsa', 'num_rotatable_bonds', 'num_h_donors', 
                     'num_h_acceptors', 'num_rings', 'num_aromatic_rings', 'num_aliphatic_rings']:
            if desc not in features:
                features[desc] = 0
    
    # ãƒãƒªãƒãƒ¼é–¢é€£ã®ã‚«ã‚¹ã‚¿ãƒ æ¯”ç‡
    if mol.GetNumHeavyAtoms() > 0:
        features['rotatable_per_heavy'] = features['num_rotatable_bonds'] / mol.GetNumHeavyAtoms()
        features['rings_per_heavy'] = features.get('num_rings', 0) / mol.GetNumHeavyAtoms()
        features['aromatic_atom_ratio'] = features.get('aromatic_atoms', 0) / mol.GetNumHeavyAtoms()
    else:
        features['rotatable_per_heavy'] = 0
        features['rings_per_heavy'] = 0
        features['aromatic_atom_ratio'] = 0
    
    return features

def generate_basic_smiles_features(smiles_list):
    """RDKitãªã—ã§ã®SMILESåŸºæœ¬ç‰¹å¾´é‡ç”Ÿæˆ"""
    print("RDKitãªã—ã§ã®åŸºæœ¬SMILESç‰¹å¾´é‡ç”Ÿæˆä¸­...")
    
    features = []
    feature_names = [
        'smiles_length', 'num_C', 'num_N', 'num_O', 'num_S', 'num_F', 'num_Cl', 'num_Br',
        'num_equals', 'num_hash', 'num_parens', 'num_brackets', 'num_rings_estimated',
        'aromatic_estimated', 'double_bonds_estimated', 'triple_bonds_estimated'
    ]
    
    for smiles in smiles_list:
        try:
            feat = {}
            feat['smiles_length'] = len(smiles)
            feat['num_C'] = smiles.count('C')
            feat['num_N'] = smiles.count('N')
            feat['num_O'] = smiles.count('O')
            feat['num_S'] = smiles.count('S')
            feat['num_F'] = smiles.count('F')
            feat['num_Cl'] = smiles.count('Cl')
            feat['num_Br'] = smiles.count('Br')
            feat['num_equals'] = smiles.count('=')
            feat['num_hash'] = smiles.count('#')
            feat['num_parens'] = smiles.count('(') + smiles.count(')')
            feat['num_brackets'] = smiles.count('[') + smiles.count(']')
            feat['num_rings_estimated'] = smiles.count('1') + smiles.count('2') + smiles.count('3')
            feat['aromatic_estimated'] = smiles.count('c') + smiles.count('n') + smiles.count('o')
            feat['double_bonds_estimated'] = smiles.count('=')
            feat['triple_bonds_estimated'] = smiles.count('#')
            
            features.append([feat[name] for name in feature_names])
        except:
            features.append([0] * len(feature_names))
    
    return pd.DataFrame(features, columns=feature_names), list(range(len(smiles_list)))

def generate_rdkit_features(smiles_list, calculator):
    """RDKitä½¿ç”¨æ™‚ã®é«˜ç²¾åº¦åˆ†å­ç‰¹å¾´é‡ç”Ÿæˆ"""
    print("RDKitä½¿ç”¨ã§ã®é«˜ç²¾åº¦åˆ†å­ç‰¹å¾´é‡ç”Ÿæˆä¸­...")
    
    # ã‚µãƒ³ãƒ—ãƒ«åˆ†å­ã§ç‰¹å¾´é‡æ§‹é€ ã‚’ä½œæˆ
    sample_mol = Chem.MolFromSmiles('CC')
    sample_descriptors = list(calculator.CalcDescriptors(sample_mol))
    sample_polymer_features = get_safe_polymer_features(sample_mol)
    
    # ãƒ•ã‚£ãƒ³ã‚¬ãƒ¼ãƒ—ãƒªãƒ³ãƒˆ
    sample_morgan_fp = AllChem.GetMorganFingerprintAsBitVect(sample_mol, 2, nBits=256)
    sample_morgan_features = np.zeros((256,))
    AllChem.DataStructs.ConvertToNumpyArray(sample_morgan_fp, sample_morgan_features)
    
    sample_maccs_fp = MACCSkeys.GenMACCSKeys(sample_mol)
    sample_maccs_features = np.zeros((167,))
    AllChem.DataStructs.ConvertToNumpyArray(sample_maccs_fp, sample_maccs_features)
    
    # ç‰¹å¾´é‡åä½œæˆ
    descriptor_names = [x[0] for x in Descriptors._descList]
    polymer_feature_names = list(sample_polymer_features.keys())
    morgan_feature_names = [f'morgan_{i}' for i in range(len(sample_morgan_features))]
    maccs_feature_names = [f'maccs_{i}' for i in range(len(sample_maccs_features))]
    
    feature_names = descriptor_names + polymer_feature_names + morgan_feature_names + maccs_feature_names
    
    features = []
    valid_indices = []
    
    for i, smiles in enumerate(smiles_list):
        try:
            mol = Chem.MolFromSmiles(smiles)
            
            if mol is None and '*' in smiles:
                modified_smiles = smiles.replace('*', 'C')
                mol = Chem.MolFromSmiles(modified_smiles)
            
            if mol is None:
                mol = Chem.MolFromSmiles(smiles, sanitize=False)
                if mol is not None:
                    try:
                        Chem.SanitizeMol(mol, sanitizeOps=Chem.SanitizeFlags.SANITIZE_ALL^Chem.SanitizeFlags.SANITIZE_KEKULIZE)
                    except:
                        pass
            
            if mol is not None:
                # è¨˜è¿°å­è¨ˆç®—
                descriptors = list(calculator.CalcDescriptors(mol))
                polymer_features = list(get_safe_polymer_features(mol).values())
                
                # ãƒ•ã‚£ãƒ³ã‚¬ãƒ¼ãƒ—ãƒªãƒ³ãƒˆè¨ˆç®—
                with warnings.catch_warnings():
                    warnings.simplefilter("ignore")
                    morgan_fp = AllChem.GetMorganFingerprintAsBitVect(mol, 2, nBits=256)
                    morgan_features = np.zeros((256,))
                    AllChem.DataStructs.ConvertToNumpyArray(morgan_fp, morgan_features)
                    
                    maccs_fp = MACCSkeys.GenMACCSKeys(mol)
                    maccs_features = np.zeros((167,))
                    AllChem.DataStructs.ConvertToNumpyArray(maccs_fp, maccs_features)
                
                all_features = descriptors + polymer_features + list(morgan_features) + list(maccs_features)
                
                if len(all_features) != len(feature_names):
                    if len(all_features) < len(feature_names):
                        all_features = all_features + [0] * (len(feature_names) - len(all_features))
                    else:
                        all_features = all_features[:len(feature_names)]
                
                features.append(all_features)
                valid_indices.append(i)
            else:
                if i < 5:
                    print(f"è­¦å‘Š: SMILESè§£æä¸å¯: {smiles}")
        except Exception as e:
            if i < 5:
                print(f"SMILES {smiles} å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}")
    
    print(f"{len(smiles_list)} ä¸­ {len(valid_indices)} ã®SMILESæ–‡å­—åˆ—ã‚’æ­£å¸¸ã«å‡¦ç†")
    print(f"ç‰¹å¾´é‡ãƒ™ã‚¯ãƒˆãƒ«é•·: {len(feature_names)}")
    
    return pd.DataFrame(features, index=valid_indices, columns=feature_names), valid_indices

def generate_molecule_features(smiles_list):
    """SMILESæ–‡å­—åˆ—ã‹ã‚‰åˆ†å­ç‰¹å¾´é‡ã‚’ç”Ÿæˆ"""
    if RDKIT_AVAILABLE:
        descriptor_names = [x[0] for x in Descriptors._descList]
        calculator = MoleculeDescriptors.MolecularDescriptorCalculator(descriptor_names)
        return generate_rdkit_features(smiles_list, calculator)
    else:
        return generate_basic_smiles_features(smiles_list)

def clean_features(df, feature_cols):
    """ç‰¹å¾´é‡ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°"""
    df_clean = df.copy()
    
    # ç„¡é™å€¤ã‚’NaNã§ç½®æ›
    df_clean[feature_cols] = df_clean[feature_cols].replace([np.inf, -np.inf], np.nan)
    
    # æœ‰åŠ¹ãªåˆ—ã‚’ç‰¹å®š
    valid_cols = []
    for col in feature_cols:
        if df_clean[col].notna().sum() > 0 and df_clean[col].nunique() > 1:
            valid_cols.append(col)
    
    print(f"ç„¡åŠ¹åˆ—é™¤å»å¾Œ: {len(valid_cols)} / {len(feature_cols)} ç‰¹å¾´é‡ã‚’ä¿æŒ")
    
    # å¤–ã‚Œå€¤å‡¦ç†
    for col in valid_cols:
        if df_clean[col].dtype.kind in 'fc':
            mean = df_clean[col].mean()
            std = df_clean[col].std()
            if std > 0:
                lower_bound = mean - 5 * std
                upper_bound = mean + 5 * std
                df_clean[col] = df_clean[col].clip(lower=lower_bound, upper=upper_bound)
    
    return df_clean, valid_cols

def select_features_for_target(df, target_col, all_features, max_features=500):
    """ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ¥ç‰¹å¾´é‡é¸æŠ"""
    df_valid = df[df[target_col].notna()].copy()
    
    if len(df_valid) < 50:
        return all_features[:min(len(all_features), max_features)]
    
    correlations = []
    for col in all_features:
        if df_valid[col].dtype.kind in 'fc':
            corr = df_valid[col].corr(df_valid[target_col])
            if not pd.isna(corr):
                correlations.append((col, abs(corr)))
    
    correlations.sort(key=lambda x: x[1], reverse=True)
    top_features = [x[0] for x in correlations[:max_features]]
    
    print(f"{target_col} ç”¨ã«ç›¸é–¢ãƒ™ãƒ¼ã‚¹ã§ {len(top_features)} ç‰¹å¾´é‡ã‚’é¸æŠ")
    return top_features

def get_property_hyperparams(property_name):
    """å„ç‰¹æ€§ã®æœ€é©åŒ–æ¸ˆã¿ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿"""
    params = {
        'Tg': {
            'xgb': {'n_estimators': 1000, 'learning_rate': 0.01, 'max_depth': 6,
                   'subsample': 0.8, 'colsample_bytree': 0.8, 'min_child_weight': 3,
                   'reg_alpha': 0.01, 'reg_lambda': 1.0},
            'cat': {'iterations': 1000, 'learning_rate': 0.03, 'depth': 6, 'l2_leaf_reg': 3}
        },
        'FFV': {
            'xgb': {'n_estimators': 2000, 'learning_rate': 0.005, 'max_depth': 8,
                   'subsample': 0.7, 'colsample_bytree': 0.7, 'min_child_weight': 2,
                   'reg_alpha': 0.1, 'reg_lambda': 0.5},
            'cat': {'iterations': 1500, 'learning_rate': 0.02, 'depth': 7, 'l2_leaf_reg': 2}
        },
        'Tc': {
            'xgb': {'n_estimators': 1500, 'learning_rate': 0.01, 'max_depth': 7,
                   'subsample': 0.85, 'colsample_bytree': 0.75, 'min_child_weight': 3,
                   'reg_alpha': 0.05, 'reg_lambda': 1.0},
            'cat': {'iterations': 1200, 'learning_rate': 0.02, 'depth': 5, 'l2_leaf_reg': 4}
        },
        'Density': {
            'xgb': {'n_estimators': 1200, 'learning_rate': 0.01, 'max_depth': 6,
                   'subsample': 0.8, 'colsample_bytree': 0.8, 'min_child_weight': 2,
                   'reg_alpha': 0.1, 'reg_lambda': 0.5},
            'cat': {'iterations': 1000, 'learning_rate': 0.03, 'depth': 6, 'l2_leaf_reg': 3}
        },
        'Rg': {
            'xgb': {'n_estimators': 1000, 'learning_rate': 0.02, 'max_depth': 7,
                   'subsample': 0.8, 'colsample_bytree': 0.7, 'min_child_weight': 3,
                   'reg_alpha': 0.05, 'reg_lambda': 1.0},
            'cat': {'iterations': 1200, 'learning_rate': 0.02, 'depth': 7, 'l2_leaf_reg': 3}
        }
    }
    return params.get(property_name, params['Tg'])

class AveragingEnsemble:
    """ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«äºˆæ¸¬ç”¨ã‚¯ãƒ©ã‚¹"""
    def __init__(self, models, imputer, scaler):
        self.models = models
        self.imputer = imputer
        self.scaler = scaler
        
    def predict(self, X):
        try:
            X = X.copy()
            X = X.replace([np.inf, -np.inf], np.nan)
            X_imputed = self.imputer.transform(X)
            X_imputed = np.nan_to_num(X_imputed, nan=0.0, posinf=0.0, neginf=0.0)
            X_scaled = self.scaler.transform(X_imputed)
            
            preds = np.column_stack([model.predict(X_scaled) for model in self.models])
            return np.mean(preds, axis=1)
        except Exception as e:
            print(f"äºˆæ¸¬ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return np.zeros(X.shape[0])

class MeanPredictor:
    """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨å¹³å‡äºˆæ¸¬å™¨"""
    def __init__(self, value):
        self.value = value
        
    def predict(self, X):
        return np.full(len(X), self.value)

def train_advanced_model(df, target_col, feature_cols, n_splits=5, seed=42):
    """é«˜åº¦ãªã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«è¨“ç·´"""
    valid_idx = df[target_col].notna()
    df_valid = df.loc[valid_idx]
    
    if len(df_valid) < 30:
        print(f"{target_col} ã®ãƒ‡ãƒ¼ã‚¿ä¸è¶³ã€å¹³å‡å€¤äºˆæ¸¬ã‚’ä½¿ç”¨")
        mean_val = df_valid[target_col].mean() if len(df_valid) > 0 else 0
        return {'model': MeanPredictor(mean_val), 'cv_score': 0.0}
    
    X = df_valid[feature_cols].copy()
    y = df_valid[target_col].values
    
    imputer = KNNImputer(n_neighbors=5)
    X_imputed = imputer.fit_transform(X)
    
    scaler = PowerTransformer(method='yeo-johnson')
    X_scaled = scaler.fit_transform(X_imputed)
    X_scaled = np.nan_to_num(X_scaled, nan=0.0, posinf=0.0, neginf=0.0)
    
    print(f"{target_col} ç”¨ã®é«˜åº¦ãªãƒ¢ãƒ‡ãƒ«ã‚’ {len(y)} ã‚µãƒ³ãƒ—ãƒ«ã§è¨“ç·´")
    
    n_splits = min(n_splits, len(y) // 10 or 2)
    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)
    
    hyperparams = get_property_hyperparams(target_col)
    
    all_models = []
    oof_preds = np.zeros(len(y))
    cv_scores = []
    
    for fold, (train_idx, val_idx) in enumerate(kf.split(X_scaled)):
        X_train, X_val = X_scaled[train_idx], X_scaled[val_idx]
        y_train, y_val = y[train_idx], y[val_idx]
        
        fold_models = []
        fold_preds = []
        
        # XGBoost
        try:
            xgb_model = xgb.XGBRegressor(**hyperparams['xgb'], random_state=seed+fold)
            xgb_model.fit(X_train, y_train)
            xgb_preds = xgb_model.predict(X_val)
            fold_preds.append(xgb_preds)
            fold_models.append(xgb_model)
            print(f"  ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰ {fold+1} XGB MAE: {mean_absolute_error(y_val, xgb_preds):.6f}")
        except Exception as e:
            print(f"  XGBãƒ¢ãƒ‡ãƒ«å¤±æ•—: {str(e)}")
        
        # CatBoost
        try:
            cat_model = CatBoostRegressor(**hyperparams['cat'], random_seed=seed+fold, verbose=False)
            cat_model.fit(X_train, y_train)
            cat_preds = cat_model.predict(X_val)
            fold_preds.append(cat_preds)
            fold_models.append(cat_model)
            print(f"  ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰ {fold+1} CatBoost MAE: {mean_absolute_error(y_val, cat_preds):.6f}")
        except Exception as e:
            print(f"  CatBoostãƒ¢ãƒ‡ãƒ«å¤±æ•—: {str(e)}")
        
        # Random Forest
        try:
            rf_model = RandomForestRegressor(
                n_estimators=200, max_depth=12, min_samples_split=5,
                min_samples_leaf=2, random_state=seed+fold, n_jobs=-1
            )
            rf_model.fit(X_train, y_train)
            rf_preds = rf_model.predict(X_val)
            fold_preds.append(rf_preds)
            fold_models.append(rf_model)
            print(f"  ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰ {fold+1} RF MAE: {mean_absolute_error(y_val, rf_preds):.6f}")
        except Exception as e:
            print(f"  RFãƒ¢ãƒ‡ãƒ«å¤±æ•—: {str(e)}")
        
        # Gradient Boosting
        try:
            gb_model = GradientBoostingRegressor(
                n_estimators=200, learning_rate=0.05, max_depth=6,
                subsample=0.8, random_state=seed+fold
            )
            gb_model.fit(X_train, y_train)
            gb_preds = gb_model.predict(X_val)
            fold_preds.append(gb_preds)
            fold_models.append(gb_model)
            print(f"  ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰ {fold+1} GB MAE: {mean_absolute_error(y_val, gb_preds):.6f}")
        except Exception as e:
            print(f"  GBãƒ¢ãƒ‡ãƒ«å¤±æ•—: {str(e)}")
        
        # KNNï¼ˆå°ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”¨ï¼‰
        if len(y_train) < 1000:
            try:
                knn_model = KNeighborsRegressor(n_neighbors=7)
                knn_model.fit(X_train, y_train)
                knn_preds = knn_model.predict(X_val)
                fold_preds.append(knn_preds)
                fold_models.append(knn_model)
                print(f"  ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰ {fold+1} KNN MAE: {mean_absolute_error(y_val, knn_preds):.6f}")
            except Exception as e:
                print(f"  KNNãƒ¢ãƒ‡ãƒ«å¤±æ•—: {str(e)}")
        
        all_models.extend(fold_models)
        
        if fold_preds:
            ensemble_preds = np.mean(np.column_stack(fold_preds), axis=1)
            oof_preds[val_idx] = ensemble_preds
            fold_score = mean_absolute_error(y_val, ensemble_preds)
            cv_scores.append(fold_score)
            print(f"  ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰ {fold+1} ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ« MAE: {fold_score:.6f}")
    
    if not all_models:
        print("  å…¨ãƒ¢ãƒ‡ãƒ«å¤±æ•—ã€‚å¹³å‡å€¤äºˆæ¸¬ã‚’ä½¿ç”¨ã€‚")
        mean_val = np.mean(y)
        return {'model': MeanPredictor(mean_val), 'cv_score': 0.0}
    
    if cv_scores:
        cv_score = mean_absolute_error(y[~np.isnan(oof_preds)], oof_preds[~np.isnan(oof_preds)])
        print(f"{target_col} ã®ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ MAE: {cv_score:.6f}")
    else:
        cv_score = 0.0
    
    final_model = AveragingEnsemble(all_models, imputer, scaler)
    
    return {'model': final_model, 'cv_score': cv_score}

def save_experiment_results(experiment_dir, models, submission, weights, property_ranges, elapsed_time):
    """å®Ÿé¨“çµæœã®ä¿å­˜"""
    print(f"\nğŸ’¾ å®Ÿé¨“çµæœã‚’ä¿å­˜ä¸­: {experiment_dir}")
    
    # ãƒ¢ãƒ‡ãƒ«æƒ…å ±
    model_info = {}
    for col, model_data in models.items():
        model_info[col] = {
            'cv_score': model_data['cv_score'],
            'model_type': type(model_data['model']).__name__
        }
    
    # å®Ÿé¨“ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
    metadata = {
        'experiment_name': experiment_dir.name,
        'timestamp': datetime.now().isoformat(),
        'rdkit_available': RDKIT_AVAILABLE,
        'model_info': model_info,
        'weights': weights,
        'property_ranges': property_ranges,
        'elapsed_time_minutes': elapsed_time / 60
    }
    
    # ä¿å­˜
    import json
    with open(experiment_dir / 'metadata.json', 'w') as f:
        json.dump(metadata, f, indent=2)
    
    submission.to_csv(experiment_dir / 'submission.csv', index=False)
    
    print(f"âœ… å®Ÿé¨“çµæœä¿å­˜å®Œäº†")

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    start_time = time.time()
    
    print(f"é«˜åº¦ãªãƒãƒªãƒãƒ¼ç‰¹æ€§äºˆæ¸¬ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³é–‹å§‹...")
    print(f"å®Ÿè¡Œæ—¥æ™‚: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # å†ç¾æ€§ã®ãŸã‚ã®ãƒ©ãƒ³ãƒ€ãƒ ã‚·ãƒ¼ãƒ‰è¨­å®š
    SEED = 42
    np.random.seed(SEED)
    
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    train, test, submission = load_local_data()
    
    # ãƒ‡ãƒ¼ã‚¿åˆ†æ
    target_cols = ['Tg', 'FFV', 'Tc', 'Density', 'Rg']
    available_samples, property_ranges, weights = analyze_data(train, target_cols)
    
    # åˆ†å­ç‰¹å¾´é‡ç”Ÿæˆ
    print("\nğŸ§¬ åˆ†å­ç‰¹å¾´é‡ç”Ÿæˆä¸­...")
    train_features, train_valid_idx = generate_molecule_features(train['SMILES'])
    test_features, test_valid_idx = generate_molecule_features(test['SMILES'])
    
    # ç‰¹å¾´é‡çµåˆ
    train_with_features = pd.concat([
        train.iloc[train_valid_idx].reset_index(drop=True),
        train_features.reset_index(drop=True)
    ], axis=1)
    
    test_with_features = pd.concat([
        test.iloc[test_valid_idx].reset_index(drop=True),
        test_features.reset_index(drop=True)
    ], axis=1)
    
    print(f"ç‰¹å¾´é‡ä»˜ãè¨“ç·´ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {train_with_features.shape}")
    print(f"ç‰¹å¾´é‡ä»˜ããƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {test_with_features.shape}")
    
    # ç‰¹å¾´é‡ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
    all_feature_cols = train_features.columns.tolist()
    train_with_features, valid_feature_cols = clean_features(train_with_features, all_feature_cols)
    test_with_features, _ = clean_features(test_with_features, all_feature_cols)
    
    # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ¥ç‰¹å¾´é‡é¸æŠ
    target_features = {}
    for col in target_cols:
        target_features[col] = select_features_for_target(train_with_features, col, valid_feature_cols)
    
    # ãƒ¢ãƒ‡ãƒ«è¨“ç·´
    print("\nğŸ¤– ãƒ¢ãƒ‡ãƒ«è¨“ç·´é–‹å§‹...")
    models = {}
    for col in target_cols:
        print(f"\n{col} ç”¨ã®é«˜åº¦ãªãƒ¢ãƒ‡ãƒ«è¨“ç·´ä¸­...")
        models[col] = train_advanced_model(
            train_with_features, col, target_features[col], seed=SEED
        )
    
    print("\nâœ… å…¨ãƒ¢ãƒ‡ãƒ«è¨“ç·´å®Œäº†")
    
    # äºˆæ¸¬ç”Ÿæˆ
    print("\nğŸ”® ãƒ†ã‚¹ãƒˆäºˆæ¸¬ç”Ÿæˆä¸­...")
    test_preds = {}
    fallbacks = {'Tg': 400, 'FFV': 0.2, 'Tc': 0.2, 'Density': 1.0, 'Rg': 10.0}
    
    for col in target_cols:
        print(f"{col} ã®äºˆæ¸¬ç”Ÿæˆä¸­...")
        try:
            test_preds[col] = models[col]['model'].predict(test_with_features[target_features[col]])
        except Exception as e:
            print(f"{col} ã®äºˆæ¸¬ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)}")
            test_preds[col] = np.full(len(test_with_features), fallbacks[col])
    
    # æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
    submission_df = pd.DataFrame({'id': test_with_features['id']})
    for col in target_cols:
        submission_df[col] = test_preds[col]
    
    print("\nğŸ“‹ æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼:")
    print(submission_df.head())
    
    # æ¬ æè¡Œã®è£œå®Œ
    if len(submission_df) < len(test):
        print(f"è­¦å‘Š: æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ã« {len(submission_df)} è¡Œã€ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã« {len(test)} è¡Œ")
        missing_ids = set(test['id']) - set(submission_df['id'])
        print(f"{len(missing_ids)} ä¸è¶³è¡Œã‚’è¿½åŠ ")
        
        for missing_id in missing_ids:
            row = {'id': missing_id}
            row.update(fallbacks)
            submission_df = pd.concat([submission_df, pd.DataFrame([row])], ignore_index=True)
    
    # CVãƒ™ãƒ¼ã‚¹wMAEè¨ˆç®—
    weighted_scores = []
    for col in target_cols:
        if 'cv_score' in models[col] and models[col]['cv_score'] > 0:
            weighted_score = models[col]['cv_score'] * weights[col] / property_ranges[col]
            weighted_scores.append(weighted_score)
            print(f"{col} ã®é‡ã¿ä»˜ãã‚¹ã‚³ã‚¢: {weighted_score:.6f}")
    
    if weighted_scores:
        estimated_wmae = sum(weighted_scores)
        print(f"\nğŸ¯ æ¨å®šé‡ã¿ä»˜ãMAE: {estimated_wmae:.6f}")
    else:
        print("\nâš ï¸  é‡ã¿ä»˜ãMAEæ¨å®šä¸å¯ï¼ˆCVã‚¹ã‚³ã‚¢åˆ©ç”¨ä¸å¯ï¼‰")
    
    # å®Ÿè¡Œæ™‚é–“
    elapsed_time = time.time() - start_time
    print(f"â±ï¸  ç·å®Ÿè¡Œæ™‚é–“: {elapsed_time/60:.2f} åˆ†")
    
    # å®Ÿé¨“çµæœä¿å­˜
    save_experiment_results(EXPERIMENT_DIR, models, submission_df, weights, property_ranges, elapsed_time)
    
    print("\nğŸ‰ é«˜åº¦ãªãƒãƒªãƒãƒ¼ç‰¹æ€§äºˆæ¸¬ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Œäº†")
    print(f"ğŸ“ å®Ÿé¨“çµæœ: {EXPERIMENT_DIR}")

if __name__ == "__main__":
    main()
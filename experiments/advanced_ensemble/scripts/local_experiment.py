#!/usr/bin/env python3
"""
NeurIPS Open Polymer Prediction 2025 - WandBçµ±åˆãƒ­ãƒ¼ã‚«ãƒ«å®Ÿè¡Œç‰ˆ
é«˜åº¦ãªã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ« + WandBå®Ÿé¨“ç®¡ç†ã«ã‚ˆã‚‹ãƒãƒªãƒãƒ¼ç‰¹æ€§äºˆæ¸¬

ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯Kaggleãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã‚’ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã§å®Ÿè¡Œã§ãã‚‹ã‚ˆã†ã«å¤‰æ›ã—ã€
WandBã«ã‚ˆã‚‹å®Ÿé¨“è¿½è·¡æ©Ÿèƒ½ã‚’çµ±åˆã—ãŸã‚‚ã®ã§ã™ã€‚
"""

import os
import sys
import time
import json
import warnings
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import wandb
from pathlib import Path
from datetime import datetime

# MLé–¢é€£ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
from sklearn.model_selection import KFold, train_test_split
from sklearn.metrics import mean_absolute_error
from sklearn.preprocessing import StandardScaler, RobustScaler, PowerTransformer
from sklearn.impute import SimpleImputer, KNNImputer
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.neighbors import KNeighborsRegressor
import xgboost as xgb
from catboost import CatBoostRegressor, Pool

warnings.filterwarnings('ignore')

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã®è¨­å®š
PROJECT_ROOT = Path(__file__).parent.parent.parent.parent  # 4ã¤ä¸ŠãŒãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆ
DATA_DIR = PROJECT_ROOT / "data" / "raw"
EXPERIMENTS_DIR = Path(__file__).parent.parent  # advanced_ensemble ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
MODELS_DIR = EXPERIMENTS_DIR / "results" / "models"

# ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
EXPERIMENTS_DIR.mkdir(parents=True, exist_ok=True)
MODELS_DIR.mkdir(parents=True, exist_ok=True)

# å®Ÿé¨“ç®¡ç†è¨­å®š
EXPERIMENT_NAME = f"advanced_ensemble_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
EXPERIMENT_DIR = EXPERIMENTS_DIR / "results" / "runs" / EXPERIMENT_NAME
EXPERIMENT_DIR.mkdir(parents=True, exist_ok=True)

print(f"ğŸš€ å®Ÿé¨“é–‹å§‹: {EXPERIMENT_NAME}")
print(f"ğŸ“ å®Ÿé¨“ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {EXPERIMENT_DIR}")

# è¨­å®š
SEED = 42
np.random.seed(SEED)

# WandBè¨­å®š
WANDB_PROJECT = "neurips-polymer-prediction-2025"
WANDB_ENTITY = None  # ãƒãƒ¼ãƒ åãŒã‚ã‚Œã°è¨­å®š

# RDKitåˆ©ç”¨å¯èƒ½æ€§ãƒã‚§ãƒƒã‚¯
try:
    from rdkit import Chem
    from rdkit.Chem import Descriptors, rdMolDescriptors
    from rdkit.Chem.rdMolDescriptors import CalcMolFormula
    from rdkit import DataStructs
    rdkit_available = True
    print("âœ… RDKitåˆ©ç”¨å¯èƒ½ - é«˜ç²¾åº¦åˆ†å­ç‰¹å¾´é‡ã‚’ä½¿ç”¨")
except ImportError:
    rdkit_available = False
    print("âš ï¸  RDKitåˆ©ç”¨ä¸å¯ - åŸºæœ¬SMILESç‰¹å¾´é‡ã‚’ä½¿ç”¨")

def init_wandb(offline_mode=False):
    """WandBåˆæœŸåŒ–"""
    try:
        mode = "offline" if offline_mode else "online"
        run = wandb.init(
            project=WANDB_PROJECT,
            entity=WANDB_ENTITY,
            name=EXPERIMENT_NAME,
            mode=mode,
            config={
                "experiment_name": EXPERIMENT_NAME,
                "seed": SEED,
                "rdkit_available": rdkit_available,
                "model_types": ["xgboost", "catboost", "random_forest", "gradient_boosting", "knn"],
                "cv_folds": 5,
                "max_features": 500,
                "hyperparameters": {
                    "xgboost": {
                        "n_estimators": 200,
                        "max_depth": 8,
                        "learning_rate": 0.05,
                        "subsample": 0.8,
                        "colsample_bytree": 0.8
                    },
                    "catboost": {
                        "iterations": 200,
                        "depth": 7,
                        "learning_rate": 0.08
                    },
                    "random_forest": {
                        "n_estimators": 300,
                        "max_depth": 15
                    },
                    "gradient_boosting": {
                        "n_estimators": 200,
                        "max_depth": 8,
                        "learning_rate": 0.1
                    },
                    "knn": {
                        "n_neighbors": 10
                    }
                }
            }
        )
        print(f"âœ… WandBåˆæœŸåŒ–æˆåŠŸï¼ˆ{mode}ãƒ¢ãƒ¼ãƒ‰ï¼‰")
        return True, run
    except Exception as e:
        print(f"âš ï¸  WandBåˆæœŸåŒ–å¤±æ•—: {e}")
        print("ğŸ“ WandBãªã—ã§å®Ÿé¨“ç¶™ç¶š")
        return False, None

def load_local_data():
    """ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿"""
    print("ğŸ“‚ ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­...")
    print(f"ğŸ“ ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {DATA_DIR}")
    train_path = DATA_DIR / "train.csv"
    test_path = DATA_DIR / "test.csv"
    sample_submission_path = DATA_DIR / "sample_submission.csv"
    
    print(f"ğŸ” è¨“ç·´ãƒ‡ãƒ¼ã‚¿å­˜åœ¨ç¢ºèª: {train_path} -> {train_path.exists()}")
    print(f"ğŸ” ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿å­˜åœ¨ç¢ºèª: {test_path} -> {test_path.exists()}")
    print(f"ğŸ” ã‚µãƒ³ãƒ—ãƒ«æå‡ºå­˜åœ¨ç¢ºèª: {sample_submission_path} -> {sample_submission_path.exists()}")
    
    if not all([train_path.exists(), test_path.exists(), sample_submission_path.exists()]):
        raise FileNotFoundError("å¿…è¦ãªãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
    
    train = pd.read_csv(train_path)
    test = pd.read_csv(test_path)
    submission = pd.read_csv(sample_submission_path)
    
    print(f"âœ… è¨“ç·´ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {train.shape}")
    print(f"âœ… ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {test.shape}")
    
    return train, test, submission

def basic_smiles_features(smiles):
    """åŸºæœ¬çš„ãªSMILESç‰¹å¾´é‡ï¼ˆRDKitä¸ä½¿ç”¨æ™‚ï¼‰"""
    if pd.isna(smiles) or smiles == '':
        return [0] * 16
    
    features = [
        len(smiles),                    # SMILESæ–‡å­—åˆ—é•·
        smiles.count('C'),             # ç‚­ç´ æ•°
        smiles.count('N'),             # çª’ç´ æ•°
        smiles.count('O'),             # é…¸ç´ æ•°
        smiles.count('S'),             # ç¡«é»„æ•°
        smiles.count('P'),             # ãƒªãƒ³æ•°
        smiles.count('F'),             # ãƒ•ãƒƒç´ æ•°
        smiles.count('Cl'),            # å¡©ç´ æ•°
        smiles.count('='),             # äºŒé‡çµåˆæ•°
        smiles.count('#'),             # ä¸‰é‡çµåˆæ•°
        smiles.count('('),             # åˆ†å²æ•°
        smiles.count('['),             # ç‰¹æ®ŠåŸå­æ•°
        smiles.count('@'),             # ã‚­ãƒ©ãƒ«ä¸­å¿ƒæ•°
        smiles.count('c'),             # èŠ³é¦™æ—ç‚­ç´ æ•°
        smiles.count(':'),             # èŠ³é¦™æ—çµåˆæ•°
        smiles.count('-'),             # å˜çµåˆæ•°
    ]
    return features

def rdkit_molecular_features(smiles):
    """RDKitã‚’ä½¿ç”¨ã—ãŸåˆ†å­ç‰¹å¾´é‡"""
    if pd.isna(smiles) or smiles == '':
        return [0] * 100  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆç‰¹å¾´é‡æ•°
    
    mol = Chem.MolFromSmiles(smiles)
    if mol is None:
        return [0] * 100
    
    features = []
    
    # åŸºæœ¬è¨˜è¿°å­ï¼ˆ30å€‹ï¼‰
    basic_descriptors = [
        Descriptors.MolWt,              # åˆ†å­é‡
        Descriptors.NumHDonors,         # æ°´ç´ çµåˆãƒ‰ãƒŠãƒ¼æ•°
        Descriptors.NumHAcceptors,      # æ°´ç´ çµåˆã‚¢ã‚¯ã‚»ãƒ—ã‚¿ãƒ¼æ•°
        Descriptors.TPSA,               # ãƒˆãƒãƒ­ã‚¸ã‚«ãƒ«æ¥µæ€§è¡¨é¢ç©
        Descriptors.MolLogP,            # åˆ†é…ä¿‚æ•°
        Descriptors.NumRotatableBonds,  # å›è»¢å¯èƒ½çµåˆæ•°
        Descriptors.NumAromaticRings,   # èŠ³é¦™ç’°æ•°
        Descriptors.NumSaturatedRings,  # é£½å’Œç’°æ•°
        Descriptors.NumAliphaticRings,  # è„‚è‚ªæ—ç’°æ•°
        Descriptors.RingCount,          # ç’°æ•°
        Descriptors.NumHeteroatoms,     # ãƒ˜ãƒ†ãƒ­åŸå­æ•°
        Descriptors.FractionCSP3,       # sp3ç‚­ç´ ã®å‰²åˆ
        Descriptors.BalabanJ,           # Balaban JæŒ‡æ•°
        Descriptors.BertzCT,            # Bertzåˆ†å­è¤‡é›‘åº¦
        Descriptors.Chi0,               # åˆ†å­é€£çµæ€§æŒ‡æ•° 0æ¬¡
        Descriptors.Chi1,               # åˆ†å­é€£çµæ€§æŒ‡æ•° 1æ¬¡
        Descriptors.Chi0n,              # æ­£è¦åŒ–åˆ†å­é€£çµæ€§æŒ‡æ•° 0æ¬¡
        Descriptors.Chi1n,              # æ­£è¦åŒ–åˆ†å­é€£çµæ€§æŒ‡æ•° 1æ¬¡
        Descriptors.HallKierAlpha,      # Hall-Kier Î±
        Descriptors.Kappa1,             # Kappaå½¢çŠ¶æŒ‡æ•° 1
        Descriptors.Kappa2,             # Kappaå½¢çŠ¶æŒ‡æ•° 2
        Descriptors.Kappa3,             # Kappaå½¢çŠ¶æŒ‡æ•° 3
        Descriptors.LabuteASA,          # Labuteæ¥è§¦é¢ç©
        Descriptors.PEOE_VSA1,          # éƒ¨åˆ†é›»è·åŠ é‡è¡¨é¢ç© 1
        Descriptors.SMR_VSA1,           # SMR åŠ é‡è¡¨é¢ç© 1
        Descriptors.SlogP_VSA1,         # SlogP åŠ é‡è¡¨é¢ç© 1
        Descriptors.EState_VSA1,        # EState åŠ é‡è¡¨é¢ç© 1
        Descriptors.VSA_EState1,        # VSA EState 1
        Descriptors.Ipc,                # æƒ…å ±å«æœ‰é‡
        Descriptors.BertzCT            # å†åº¦Bertzè¤‡é›‘åº¦
    ]
    
    for desc_func in basic_descriptors:
        try:
            features.append(desc_func(mol))
        except:
            features.append(0)
    
    # Morganãƒ•ã‚£ãƒ³ã‚¬ãƒ¼ãƒ—ãƒªãƒ³ãƒˆï¼ˆ70å€‹ã®ãƒ“ãƒƒãƒˆï¼‰
    try:
        morgan_fp = rdMolDescriptors.GetMorganFingerprintAsBitVect(mol, 2, nBits=70)
        features.extend(list(morgan_fp))
    except:
        features.extend([0] * 70)
    
    return features[:100]  # 100å€‹ã®ç‰¹å¾´é‡ã«åˆ¶é™

def calculate_weighted_mae(y_true_df, y_pred_df, target_columns, train_df):
    """
    Calculate weighted MAE (wMAE) score according to the competition formula

    Formula: wMAE = Î£(w_i Ã— MAE_i)
    where w_i = (1/r_i) Ã— (K Ã— âˆš(1/n_i)) / Î£(âˆš(1/n_j))

    Args:
        y_true_df: DataFrame with true values
        y_pred_df: DataFrame with predicted values  
        target_columns: List of target column names
        train_df: Training DataFrame to calculate ranges and sample counts

    Returns:
        wMAE score and individual weights
    """
    # Calculate sample counts and ranges from training data
    K = len(target_columns)  # Total number of tasks
    sample_counts = {}
    value_ranges = {}

    for target in target_columns:
        if target in train_df.columns:
            valid_data = train_df[target].dropna()
            sample_counts[target] = len(valid_data)
            value_ranges[target] = valid_data.max() - valid_data.min()
        else:
            sample_counts[target] = 1  # Default if not available
            value_ranges[target] = 1.0  # Default if not available

    # Calculate sqrt(1/n_i) for each target
    sqrt_inv_n = {}
    for target in target_columns:
        sqrt_inv_n[target] = np.sqrt(1.0 / sample_counts[target])

    # Calculate sum of sqrt(1/n_j) for all j
    sum_sqrt_inv_n = sum(sqrt_inv_n.values())

    # Calculate weights w_i = (1/r_i) Ã— (K Ã— âˆš(1/n_i)) / Î£(âˆš(1/n_j))
    weights = {}
    for target in target_columns:
        weights[target] = (1.0 / value_ranges[target]) * (K * sqrt_inv_n[target]) / sum_sqrt_inv_n

    # Calculate MAE for each target and weighted sum
    individual_maes = {}
    weighted_sum = 0.0

    for target in target_columns:
        if target in y_true_df.columns and target in y_pred_df.columns:
            # Get valid (non-null) predictions and true values
            valid_mask = y_true_df[target].notna() & y_pred_df[target].notna()
            if valid_mask.sum() > 0:
                y_true_valid = y_true_df[target][valid_mask]
                y_pred_valid = y_pred_df[target][valid_mask]
                mae = mean_absolute_error(y_true_valid, y_pred_valid)
                individual_maes[target] = mae
                weighted_sum += weights[target] * mae
            else:
                individual_maes[target] = 0.0
        else:
            individual_maes[target] = 0.0

    return weighted_sum, weights, individual_maes, sample_counts, value_ranges

def feature_engineering(df, wandb_available=False):
    """ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°"""
    print("ğŸ§¬ åˆ†å­ç‰¹å¾´é‡ç”Ÿæˆä¸­...")
    
    if rdkit_available:
        print("  RDKitãƒ™ãƒ¼ã‚¹åˆ†å­è¨˜è¿°å­ã¨ãƒ•ã‚£ãƒ³ã‚¬ãƒ¼ãƒ—ãƒªãƒ³ãƒˆã‚’ä½¿ç”¨")
        features_list = []
        for i, smiles in enumerate(df['SMILES']):
            if i % 1000 == 0:
                print(f"  é€²æ—: {i}/{len(df)}")
            features = rdkit_molecular_features(smiles)
            features_list.append(features)
        
        feature_names = [f'rdkit_feature_{i}' for i in range(100)]
        
        if wandb_available:
            wandb.log({"feature_engineering/method": "rdkit", "feature_engineering/feature_count": 100})
    else:
        print("  åŸºæœ¬SMILESç‰¹å¾´é‡ã‚’ä½¿ç”¨")
        features_list = []
        for smiles in df['SMILES']:
            features = basic_smiles_features(smiles)
            features_list.append(features)
        
        feature_names = [
            'smiles_length', 'carbon_count', 'nitrogen_count', 'oxygen_count', 'sulfur_count',
            'phosphorus_count', 'fluorine_count', 'chlorine_count', 'double_bond_count', 
            'triple_bond_count', 'branch_count', 'special_atom_count', 'chiral_count',
            'aromatic_carbon_count', 'aromatic_bond_count', 'single_bond_count'
        ]
        
        if wandb_available:
            wandb.log({"feature_engineering/method": "basic_smiles", "feature_engineering/feature_count": 16})
    
    features_df = pd.DataFrame(features_list, columns=feature_names)
    
    print(f"âœ… ç‰¹å¾´é‡ç”Ÿæˆå®Œäº†: {features_df.shape[1]}å€‹ã®ç‰¹å¾´é‡")
    return features_df

def train_models_for_target(X, y, target_name, wandb_available=False, n_splits=5):
    """ç‰¹å®šã®ç‰¹æ€§ã«å¯¾ã™ã‚‹ãƒ¢ãƒ‡ãƒ«è¨“ç·´ã¨ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«"""
    print(f"ğŸ¤– {target_name}ç”¨ã®é«˜åº¦ãªãƒ¢ãƒ‡ãƒ«è¨“ç·´ä¸­...")
    
    kf = KFold(n_splits=n_splits, shuffle=True, random_state=SEED)
    models_performance = {}
    
    # ãƒ¢ãƒ‡ãƒ«å®šç¾©ï¼ˆãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´ç‰ˆï¼‰
    models = {
        'XGBoost': xgb.XGBRegressor(
            n_estimators=200, max_depth=8, learning_rate=0.05,
            subsample=0.8, colsample_bytree=0.8, random_state=SEED, n_jobs=-1
        ),
        'CatBoost': CatBoostRegressor(
            iterations=200, depth=7, learning_rate=0.08,
            random_seed=SEED, verbose=False,
            train_dir=str(EXPERIMENT_DIR / "catboost_info")
        ),
        'RandomForest': RandomForestRegressor(
            n_estimators=300, max_depth=15, random_state=SEED, n_jobs=-1
        ),
        'GradientBoosting': GradientBoostingRegressor(
            n_estimators=200, max_depth=8, learning_rate=0.1, random_state=SEED
        ),
        'KNN': KNeighborsRegressor(n_neighbors=10, n_jobs=-1)
    }
    
    cv_results = {}
    fold_predictions = {model_name: [] for model_name in models.keys()}
    fold_true_values = []
    
    for fold, (train_idx, val_idx) in enumerate(kf.split(X)):
        X_train_fold = X.iloc[train_idx]
        X_val_fold = X.iloc[val_idx]
        y_train_fold = y.iloc[train_idx]
        y_val_fold = y.iloc[val_idx]
        
        # ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†
        scaler = StandardScaler()
        X_train_scaled = pd.DataFrame(
            scaler.fit_transform(X_train_fold), 
            columns=X_train_fold.columns, 
            index=X_train_fold.index
        )
        X_val_scaled = pd.DataFrame(
            scaler.transform(X_val_fold), 
            columns=X_val_fold.columns, 
            index=X_val_fold.index
        )
        
        fold_true_values.append(y_val_fold)
        
        for model_name, model in models.items():
            # ãƒ¢ãƒ‡ãƒ«è¨“ç·´
            model.fit(X_train_scaled, y_train_fold)
            y_pred = model.predict(X_val_scaled)
            fold_predictions[model_name].append(y_pred)
            
            mae = mean_absolute_error(y_val_fold, y_pred)
            
            if model_name not in cv_results:
                cv_results[model_name] = {
                    'cv_scores': [],
                    'predictions': []
                }
            
            cv_results[model_name]['cv_scores'].append(mae)
            cv_results[model_name]['predictions'].append(y_pred)
            
            print(f"    ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰ {fold+1} {model_name} MAE: {mae:.6f}")
            
            if wandb_available:
                wandb.log({
                    f"{target_name}/{model_name}/fold_{fold+1}_mae": mae,
                    f"{target_name}/{model_name}/fold": fold+1
                })
    
    # å„ãƒ¢ãƒ‡ãƒ«ã®å¹³å‡æ€§èƒ½ã‚’è¨ˆç®—
    for model_name in models.keys():
        avg_mae = np.mean(cv_results[model_name]['cv_scores'])
        std_mae = np.std(cv_results[model_name]['cv_scores'])
        cv_results[model_name]['cv_mae'] = float(avg_mae)
        cv_results[model_name]['cv_std'] = float(std_mae)
        cv_results[model_name]['cv_scores'] = [float(score) for score in cv_results[model_name]['cv_scores']]
        
        print(f"    {model_name} å¹³å‡ CV MAE: {avg_mae:.6f} (Â±{std_mae:.6f})")
        
        if wandb_available:
            wandb.log({
                f"{target_name}/{model_name}/cv_mae": avg_mae,
                f"{target_name}/{model_name}/cv_std": std_mae
            })
    
    # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«äºˆæ¸¬ã®è¨ˆç®—
    print(f"\n  ğŸ¯ {target_name}ã®ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«äºˆæ¸¬ã‚’è¨ˆç®—ä¸­...")
    
    # 1. å˜ç´”å¹³å‡ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«
    ensemble_predictions_simple = []
    for fold in range(n_splits):
        fold_ensemble = np.mean([fold_predictions[model][fold] for model in models.keys()], axis=0)
        ensemble_predictions_simple.append(fold_ensemble)
    
    # å˜ç´”å¹³å‡ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã®MAEè¨ˆç®—
    simple_ensemble_maes = []
    for fold in range(n_splits):
        mae = mean_absolute_error(fold_true_values[fold], ensemble_predictions_simple[fold])
        simple_ensemble_maes.append(mae)
    
    simple_ensemble_avg_mae = np.mean(simple_ensemble_maes)
    simple_ensemble_std_mae = np.std(simple_ensemble_maes)
    
    print(f"    å˜ç´”å¹³å‡ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ« MAE: {simple_ensemble_avg_mae:.6f} (Â±{simple_ensemble_std_mae:.6f})")
    
    # 2. åŠ é‡å¹³å‡ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ï¼ˆæ€§èƒ½ãƒ™ãƒ¼ã‚¹ã®é‡ã¿ï¼‰
    # å„ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã‚’æ€§èƒ½ï¼ˆMAEï¼‰ã®é€†æ•°ã«åŸºã¥ã„ã¦è¨ˆç®—
    model_weights = {}
    total_inv_mae = sum(1.0 / cv_results[model]['cv_mae'] for model in models.keys())
    for model in models.keys():
        model_weights[model] = (1.0 / cv_results[model]['cv_mae']) / total_inv_mae
    
    print(f"\n  ğŸ“Š æœ€é©åŒ–ã•ã‚ŒãŸé‡ã¿:")
    for model, weight in model_weights.items():
        print(f"    {model}: {weight:.4f}")
    
    # åŠ é‡å¹³å‡ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«äºˆæ¸¬
    ensemble_predictions_weighted = []
    for fold in range(n_splits):
        fold_ensemble = sum(
            fold_predictions[model][fold] * model_weights[model] 
            for model in models.keys()
        )
        ensemble_predictions_weighted.append(fold_ensemble)
    
    # åŠ é‡å¹³å‡ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã®MAEè¨ˆç®—
    weighted_ensemble_maes = []
    for fold in range(n_splits):
        mae = mean_absolute_error(fold_true_values[fold], ensemble_predictions_weighted[fold])
        weighted_ensemble_maes.append(mae)
    
    weighted_ensemble_avg_mae = np.mean(weighted_ensemble_maes)
    weighted_ensemble_std_mae = np.std(weighted_ensemble_maes)
    
    print(f"    åŠ é‡å¹³å‡ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ« MAE: {weighted_ensemble_avg_mae:.6f} (Â±{weighted_ensemble_std_mae:.6f})")
    
    # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«çµæœã‚’cv_resultsã«è¿½åŠ 
    cv_results['SimpleEnsemble'] = {
        'cv_mae': float(simple_ensemble_avg_mae),
        'cv_std': float(simple_ensemble_std_mae),
        'cv_scores': [float(score) for score in simple_ensemble_maes]
    }
    
    cv_results['WeightedEnsemble'] = {
        'cv_mae': float(weighted_ensemble_avg_mae),
        'cv_std': float(weighted_ensemble_std_mae),
        'cv_scores': [float(score) for score in weighted_ensemble_maes],
        'weights': {model: float(weight) for model, weight in model_weights.items()}
    }
    
    if wandb_available:
        wandb.log({
            f"{target_name}/SimpleEnsemble/cv_mae": simple_ensemble_avg_mae,
            f"{target_name}/SimpleEnsemble/cv_std": simple_ensemble_std_mae,
            f"{target_name}/WeightedEnsemble/cv_mae": weighted_ensemble_avg_mae,
            f"{target_name}/WeightedEnsemble/cv_std": weighted_ensemble_std_mae,
            f"{target_name}/WeightedEnsemble/weights": model_weights
        })
    
    # æœ€è‰¯ã®ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠï¼ˆå€‹åˆ¥ãƒ¢ãƒ‡ãƒ«ã¨ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã‚’å«ã‚€ï¼‰
    all_models = list(cv_results.keys())
    best_model = min(all_models, key=lambda m: cv_results[m]['cv_mae'])
    print(f"\n  ğŸ† {target_name}ã®æœ€è‰¯ãƒ¢ãƒ‡ãƒ«: {best_model} (MAE: {cv_results[best_model]['cv_mae']:.6f})")
    
    # å…¨ãƒ‡ãƒ¼ã‚¿ã§æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´
    print(f"\n  ğŸ’¾ {target_name}ã®æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ä¸­...")
    scaler = StandardScaler()
    X_scaled = pd.DataFrame(
        scaler.fit_transform(X), 
        columns=X.columns, 
        index=X.index
    )
    
    trained_models = {}
    for model_name, model in models.items():
        model.fit(X_scaled, y)
        trained_models[model_name] = model
    
    # ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ã¨è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’è¿”ã™
    return cv_results, trained_models, scaler, model_weights

def main_experiment(offline_wandb=True):
    """ãƒ¡ã‚¤ãƒ³å®Ÿé¨“é–¢æ•°"""
    start_time = time.time()
    
    # WandBåˆæœŸåŒ–
    wandb_available, wandb_run = init_wandb(offline_mode=offline_wandb)
    
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    train, test, submission = load_local_data()
    
    if wandb_available:
        wandb.log({
            "data/train_size": len(train),
            "data/test_size": len(test),
            "data/train_columns": list(train.columns),
            "data/test_columns": list(test.columns)
        })
    
    # ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
    train_features = feature_engineering(train, wandb_available)
    test_features = feature_engineering(test, wandb_available)
    
    # ç‰¹æ€§åˆ—ã®ç‰¹å®š
    target_columns = [col for col in train.columns if col not in ['SMILES', 'Id']]
    print(f"ğŸ¯ å¯¾è±¡ç‰¹æ€§: {target_columns}")
    
    if wandb_available:
        wandb.log({"experiment/target_properties": target_columns})
    
    # å„ç‰¹æ€§ã«å¯¾ã—ã¦ãƒ¢ãƒ‡ãƒ«è¨“ç·´ï¼ˆã‚µãƒ³ãƒ—ãƒ«å®Ÿè¡Œã®ãŸã‚åˆ¶é™ï¼‰
    results = {}
    # ã™ã¹ã¦ã®ç‰¹æ€§ã‚’å‡¦ç†ï¼ˆidã‚’é™¤ãï¼‰
    actual_targets = [col for col in target_columns if col != 'id']
    
    for target_col in actual_targets:
        if target_col in train.columns:
            # æ¬ æå€¤é™¤å»
            valid_mask = ~train[target_col].isna()
            if valid_mask.sum() < 10:
                print(f"âš ï¸  {target_col}: ãƒ‡ãƒ¼ã‚¿ä¸è¶³ï¼ˆ{valid_mask.sum()}ä»¶ï¼‰- ã‚¹ã‚­ãƒƒãƒ—")
                continue
            
            X_valid = train_features[valid_mask]
            y_valid = train[target_col][valid_mask]
            
            print(f"\nğŸ“Š {target_col} - æœ‰åŠ¹ãƒ‡ãƒ¼ã‚¿: {len(X_valid)}ä»¶")
            
            # ãƒ¢ãƒ‡ãƒ«è¨“ç·´ã¨ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«
            cv_results, trained_models, scaler, ensemble_weights = train_models_for_target(
                X_valid, y_valid, target_col, wandb_available, n_splits=3  # é«˜é€ŸåŒ–ã®ãŸã‚3-fold
            )
            results[target_col] = {
                'cv_results': cv_results,
                'trained_models': trained_models,
                'scaler': scaler,
                'ensemble_weights': ensemble_weights
            }
    
    # å®Ÿé¨“çµæœä¿å­˜
    elapsed_time = time.time() - start_time
    
    experiment_metadata = {
        "experiment_name": EXPERIMENT_NAME,
        "timestamp": datetime.now().isoformat(),
        "rdkit_available": rdkit_available,
        "elapsed_time": elapsed_time,
        "results": {
            target: {
                model: {
                    'cv_mae': data['cv_results'][model]['cv_mae'],
                    'cv_std': data['cv_results'][model]['cv_std'],
                    'cv_scores': data['cv_results'][model]['cv_scores']
                } if model not in ['WeightedEnsemble'] else {
                    'cv_mae': data['cv_results'][model]['cv_mae'],
                    'cv_std': data['cv_results'][model]['cv_std'],
                    'cv_scores': data['cv_results'][model]['cv_scores'],
                    'weights': data['cv_results'][model]['weights']
                }
                for model in data['cv_results'] if 'predictions' not in data['cv_results'][model]
            }
            for target, data in results.items()
        },
        "ensemble_info": {
            target: {
                "best_model": min(data['cv_results'].keys(), key=lambda m: data['cv_results'][m]['cv_mae']),
                "weighted_ensemble_weights": data['ensemble_weights']
            } for target, data in results.items()
        },
        "hyperparameters": {
            "xgboost": {
                "n_estimators": 200,
                "max_depth": 8,
                "learning_rate": 0.05,
                "subsample": 0.8,
                "colsample_bytree": 0.8
            },
            "catboost": {
                "iterations": 200,
                "depth": 7,
                "learning_rate": 0.08
            },
            "random_forest": {
                "n_estimators": 300,
                "max_depth": 15
            },
            "gradient_boosting": {
                "n_estimators": 200,
                "max_depth": 8,
                "learning_rate": 0.1
            },
            "knn": {
                "n_neighbors": 10
            }
        }
    }
    
    metadata_path = EXPERIMENT_DIR / "metadata.json"
    with open(metadata_path, 'w', encoding='utf-8') as f:
        json.dump(experiment_metadata, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ“Š å®Ÿé¨“çµæœã‚µãƒãƒªãƒ¼:")
    for target, target_data in results.items():
        print(f"  {target}:")
        for model, performance in target_data['cv_results'].items():
            if model == 'WeightedEnsemble' and 'weights' in performance:
                print(f"    {model}: {performance['cv_mae']:.6f} (Â±{performance['cv_std']:.6f})")
                print(f"      é‡ã¿: {performance['weights']}")
            else:
                print(f"    {model}: {performance['cv_mae']:.6f} (Â±{performance['cv_std']:.6f})")
    
    # wMAEè¨ˆç®—ã®ãŸã‚ã®æº–å‚™
    print(f"\nğŸ“Š wMAEï¼ˆé‡ã¿ä»˜ãå¹³å‡çµ¶å¯¾èª¤å·®ï¼‰è¨ˆç®—:")
    
    # å„ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬å€¤ã‚’æ ¼ç´ã™ã‚‹DataFrameï¼ˆCVã®MAEã‚’ä½¿ã£ãŸæ¨å®šï¼‰
    # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã‚’å«ã‚€ã™ã¹ã¦ã®ãƒ¢ãƒ‡ãƒ«åã‚’å–å¾—
    model_names = list(next(iter(results.values()))['cv_results'].keys())
    
    for model_name in model_names:
        print(f"\n  {model_name}ãƒ¢ãƒ‡ãƒ«ã®wMAEè¨ˆç®—:")
        
        # CVã®MAEã‚’ä½¿ã£ã¦ç–‘ä¼¼çš„ãªDataFrameã‚’ä½œæˆ
        y_true_dict = {}
        y_pred_dict = {}
        
        for target in actual_targets:
            if target in results and model_name in results[target]['cv_results']:
                # CVã®MAEã‚’æŒã¤ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆï¼ˆå®Ÿéš›ã®äºˆæ¸¬ã§ã¯ãªãæ¨å®šï¼‰
                cv_mae = results[target]['cv_results'][model_name]['cv_mae']
                y_true_dict[target] = [0.0]  # ãƒ€ãƒŸãƒ¼å€¤
                y_pred_dict[target] = [cv_mae]  # MAEã‚’äºˆæ¸¬èª¤å·®ã¨ã—ã¦ä½¿ç”¨
        
        y_true_df = pd.DataFrame(y_true_dict)
        y_pred_df = pd.DataFrame(y_pred_dict)
        
        # wMAEè¨ˆç®—
        try:
            wmae, weights, individual_maes, sample_counts, value_ranges = calculate_weighted_mae(
                y_true_df, y_pred_df, actual_targets, train
            )
            
            print(f"    é‡ã¿:")
            for target in actual_targets:
                if target in weights:
                    print(f"      {target}: {weights[target]:.4f}")
            
            print(f"    å€‹åˆ¥MAE:")
            for target in actual_targets:
                if target in individual_maes:
                    print(f"      {target}: {individual_maes[target]:.4f}")
            
            print(f"    æ¨å®šwMAE: {wmae:.4f}")
            
            if wandb_available:
                wandb.log({
                    f"wmae/{model_name}/estimated_wmae": wmae,
                    f"wmae/{model_name}/weights": weights,
                    f"wmae/{model_name}/individual_maes": individual_maes
                })
        except Exception as e:
            print(f"    wMAEè¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
    
    if wandb_available:
        # ç·åˆæŒ‡æ¨™ã‚’WandBã«è¨˜éŒ²
        avg_performance = {}
        for target, target_data in results.items():
            for model, performance in target_data['cv_results'].items():
                if model not in avg_performance:
                    avg_performance[model] = []
                avg_performance[model].append(performance['cv_mae'])
        
        for model, maes in avg_performance.items():
            avg_mae = np.mean(maes)
            wandb.log({f"overall/{model}_avg_mae": avg_mae})
        
        wandb.log({
            "experiment/elapsed_time": elapsed_time,
            "experiment/completed_targets": len(results)
        })
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚‚WandBã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        wandb.save(str(metadata_path))
        
        wandb.finish()
        print("âœ… WandBå®Ÿé¨“è¨˜éŒ²å®Œäº†")
    
    print(f"â±ï¸  ç·å®Ÿè¡Œæ™‚é–“: {elapsed_time:.2f} ç§’")
    print("ğŸ‰ WandBçµ±åˆå®Ÿé¨“å®Œäº†!")
    
    return results

if __name__ == "__main__":
    results = main_experiment(offline_wandb=True)
    print(f"\nğŸ¯ æœ€çµ‚çµæœ: {len(results)}å€‹ã®ç‰¹æ€§ã§å®Ÿé¨“å®Œäº†")